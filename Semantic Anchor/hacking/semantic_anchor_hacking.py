#!/usr/bin/env python3
"""
Semantic Anchor Evaluator: hacking
====================================

Auto-generated by semantic_anchor_generator.py

Loads anchors from: anchors_list_hacking.json

Usage:
  python semantic_anchor_hacking.py                       # Interactive (cosine mode)
  python semantic_anchor_hacking.py --mode nli             # NLI entailment scoring
  python semantic_anchor_hacking.py --mode hybrid          # NLI + smart contrastive (recommended)
  python semantic_anchor_hacking.py --mode llm             # LLM-as-judge scoring
  python semantic_anchor_hacking.py --compare              # Compare all modes side-by-side
  python semantic_anchor_hacking.py --compare -f input.txt # Compare all modes on file
  python semantic_anchor_hacking.py --verbose              # Full table
  python semantic_anchor_hacking.py --show-examples        # View anchors
  python semantic_anchor_hacking.py --file input.txt       # File mode (### separated)
  python semantic_anchor_hacking.py --graph                # 3D anchor spread visualization

Scoring modes:
  cosine:  Cosine similarity (bi-encoder). Fast, good for clean text.
  nli:     NLI entailment on ALL anchors + cosine contrastive. Handles paraphrases well.
  hybrid:  NLI scoring + smart contrastive (NLI-confirmed negatives). Best accuracy.
           Uses cosine gap as a gate, then NLI on top-5 negatives to confirm.
  llm:     LLM-as-judge. Sends proposition + message to an LLM API.
           Requires config_hacking.ini with provider, model, api_key.
  compare: Runs cosine + nli + hybrid (+ llm if configured) side-by-side.
           Shows verdict, score, neg_cos, top-3 anchors per mode in a table.

Requirements:
  pip install sentence-transformers
  pip install matplotlib scikit-learn       (only for --graph)
  pip install autocorrect                   (only for --spellcheck)
  pip install httpx                         (only for --mode llm)

Long message handling:
  All modes use proposition-guided extraction for messages > EXTRACTION_MIN_WORDS.
"""

import argparse
import json
import os
import sys

# ---------------------------------------------------------------------------
# Configuration loader — reads from anchors_list_<name>.json
# ---------------------------------------------------------------------------

SCRIPT_NAME = "hacking"

def _find_anchors_file():
    """Locate the anchors JSON file relative to this script's directory."""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    filename = "anchors_list_{}.json".format(SCRIPT_NAME)
    # Look in same directory as the script
    path = os.path.join(script_dir, filename)
    if os.path.exists(path):
        return path
    # Look in parent directory (if script is in a subfolder)
    parent = os.path.join(os.path.dirname(script_dir), filename)
    if os.path.exists(parent):
        return parent
    return None


def _load_anchors_config():
    """Load proposition, anchors, thresholds, negative and neutral anchors from JSON."""
    path = _find_anchors_file()
    if path is None:
        print("\n  ERROR: Anchors file not found: anchors_list_{}.json".format(SCRIPT_NAME))
        print("  Expected location: same directory as this script")
        print("  Generate it with: python semantic_anchor_generator.py -name {} -ga".format(
            SCRIPT_NAME))
        sys.exit(1)

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    metadata = data.get("metadata", {})

    return (
        data["proposition"],
        data["anchors"],
        data.get("match_threshold", 0.55),
        data.get("warning_threshold", 0.45),
        data.get("negative_anchors", {}),
        data.get("neutral_anchors", []),
        metadata.get("embedding_model", "all-mpnet-base-v2"),
        metadata.get("nli_model", "cross-encoder/nli-deberta-v3-large"),
    )


def _load_evaluator_config():
    """
    Load config_<name>.ini and override model names from JSON defaults.
    Priority: config_<name>.ini > anchors_list JSON metadata > hardcoded defaults.
    """
    import configparser as _cp

    # Start with JSON defaults
    (proposition, anchors, match_thresh, warn_thresh,
     neg_anchors, neutral_anchors,
     json_emb_model, json_nli_model) = _load_anchors_config()

    emb_model = json_emb_model
    nli_model = json_nli_model
    emb_source = "anchors JSON"
    nli_source = "anchors JSON"

    # Look for config_<n>.ini
    script_dir = os.path.dirname(os.path.abspath(__file__))
    cfg_path = os.path.join(script_dir, "config_{}.ini".format(SCRIPT_NAME))
    if not os.path.exists(cfg_path):
        # Also check parent directory
        cfg_path = os.path.join(os.path.dirname(script_dir),
                                "config_{}.ini".format(SCRIPT_NAME))

    if os.path.exists(cfg_path):
        cfg = _cp.ConfigParser()
        cfg.read(cfg_path)

        # [models] section takes priority, then [llm_judge]
        for section in ("models", "llm_judge"):
            if cfg.has_section(section):
                val = cfg.get(section, "embedding_model", fallback=None)
                if val and val.strip():
                    emb_model = val.strip()
                    emb_source = "config_{}.ini [{}]".format(SCRIPT_NAME, section)
                val = cfg.get(section, "nli_model", fallback=None)
                if val and val.strip():
                    nli_model = val.strip()
                    nli_source = "config_{}.ini [{}]".format(SCRIPT_NAME, section)

    # Store sources for startup logging
    _model_sources["embedding"] = (emb_model, emb_source)
    _model_sources["nli"] = (nli_model, nli_source)

    return (proposition, anchors, match_thresh, warn_thresh,
            neg_anchors, neutral_anchors, emb_model, nli_model)


# Model source tracking — filled by _load_evaluator_config, read by main()
_model_sources = {}

(PROPOSITION, ANCHORS, MATCH_THRESHOLD, WARNING_THRESHOLD,
 NEGATIVE_ANCHORS, NEUTRAL_ANCHORS,
 EMBEDDING_MODEL, NLI_MODEL) = _load_evaluator_config()

# ---------------------------------------------------------------------------
# Reranking config
# ---------------------------------------------------------------------------

RERANK_ALPHA = 0.5         # weight for cosine in combined score (0.5 = equal weight)
RERANK_TOP_K = 5           # number of top anchors to run cross-encoder on
RERANK_SKIP_BELOW = 0.25   # skip reranking if top cosine is below this

# ---------------------------------------------------------------------------
# Proposition-guided extraction config
# ---------------------------------------------------------------------------

EXTRACTION_MIN_WORDS = 30       # only extract for messages longer than this
EXTRACTION_RELEVANCE_TAU = 0.18 # low threshold — keep any topically related sentence
EXTRACTION_MIN_SELECTED = 1     # need at least this many sentences to use extraction
EXTRACTION_MAX_WINDOW = 3       # also try sliding windows of this size for adjacent patterns

# ---------------------------------------------------------------------------
# Colors
# ---------------------------------------------------------------------------

BLUE = "\033[94m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
BOLD = "\033[1m"
DIM = "\033[2m"
CYAN = "\033[96m"
RESET = "\033[0m"


def color_score(score):
    s = "{:.4f}".format(score)
    if score >= MATCH_THRESHOLD:
        return RED + BOLD + s + RESET
    elif score >= WARNING_THRESHOLD:
        return YELLOW + s + RESET
    else:
        return GREEN + s + RESET


# ---------------------------------------------------------------------------
# Spell correction (disabled by default — enable via --spellcheck or config)
# ---------------------------------------------------------------------------

SPELLCHECK_ENABLED = False  # Set by CLI flag or config_<name>.ini

_spellchecker = None

def _load_spellchecker():
    global _spellchecker
    if _spellchecker is not None:
        return _spellchecker
    try:
        from autocorrect import Speller
        _spellchecker = Speller(lang="en")
        return _spellchecker
    except ImportError:
        pass
    return None


def correct_spelling(text):
    """Correct spelling if SPELLCHECK_ENABLED, otherwise pass through."""
    if not SPELLCHECK_ENABLED:
        return text, []

    checker = _load_spellchecker()
    if checker is None:
        return text, []

    corrected = checker(text)
    if corrected == text:
        return text, []

    orig_words = text.split()
    corr_words = corrected.split()
    changes = []
    for o, c in zip(orig_words, corr_words):
        if o.lower() != c.lower():
            changes.append((o, c))
    return corrected, changes


# ---------------------------------------------------------------------------
# Sentence splitting (robust, always splits)
# ---------------------------------------------------------------------------

def _split_into_sentences(text):
    """
    Split text into sentences. Always splits regardless of length.
    Handles common abbreviations and edge cases.
    """
    import re
    text = text.strip()
    if not text:
        return []

    # Split on sentence boundaries: .!? followed by space+uppercase or end
    parts = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)

    sentences = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        # Also split on semicolons and long conjunctions that separate clauses
        sub_parts = re.split(r';\s+|(?<=\.)\s+(?:Also|Separately|Additionally|Furthermore|Moreover),?\s+', p)
        for sp in sub_parts:
            sp = sp.strip()
            if sp:
                sentences.append(sp)

    # Merge very short fragments (< 3 words) with previous
    merged = []
    for s in sentences:
        if merged and len(s.split()) < 3:
            merged[-1] = merged[-1] + " " + s
        else:
            merged.append(s)

    return merged if merged else [text]


def _split_sentences_legacy(text):
    """Legacy sentence splitter for backward compatibility (skips short texts)."""
    import re
    text = text.strip()
    if not text:
        return []
    if len(text.split()) < 60:
        return [text]
    parts = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
    sentences = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        if sentences and len(p.split()) < 4:
            sentences[-1] = sentences[-1] + " " + p
        else:
            sentences.append(p)
    return sentences if sentences else [text]


# ---------------------------------------------------------------------------
# Proposition-Guided Extraction (core long-message defense)
# ---------------------------------------------------------------------------

def extract_relevant_chunks(model, message, all_embeddings, all_texts):
    """
    Proposition-guided extraction for long messages.

    Instead of embedding the full message (which dilutes signal when harmful
    intent is distributed across innocent-looking sentences), this function:

    1. Splits the message into individual sentences
    2. Embeds each sentence independently
    3. Scores each sentence against ALL anchors (max cosine to any anchor)
    4. Selects sentences above a low relevance threshold tau
    5. Returns the composed relevant chunk for downstream scoring

    Also computes sliding windows to catch adjacent-sentence patterns.

    Returns:
        dict with keys:
            'extracted_text': str - the composed relevant sentences
            'was_extracted': bool - whether extraction was applied
            'all_sentences': list[str] - all sentences from the message
            'selected_indices': list[int] - which sentences were selected
            'sentence_scores': list[float] - max-anchor score per sentence
            'method': str - 'extraction' or 'passthrough'
            'sliding_window_text': str - best sliding window chunk
            'sliding_window_score': float - score of best window
    """
    import numpy as np
    from sentence_transformers.util import cos_sim

    words = message.split()
    result = {
        'extracted_text': message,
        'was_extracted': False,
        'all_sentences': [message],
        'selected_indices': [0],
        'sentence_scores': [0.0],
        'method': 'passthrough',
        'sliding_window_text': message,
        'sliding_window_score': 0.0,
    }

    # Only extract for messages above the word threshold
    if len(words) <= EXTRACTION_MIN_WORDS:
        return result

    # Step 1: Split into sentences
    sentences = _split_into_sentences(message)
    if len(sentences) <= 1:
        return result

    # Step 2: Embed all sentences in one batch
    sent_embeddings = model.encode(sentences, show_progress_bar=False)

    # Step 3: Score each sentence against all anchors (max cosine to any anchor)
    # This is the proposition-guided relevance filter
    sim_matrix = cos_sim(sent_embeddings, all_embeddings)  # (n_sents, n_anchors)
    sentence_scores = sim_matrix.max(dim=1).values.tolist()  # max anchor score per sentence

    # Step 4: Select sentences above the relevance threshold
    selected_indices = [
        i for i, score in enumerate(sentence_scores)
        if score >= EXTRACTION_RELEVANCE_TAU
    ]

    # Step 5: Sliding window analysis (catches adjacent-sentence patterns)
    best_window_score = 0.0
    best_window_text = message
    window_size = min(EXTRACTION_MAX_WINDOW, len(sentences))

    for ws in range(2, window_size + 1):
        for start in range(len(sentences) - ws + 1):
            window_text = " ".join(sentences[start:start + ws])
            window_emb = model.encode([window_text], show_progress_bar=False)
            window_sims = cos_sim(window_emb, all_embeddings)[0]
            window_max = float(window_sims.max())
            if window_max > best_window_score:
                best_window_score = window_max
                best_window_text = window_text

    # Step 6: Build the extracted chunk
    if len(selected_indices) >= EXTRACTION_MIN_SELECTED:
        extracted_text = " ".join(sentences[i] for i in selected_indices)
        result.update({
            'extracted_text': extracted_text,
            'was_extracted': True,
            'all_sentences': sentences,
            'selected_indices': selected_indices,
            'sentence_scores': sentence_scores,
            'method': 'extraction',
            'sliding_window_text': best_window_text,
            'sliding_window_score': best_window_score,
        })
    else:
        # Not enough relevant sentences found — fall back to full message
        # but still report the analysis
        result.update({
            'all_sentences': sentences,
            'selected_indices': selected_indices,
            'sentence_scores': sentence_scores,
            'sliding_window_text': best_window_text,
            'sliding_window_score': best_window_score,
        })

    return result


def _display_extraction_info(extraction, verbose=False):
    """Print extraction diagnostic info."""
    if not extraction['was_extracted']:
        return

    n_total = len(extraction['all_sentences'])
    n_selected = len(extraction['selected_indices'])
    n_dropped = n_total - n_selected

    print("  {}Extraction:{} {} sentences \u2192 {} relevant, {} noise dropped".format(
        CYAN, RESET, n_total, n_selected, n_dropped))

    if verbose:
        for i, (sent, score) in enumerate(zip(
                extraction['all_sentences'], extraction['sentence_scores'])):
            marker = "\u2714" if i in extraction['selected_indices'] else "\u2718"
            disp = sent if len(sent) <= 60 else sent[:57] + "..."
            score_color = GREEN if score < EXTRACTION_RELEVANCE_TAU else YELLOW
            print("    {} {}{:.3f}{} \"{}\"".format(
                marker, score_color, score, RESET, disp))

    if extraction['sliding_window_score'] > 0:
        ws = extraction['sliding_window_text']
        ws_disp = ws if len(ws) <= 70 else ws[:67] + "..."
        print("  {}Best window:{} score={:.4f} \"{}\"".format(
            DIM, RESET, extraction['sliding_window_score'], ws_disp))


# ---------------------------------------------------------------------------
# Embedding & Scoring
# ---------------------------------------------------------------------------

def load_model():
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        print("\n  ERROR: sentence-transformers not installed.")
        print("  Run: pip install sentence-transformers")
        sys.exit(1)
    print("  Loading embedding model: {}...".format(EMBEDDING_MODEL))
    return SentenceTransformer(EMBEDDING_MODEL)


def prepare_anchors(model):
    all_texts = []
    all_categories = []
    for cat, examples in ANCHORS.items():
        for ex in examples:
            all_texts.append(ex)
            all_categories.append(cat)
    print("  Embedding {} positive anchors...".format(len(all_texts)))
    all_embeddings = model.encode(all_texts, show_progress_bar=False)

    # Negative anchors (contrastive)
    neg_texts = []
    neg_categories = []
    neg_embeddings = None
    for cat, examples in NEGATIVE_ANCHORS.items():
        for ex in examples:
            neg_texts.append(ex)
            neg_categories.append(cat)
    if neg_texts:
        print("  Embedding {} negative anchors...".format(len(neg_texts)))
        neg_embeddings = model.encode(neg_texts, show_progress_bar=False)
    else:
        print("  No negative anchors (contrastive scoring disabled)")

    # Neutral anchors (off-topic baseline)
    neutral_embeddings = None
    if NEUTRAL_ANCHORS:
        print("  Embedding {} neutral anchors...".format(len(NEUTRAL_ANCHORS)))
        neutral_embeddings = model.encode(NEUTRAL_ANCHORS, show_progress_bar=False)

    print("  Ready.\n")
    return (all_texts, all_categories, all_embeddings,
            neg_texts, neg_categories, neg_embeddings,
            neutral_embeddings)


# ---------------------------------------------------------------------------
# Contrastive adjustment (gap-based)
# ---------------------------------------------------------------------------

def _contrastive_adjust(pos_score, pos_cos, neg_cos, neutral_cos=0.0):
    """
    Adjust a score using the contrastive cosine gap.

    Uses COSINE similarity (surface-level) to compare how close the message
    is to positive vs negative anchors, regardless of what scoring mode
    produced pos_score (cosine, NLI, or rerank).

    If neutral_cos > pos_cos, the message is off-topic — heavy suppression.
    Otherwise, the gap between pos_cos and neg_cos determines penalty.

    Returns (adjusted_score, neg_cos).
    """
    # Off-topic check: closer to neutral than to domain anchors
    if neutral_cos > 0.4 and neutral_cos > pos_cos and neutral_cos > neg_cos:
        return pos_score * 0.15, neg_cos

    # Negatives not close at all — no adjustment
    if neg_cos < 0.4:
        return pos_score, neg_cos

    gap = pos_cos - neg_cos

    if gap >= 0.10:
        # Clearly closer to positive anchors — no penalty
        return pos_score, neg_cos
    elif gap >= 0:
        # Very close call — moderate fade (up to 50%)
        fade = (0.10 - gap) / 0.10  # 0..1
        return pos_score * (1.0 - fade * 0.50), neg_cos
    else:
        # Closer to negative anchors — heavy penalty
        # Starts at 50%, scales to 95% as gap widens
        penalty = min(0.95, 0.50 + abs(gap) * 3.0)
        return pos_score * (1.0 - penalty), neg_cos


def _compute_max_cosine(model, message_embedding, target_embeddings):
    """Compute max cosine similarity between message and a set of embeddings."""
    from sentence_transformers.util import cos_sim
    if target_embeddings is None:
        return 0.0
    sims = cos_sim(message_embedding, target_embeddings)[0].tolist()
    return max(sims)


def score_message(model, message, all_embeddings, all_texts, all_categories,
                  neg_embeddings=None, neg_texts=None, neutral_embeddings=None,
                  use_extraction=True):
    """
    Cosine scoring with extraction and gap-based contrastive adjustment.

    Returns (results_list, extraction_info, neg_cos) where scores are adjusted.
    """
    from sentence_transformers.util import cos_sim

    extraction = None
    if use_extraction:
        extraction = extract_relevant_chunks(model, message, all_embeddings, all_texts)

    if extraction and extraction['was_extracted']:
        emb_extracted = model.encode(extraction['extracted_text'])
        sims_extracted = cos_sim(emb_extracted, all_embeddings)[0].tolist()

        emb_full = model.encode(message)
        sims_full = cos_sim(emb_full, all_embeddings)[0].tolist()

        emb_window = model.encode(extraction['sliding_window_text'])
        sims_window = cos_sim(emb_window, all_embeddings)[0].tolist()

        pos_sims = [max(se, sf, sw) for se, sf, sw in zip(sims_extracted, sims_full, sims_window)]
        pos_cos = max(pos_sims)

        neg_cos = 0.0
        if neg_embeddings is not None:
            neg_cos = max(
                _compute_max_cosine(model, emb_extracted, neg_embeddings),
                _compute_max_cosine(model, emb_full, neg_embeddings),
                _compute_max_cosine(model, emb_window, neg_embeddings),
            )

        neutral_cos = 0.0
        if neutral_embeddings is not None:
            neutral_cos = max(
                _compute_max_cosine(model, emb_extracted, neutral_embeddings),
                _compute_max_cosine(model, emb_full, neutral_embeddings),
                _compute_max_cosine(model, emb_window, neutral_embeddings),
            )
    else:
        emb = model.encode(message)
        pos_sims = cos_sim(emb, all_embeddings)[0].tolist()
        pos_cos = max(pos_sims)

        neg_cos = 0.0
        if neg_embeddings is not None:
            neg_cos = _compute_max_cosine(model, emb, neg_embeddings)

        neutral_cos = 0.0
        if neutral_embeddings is not None:
            neutral_cos = _compute_max_cosine(model, emb, neutral_embeddings)

    adjusted_sims = []
    for ps in pos_sims:
        adj, _ = _contrastive_adjust(ps, pos_cos, neg_cos, neutral_cos)
        adjusted_sims.append(adj)

    results = sorted(
        zip(adjusted_sims, all_texts, all_categories),
        key=lambda x: x[0], reverse=True)
    return results, extraction, neg_cos


# ---------------------------------------------------------------------------
# Cross-encoder reranking
# ---------------------------------------------------------------------------

_cross_encoder = None
_entailment_idx = None

def _load_cross_encoder():
    """Lazy-load NLI cross-encoder model for intent matching."""
    global _cross_encoder, _entailment_idx
    if _cross_encoder is not None:
        return _cross_encoder
    try:
        from sentence_transformers import CrossEncoder
    except ImportError:
        print("\n  ERROR: sentence-transformers version doesn't support CrossEncoder.")
        print("  Run: pip install -U sentence-transformers")
        sys.exit(1)
    import numpy as np
    print("  Loading NLI cross-encoder model: {}...".format(NLI_MODEL))
    _cross_encoder = CrossEncoder(NLI_MODEL)

    cal_pairs = [
        ["A person is building a weapon", "Someone wants to make a weapon"],
        ["I love eating pizza", "Someone wants to make a weapon"],
    ]
    cal_logits = np.array(_cross_encoder.predict(cal_pairs, apply_softmax=False))
    cal_exp = np.exp(cal_logits - np.max(cal_logits, axis=1, keepdims=True))
    cal_probs = cal_exp / cal_exp.sum(axis=1, keepdims=True)

    entail_row = cal_probs[0]
    contra_row = cal_probs[1]

    best_idx = int(np.argmax(entail_row))
    if contra_row[best_idx] < 0.3:
        _entailment_idx = best_idx
    else:
        gaps = entail_row - contra_row
        _entailment_idx = int(np.argmax(gaps))

    print("  Entailment index: {} (calibrated: entail_pair={:.3f}, contra_pair={:.3f})".format(
        _entailment_idx,
        entail_row[_entailment_idx],
        contra_row[_entailment_idx]))

    return _cross_encoder


def _nli_entailment_scores(pairs):
    """Compute entailment probability for each pair using NLI cross-encoder."""
    import numpy as np
    xenc = _load_cross_encoder()
    logits = xenc.predict(pairs, apply_softmax=False)
    logits = np.array(logits)
    if logits.ndim == 1:
        logits = logits.reshape(1, -1)
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)
    return probs[:, _entailment_idx].tolist()


def score_message_rerank(model, message, all_embeddings, all_texts, all_categories,
                         neg_embeddings=None, neg_texts=None, neutral_embeddings=None,
                         do_spellcheck=True, use_extraction=True):
    """
    Tiered scoring with extraction and gap-based contrastive adjustment:
      spell-correct -> extract -> cosine pass -> NLI rerank -> contrastive gap.

    Returns list of (combined, cosine, entailment, text, category, corrected_msg,
                     corrections, extraction_info, neg_cos)
    sorted by combined score descending.
    """
    # Step 0: spell correction
    corrected = message
    corrections = []
    if do_spellcheck:
        corrected, corrections = correct_spelling(message)

    # Step 1: proposition-guided extraction
    extraction = None
    effective_message = corrected

    if use_extraction:
        extraction = extract_relevant_chunks(model, corrected, all_embeddings, all_texts)
        if extraction['was_extracted']:
            effective_message = extraction['extracted_text']

    # Step 2: cosine pass (without negatives — raw positive scores for ranking)
    cosine_results_extracted, _, _ = score_message(
        model, effective_message, all_embeddings, all_texts, all_categories,
        use_extraction=False)

    if extraction and extraction['was_extracted']:
        cosine_results_full, _, _ = score_message(
            model, corrected, all_embeddings, all_texts, all_categories,
            use_extraction=False)
        cosine_results_window, _, _ = score_message(
            model, extraction['sliding_window_text'], all_embeddings, all_texts,
            all_categories, use_extraction=False)

        score_map = {}
        for score, text, cat in cosine_results_extracted:
            score_map[text] = (max(score, score_map.get(text, (0,))[0]), text, cat)
        for score, text, cat in cosine_results_full:
            if text in score_map:
                score_map[text] = (max(score, score_map[text][0]), text, cat)
        for score, text, cat in cosine_results_window:
            if text in score_map:
                score_map[text] = (max(score, score_map[text][0]), text, cat)

        cosine_results = sorted(score_map.values(), key=lambda x: x[0], reverse=True)
    else:
        cosine_results = cosine_results_extracted

    top_cosine = cosine_results[0][0]

    # Step 3: compute negative and neutral cosine for gap-based adjustment
    emb = model.encode(effective_message)
    neg_cos = 0.0
    if neg_embeddings is not None:
        neg_cos = _compute_max_cosine(model, emb, neg_embeddings)
        if extraction and extraction['was_extracted']:
            emb_full = model.encode(corrected)
            emb_win = model.encode(extraction['sliding_window_text'])
            neg_cos = max(neg_cos,
                          _compute_max_cosine(model, emb_full, neg_embeddings),
                          _compute_max_cosine(model, emb_win, neg_embeddings))
    neutral_cos = 0.0
    if neutral_embeddings is not None:
        neutral_cos = _compute_max_cosine(model, emb, neutral_embeddings)

    # Step 4: check if reranking is needed
    if top_cosine < RERANK_SKIP_BELOW:
        adjusted_results = []
        for s, t, c in cosine_results:
            adj, _ = _contrastive_adjust(s, top_cosine, neg_cos, neutral_cos)
            adjusted_results.append((adj, s, 0.0, t, c, corrected, corrections, extraction, neg_cos))
        return adjusted_results

    # Step 5: NLI cross-encoder rerank top-K
    top_k = cosine_results[:RERANK_TOP_K]

    pairs_fwd = [[effective_message, anchor_text] for _, anchor_text, _ in top_k]
    pairs_bwd = [[anchor_text, effective_message] for _, anchor_text, _ in top_k]

    ent_fwd = _nli_entailment_scores(pairs_fwd)
    ent_bwd = _nli_entailment_scores(pairs_bwd)

    if extraction and extraction['was_extracted'] and extraction['sliding_window_score'] > 0:
        win_text = extraction['sliding_window_text']
        pairs_win_fwd = [[win_text, anchor_text] for _, anchor_text, _ in top_k]
        pairs_win_bwd = [[anchor_text, win_text] for _, anchor_text, _ in top_k]
        ent_win_fwd = _nli_entailment_scores(pairs_win_fwd)
        ent_win_bwd = _nli_entailment_scores(pairs_win_bwd)
        xenc_scores = [max(f, b, wf, wb) for f, b, wf, wb in
                       zip(ent_fwd, ent_bwd, ent_win_fwd, ent_win_bwd)]
    else:
        xenc_scores = [max(f, b) for f, b in zip(ent_fwd, ent_bwd)]

    # Step 6: combine and apply gap-based contrastive adjustment
    combined_top = []
    for i, (cos_s, text, cat) in enumerate(top_k):
        xs = xenc_scores[i]
        raw_combined = RERANK_ALPHA * cos_s + (1 - RERANK_ALPHA) * xs
        adjusted, _ = _contrastive_adjust(raw_combined, top_cosine, neg_cos, neutral_cos)
        combined_top.append((adjusted, cos_s, xs, text, cat, corrected, corrections, extraction, neg_cos))

    combined_top.sort(key=lambda x: x[0], reverse=True)

    rest = []
    for s, t, c in cosine_results[RERANK_TOP_K:]:
        adj, _ = _contrastive_adjust(s, top_cosine, neg_cos, neutral_cos)
        rest.append((adj, s, 0.0, t, c, corrected, corrections, extraction, neg_cos))

    return combined_top + rest


# ---------------------------------------------------------------------------
# NLI-only scoring (--nli mode) with proposition-guided extraction
# ---------------------------------------------------------------------------

def score_message_nli(message, all_texts, all_categories, model=None,
                      all_embeddings=None, neg_texts=None, neg_embeddings=None,
                      neutral_embeddings=None,
                      do_spellcheck=True, use_extraction=True):
    """
    NLI scoring on ALL positive anchors with gap-based contrastive adjustment.

    Positive anchors are scored with NLI (semantic entailment).
    Negative anchors are scored with COSINE (surface similarity) — NLI is too
    semantic and can't distinguish "someone else's account" from "my account".

    The contrastive gap (pos_cos - neg_cos) determines the adjustment:
    - Message closer to positive anchors → keep NLI score
    - Message closer to negative anchors → suppress

    Returns (results, corrected, corrections, extraction_info, neg_cos)
    """
    # Step 0: spell correction
    corrected = message
    corrections = []
    if do_spellcheck:
        corrected, corrections = correct_spelling(message)

    # Step 1: proposition-guided extraction
    extraction = None
    if use_extraction and model is not None and all_embeddings is not None:
        extraction = extract_relevant_chunks(model, corrected, all_embeddings, all_texts)

    _load_cross_encoder()

    best_pos_scores = [0.0] * len(all_texts)
    best_sources = [""] * len(all_texts)

    # Step 2: Build views based on extraction
    views = []

    if extraction and extraction['was_extracted']:
        views.append((extraction['extracted_text'], "[extracted chunk]"))
        if extraction['sliding_window_score'] > 0:
            views.append((extraction['sliding_window_text'], "[sliding window]"))
        for sent in extraction['all_sentences']:
            label = sent if len(sent) <= 40 else sent[:37] + "..."
            views.append((sent, label))
        views.append((corrected, "[full message]"))
    else:
        sentences = _split_into_sentences(corrected)
        for sent in sentences:
            label = sent if len(sent) <= 40 else sent[:37] + "..."
            views.append((sent, label))

    # Step 3: Run NLI on ALL positive anchors for each view
    for view_text, view_label in views:
        pairs_fwd = [[view_text, anchor] for anchor in all_texts]
        pairs_bwd = [[anchor, view_text] for anchor in all_texts]
        ent_fwd = _nli_entailment_scores(pairs_fwd)
        ent_bwd = _nli_entailment_scores(pairs_bwd)

        for i in range(len(all_texts)):
            score = max(ent_fwd[i], ent_bwd[i])
            if score > best_pos_scores[i]:
                best_pos_scores[i] = score
                best_sources[i] = view_label

    # Step 4: Compute COSINE gap for contrastive adjustment
    # Use cosine (not NLI) for negatives — NLI is too semantic for this task
    neg_cos = 0.0
    pos_cos = 0.0
    neutral_cos = 0.0
    if model is not None and all_embeddings is not None:
        from sentence_transformers.util import cos_sim

        # Compute pos, neg, neutral cosine across all views
        for view_text, _ in views:
            emb = model.encode(view_text)
            # Positive cosine
            p_sims = cos_sim(emb, all_embeddings)[0].tolist()
            view_pos_cos = max(p_sims)
            if view_pos_cos > pos_cos:
                pos_cos = view_pos_cos
            # Negative cosine
            if neg_embeddings is not None:
                n_sims = cos_sim(emb, neg_embeddings)[0].tolist()
                view_neg_cos = max(n_sims)
                if view_neg_cos > neg_cos:
                    neg_cos = view_neg_cos
            # Neutral cosine
            if neutral_embeddings is not None:
                nu_sims = cos_sim(emb, neutral_embeddings)[0].tolist()
                view_neutral_cos = max(nu_sims)
                if view_neutral_cos > neutral_cos:
                    neutral_cos = view_neutral_cos

    # Step 5: Apply gap-based contrastive adjustment
    adjusted_scores = []
    for ps in best_pos_scores:
        adj, _ = _contrastive_adjust(ps, pos_cos, neg_cos, neutral_cos)
        adjusted_scores.append(adj)

    # Build results
    results = list(zip(adjusted_scores, all_texts, all_categories, best_sources))
    results.sort(key=lambda x: x[0], reverse=True)

    return results, corrected, corrections, extraction, neg_cos


# ---------------------------------------------------------------------------
# Hybrid scoring (NLI + smart contrastive with NLI confirmation)
# ---------------------------------------------------------------------------

def score_message_hybrid(message, all_texts, all_categories, model=None,
                         all_embeddings=None, neg_texts=None, neg_embeddings=None,
                         neutral_embeddings=None,
                         do_spellcheck=True, use_extraction=True):
    """
    Hybrid scoring: NLI for positives + smart contrastive with NLI confirmation.

    1. Score ALL positive anchors with NLI (high semantic accuracy)
    2. Compute cosine gap between positive and negative anchors
    3. If gap is clear (>= 0.10): keep NLI scores (no ambiguity)
    4. If gap is ambiguous (< 0.10): run NLI on top-K negative anchors
       - If message truly entails a negative anchor → suppress (benign)
       - If message does NOT entail negatives → keep (vocab overlap only)

    This avoids the core problem: cosine can't distinguish intent, but NLI can.
    Running NLI on only top-5 negatives keeps it fast.

    Returns (results, corrected, corrections, extraction_info, neg_cos, debug_info)
    where debug_info is a dict with contrastive details.
    """
    # Step 0: spell correction
    corrected = message
    corrections = []
    if do_spellcheck:
        corrected, corrections = correct_spelling(message)

    # Step 1: proposition-guided extraction
    extraction = None
    if use_extraction and model is not None and all_embeddings is not None:
        extraction = extract_relevant_chunks(model, corrected, all_embeddings, all_texts)

    _load_cross_encoder()

    best_pos_scores = [0.0] * len(all_texts)
    best_sources = [""] * len(all_texts)

    # Step 2: Build views based on extraction
    views = []
    if extraction and extraction['was_extracted']:
        views.append((extraction['extracted_text'], "[extracted chunk]"))
        if extraction['sliding_window_score'] > 0:
            views.append((extraction['sliding_window_text'], "[sliding window]"))
        for sent in extraction['all_sentences']:
            label = sent if len(sent) <= 40 else sent[:37] + "..."
            views.append((sent, label))
        views.append((corrected, "[full message]"))
    else:
        sentences = _split_into_sentences(corrected)
        for sent in sentences:
            label = sent if len(sent) <= 40 else sent[:37] + "..."
            views.append((sent, label))

    # Step 3: NLI on ALL positive anchors
    for view_text, view_label in views:
        pairs_fwd = [[view_text, anchor] for anchor in all_texts]
        pairs_bwd = [[anchor, view_text] for anchor in all_texts]
        ent_fwd = _nli_entailment_scores(pairs_fwd)
        ent_bwd = _nli_entailment_scores(pairs_bwd)
        for i in range(len(all_texts)):
            score = max(ent_fwd[i], ent_bwd[i])
            if score > best_pos_scores[i]:
                best_pos_scores[i] = score
                best_sources[i] = view_label

    # Step 4: Cosine gap for contrastive gate
    neg_cos = 0.0
    pos_cos = 0.0
    neutral_cos = 0.0
    debug = {"method": "hybrid", "gap": 0.0, "neg_nli": 0.0, "action": "none"}

    if model is not None and all_embeddings is not None:
        from sentence_transformers.util import cos_sim
        for view_text, _ in views:
            emb = model.encode(view_text)
            p_sims = cos_sim(emb, all_embeddings)[0].tolist()
            view_pos_cos = max(p_sims)
            if view_pos_cos > pos_cos:
                pos_cos = view_pos_cos
            if neg_embeddings is not None:
                n_sims = cos_sim(emb, neg_embeddings)[0].tolist()
                view_neg_cos = max(n_sims)
                if view_neg_cos > neg_cos:
                    neg_cos = view_neg_cos
            if neutral_embeddings is not None:
                nu_sims = cos_sim(emb, neutral_embeddings)[0].tolist()
                view_neutral_cos = max(nu_sims)
                if view_neutral_cos > neutral_cos:
                    neutral_cos = view_neutral_cos

    gap = pos_cos - neg_cos
    debug["gap"] = gap
    debug["pos_cos"] = pos_cos
    debug["neg_cos"] = neg_cos

    # Step 5: Smart contrastive adjustment
    adjusted_scores = list(best_pos_scores)

    # Off-topic check
    if neutral_cos > 0.4 and neutral_cos > pos_cos and neutral_cos > neg_cos:
        adjusted_scores = [s * 0.15 for s in best_pos_scores]
        debug["action"] = "neutral_suppress"

    elif neg_cos < 0.4:
        # Negatives not relevant — keep raw NLI scores
        debug["action"] = "no_neg_signal"

    elif gap >= 0.10:
        # Clearly closer to positive anchors — keep raw NLI scores
        debug["action"] = "clear_positive"

    else:
        # Ambiguous zone — run NLI on top-K negative anchors for confirmation
        NEG_CONFIRM_K = 5
        # Pick the closest negatives by cosine
        if model is not None and neg_embeddings is not None and neg_texts:
            from sentence_transformers.util import cos_sim
            # Use the best view embedding for neg selection
            best_view_text = views[0][0]
            for vt, _ in views:
                emb_v = model.encode(vt)
                pc = max(cos_sim(emb_v, all_embeddings)[0].tolist())
                if pc >= pos_cos:
                    best_view_text = vt
                    break

            emb_best = model.encode(best_view_text)
            neg_sims_all = cos_sim(emb_best, neg_embeddings)[0].tolist()
            top_neg_indices = sorted(range(len(neg_sims_all)),
                                     key=lambda i: neg_sims_all[i], reverse=True)[:NEG_CONFIRM_K]
            top_neg_texts = [neg_texts[i] for i in top_neg_indices]

            # NLI on top-K negatives (both directions)
            pairs_fwd = [[best_view_text, nt] for nt in top_neg_texts]
            pairs_bwd = [[nt, best_view_text] for nt in top_neg_texts]
            neg_ent_fwd = _nli_entailment_scores(pairs_fwd)
            neg_ent_bwd = _nli_entailment_scores(pairs_bwd)
            best_neg_nli = max(max(f, b) for f, b in zip(neg_ent_fwd, neg_ent_bwd))
            debug["neg_nli"] = best_neg_nli
            debug["neg_nli_texts"] = top_neg_texts

            if best_neg_nli >= 0.80:
                # Message truly entails a negative anchor — strong suppression
                penalty = 0.50 + (best_neg_nli - 0.80) * 2.5  # 0.50 to 1.0
                penalty = min(0.95, penalty)
                adjusted_scores = [s * (1 - penalty) for s in best_pos_scores]
                debug["action"] = "nli_confirmed_suppress"
            elif best_neg_nli >= 0.50:
                # Moderate NLI — partial fade
                fade = (best_neg_nli - 0.50) / 0.30  # 0..1
                penalty = fade * 0.50
                adjusted_scores = [s * (1 - penalty) for s in best_pos_scores]
                debug["action"] = "nli_partial_suppress"
            else:
                # NLI says negatives don't match — keep scores
                # But apply mild cosine-based fade if gap is negative
                if gap < 0:
                    mild_penalty = min(0.20, abs(gap) * 1.5)
                    adjusted_scores = [s * (1 - mild_penalty) for s in best_pos_scores]
                    debug["action"] = "nli_cleared_mild_cos"
                else:
                    debug["action"] = "nli_cleared"
        else:
            # No model or negatives — fall back to basic gap
            adjusted_scores = list(best_pos_scores)
            debug["action"] = "no_neg_data"

    results = list(zip(adjusted_scores, all_texts, all_categories, best_sources))
    results.sort(key=lambda x: x[0], reverse=True)

    return results, corrected, corrections, extraction, neg_cos, debug


# ---------------------------------------------------------------------------
# LLM Judge scoring
# ---------------------------------------------------------------------------

_llm_config = None

def _load_llm_config(silent=False):
    """Load LLM judge config from config_<n>.ini."""
    global _llm_config
    if _llm_config is not None:
        return _llm_config

    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, "config_{}.ini".format(SCRIPT_NAME))

    if not os.path.exists(config_path):
        if not silent:
            print("\n  {}ERROR: LLM judge config not found: {}{}".format(
                "\033[91m", config_path, RESET))
            print("  Create it with:")
            print("    [llm_judge]")
            print("    provider = anthropic")
            print("    model = claude-sonnet-4-20250514")
            print("    api_key = sk-ant-...")
            print("    proposition = Your proposition here")
            print("\n  Or regenerate with: python semantic_anchor_generator.py -name {} -gs".format(
                SCRIPT_NAME))
        sys.exit(1)

    import configparser
    config = configparser.ConfigParser()
    config.read(config_path)

    _llm_config = {
        "provider": config.get("llm_judge", "provider"),
        "model": config.get("llm_judge", "model"),
        "api_key": config.get("llm_judge", "api_key"),
        "proposition": config.get("llm_judge", "proposition", fallback=PROPOSITION),
        "base_url": config.get("llm_judge", "base_url",
                               fallback="http://localhost:11434"),
    }
    return _llm_config


def _call_llm_judge(message, proposition, top_anchors=None):
    """
    Call an LLM to judge whether a message matches the proposition.

    Returns dict: {match: bool, confidence: float, reasoning: str, verdict: str}
    """
    config = _load_llm_config()
    provider = config["provider"].lower()
    model = config["model"]
    api_key = config["api_key"]

    # Build the prompt
    anchors_context = ""
    if top_anchors:
        anchor_lines = []
        for score, text, cat, *rest in top_anchors[:5]:
            anchor_lines.append("  - [{:.3f}] ({}) \"{}\"".format(score, cat, text))
        anchors_context = (
            "\n\nFor reference, these are the closest semantic anchor matches "
            "(pre-scored by embedding similarity):\n" + "\n".join(anchor_lines)
        )

    system_prompt = (
        "You are a precise content classifier. Your task is to determine whether "
        "a user message matches a given proposition. You must distinguish between:\n"
        "- Messages that genuinely request/intend what the proposition describes\n"
        "- Messages that use similar vocabulary but have legitimate/benign intent\n"
        "- Educational questions ABOUT the topic (not requesting to perform it)\n\n"
        "Respond with ONLY valid JSON, no other text."
    )

    user_prompt = (
        'PROPOSITION: "{}"\n\n'
        'USER MESSAGE: "{}"\n'
        '{}\n'
        'Does this message match the proposition? Consider:\n'
        '1. Does the user intend to perform the action described in the proposition?\n'
        '2. Or are they asking for education, defense, their own account, or general info?\n'
        '3. First-person ownership ("my account", "my password") vs third-party targeting\n\n'
        'Respond with JSON:\n'
        '{{\n'
        '  "match": true/false,\n'
        '  "confidence": 0.0 to 1.0,\n'
        '  "reasoning": "brief explanation"\n'
        '}}'
    ).format(proposition, message, anchors_context)

    try:
        import httpx

        if provider == "anthropic":
            resp = httpx.post(
                "https://api.anthropic.com/v1/messages",
                headers={
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json",
                },
                json={
                    "model": model,
                    "max_tokens": 300,
                    "messages": [{"role": "user", "content": user_prompt}],
                    "system": system_prompt,
                },
                timeout=30.0,
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["content"][0]["text"]

        elif provider == "openai":
            resp = httpx.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": "Bearer " + api_key,
                    "Content-Type": "application/json",
                },
                json={
                    "model": model,
                    "max_tokens": 300,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    "temperature": 0,
                },
                timeout=30.0,
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["choices"][0]["message"]["content"]

        elif provider == "gemini" or provider == "google":
            # Google Gemini API
            url = "https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent?key={}".format(
                model, api_key)
            resp = httpx.post(
                url,
                headers={"Content-Type": "application/json"},
                json={
                    "contents": [{"parts": [{"text": system_prompt + "\n\n" + user_prompt}]}],
                    "generationConfig": {
                        "maxOutputTokens": 300,
                        "temperature": 0,
                    },
                },
                timeout=30.0,
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["candidates"][0]["content"]["parts"][0]["text"]

        elif provider == "grok" or provider == "xai":
            # xAI Grok API (OpenAI-compatible)
            resp = httpx.post(
                "https://api.x.ai/v1/chat/completions",
                headers={
                    "Authorization": "Bearer " + api_key,
                    "Content-Type": "application/json",
                },
                json={
                    "model": model,
                    "max_tokens": 300,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    "temperature": 0,
                },
                timeout=30.0,
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["choices"][0]["message"]["content"]

        elif provider in ("ollama", "local", "lmstudio", "vllm"):
            # Local model via Ollama or any OpenAI-compatible local server
            base_url = config.get("base_url", "http://localhost:11434")
            if provider == "ollama":
                # Ollama native API
                resp = httpx.post(
                    base_url.rstrip("/") + "/api/chat",
                    json={
                        "model": model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        "stream": False,
                        "options": {"temperature": 0},
                    },
                    timeout=120.0,
                )
                resp.raise_for_status()
                data = resp.json()
                text = data["message"]["content"]
            else:
                # LM Studio / vLLM / any OpenAI-compatible local server
                resp = httpx.post(
                    base_url.rstrip("/") + "/v1/chat/completions",
                    headers={"Content-Type": "application/json"},
                    json={
                        "model": model,
                        "max_tokens": 300,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        "temperature": 0,
                    },
                    timeout=120.0,
                )
                resp.raise_for_status()
                data = resp.json()
                text = data["choices"][0]["message"]["content"]

        else:
            return {"match": False, "confidence": 0.0,
                    "reasoning": "Unsupported provider: " + provider,
                    "verdict": "ERROR"}

        # Parse JSON response
        import re
        json_match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
        else:
            result = json.loads(text)

        is_match = result.get("match", False)
        confidence = float(result.get("confidence", 0.0))
        reasoning = result.get("reasoning", "")

        if is_match and confidence >= 0.7:
            verdict = "MATCH"
        elif is_match and confidence >= 0.4:
            verdict = "WARNING"
        else:
            verdict = "NO MATCH"

        return {
            "match": is_match,
            "confidence": confidence,
            "reasoning": reasoning,
            "verdict": verdict,
        }

    except Exception as e:
        return {
            "match": False,
            "confidence": 0.0,
            "reasoning": "LLM call failed: {}".format(str(e)),
            "verdict": "ERROR",
        }


def score_message_llm(message, proposition, model=None, all_embeddings=None,
                      all_texts=None, all_categories=None,
                      neg_embeddings=None, neutral_embeddings=None,
                      do_spellcheck=True, use_extraction=True):
    """
    LLM-as-judge scoring. Optionally uses cosine pre-ranking to provide
    context anchors to the LLM.

    Returns (llm_result, corrected, corrections, cosine_results)
    where llm_result is the dict from _call_llm_judge.
    """
    corrected = message
    corrections = []
    if do_spellcheck:
        corrected, corrections = correct_spelling(message)

    # Optional: cosine pre-rank for context
    cosine_results = None
    if model is not None and all_embeddings is not None:
        cosine_results, _, _ = score_message(
            model, corrected, all_embeddings, all_texts, all_categories,
            neg_embeddings=neg_embeddings, neutral_embeddings=neutral_embeddings,
            use_extraction=use_extraction)

    llm_result = _call_llm_judge(corrected, proposition, top_anchors=cosine_results)

    return llm_result, corrected, corrections, cosine_results


# ---------------------------------------------------------------------------
# Display
# ---------------------------------------------------------------------------

def print_banner():
    title = "SEMANTIC ANCHORS: " + SCRIPT_NAME.upper()
    w = 60
    print("")
    print(BOLD + "\u2554" + "\u2550" * w + "\u2557")
    print("\u2551" + title.center(w) + "\u2551")
    print("\u255a" + "\u2550" * w + "\u255d" + RESET)
    print("  Proposition: \"{}\"".format(PROPOSITION))
    print("")


def print_examples():
    total = sum(len(v) for v in ANCHORS.values())
    print("\n{}Positive Anchors ({} total){}".format(BOLD, total, RESET))
    print("=" * 70)
    for cat, examples in ANCHORS.items():
        print("\n{}{}  [{}]{} ({} examples)".format(BOLD, BLUE, cat, RESET, len(examples)))
        print("  " + "-" * 60)
        for i, ex in enumerate(examples, 1):
            print("  {}{:>3}.{} {}".format(DIM, i, RESET, ex))

    if NEGATIVE_ANCHORS:
        neg_total = sum(len(v) for v in NEGATIVE_ANCHORS.values())
        print("\n\n{}Negative (Contrastive) Anchors ({} total){}".format(BOLD, neg_total, RESET))
        print("=" * 70)
        for cat, examples in NEGATIVE_ANCHORS.items():
            print("\n{}{}  [{}]{} ({} examples)".format(BOLD, GREEN, cat, RESET, len(examples)))
            print("  " + "-" * 60)
            for i, ex in enumerate(examples, 1):
                print("  {}{:>3}.{} {}".format(DIM, i, RESET, ex))

    if NEUTRAL_ANCHORS:
        print("\n\n{}Neutral (Off-topic Baseline) Anchors ({} total){}".format(
            BOLD, len(NEUTRAL_ANCHORS), RESET))
        print("=" * 70)
        for i, ex in enumerate(NEUTRAL_ANCHORS, 1):
            print("  {}{:>3}.{} {}".format(DIM, i, RESET, ex))
    print()


def format_verdict(top_score, neg_cos=0.0):
    neg_info = ""
    if neg_cos > 0.01:
        neg_info = "  {}[neg_cos={:.3f}]{}".format(DIM, neg_cos, RESET)
    if top_score >= MATCH_THRESHOLD:
        return "{}{}\u25a0 MATCH{} (score {:.4f} \u2265 {}){}".format(
            RED, BOLD, RESET, top_score, MATCH_THRESHOLD, neg_info)
    elif top_score >= WARNING_THRESHOLD:
        return "{}{}\u25a0 WARNING{} (score {:.4f} \u2265 {}){}".format(
            YELLOW, BOLD, RESET, top_score, WARNING_THRESHOLD, neg_info)
    else:
        return "{}{}\u25a0 NO MATCH{} (score {:.4f} < {}){}".format(
            GREEN, BOLD, RESET, top_score, WARNING_THRESHOLD, neg_info)


# --- Cosine display ---

def display_default(results, extraction=None, top_n=3, verbose_extract=False, neg_score=0.0):
    if extraction:
        _display_extraction_info(extraction, verbose=verbose_extract)
    print("\n  {}\n".format(format_verdict(results[0][0], neg_score)))
    print("  {:<6} {:>8}  {:<35} {}".format("Rank", "Score", "Category", "Nearest Anchor"))
    print("  " + "-" * 95)
    for rank, (score, text, cat) in enumerate(results[:top_n], 1):
        cs = color_score(score)
        dt = text if len(text) <= 55 else text[:52] + "..."
        dc = cat if len(cat) <= 33 else cat[:30] + "..."
        print("  {:<6} {:>17}  {}{:<35}{} \"{}\"".format(rank, cs, DIM, dc, RESET, dt))
    print()


def display_verbose(results, extraction=None, neg_score=0.0):
    if extraction:
        _display_extraction_info(extraction, verbose=True)
    print("\n  {}\n".format(format_verdict(results[0][0], neg_score)))
    print("  {:<5} {:>8}  {:<35} {}".format("#", "Score", "Category", "Anchor Text"))
    print("  " + "=" * 100)
    for rank, (score, text, cat) in enumerate(results, 1):
        cs = color_score(score)
        dc = cat if len(cat) <= 33 else cat[:30] + "..."
        zone = ""
        if score >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif score >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17}  {}{:<35}{} \"{}\"{}".format(
            rank, cs, DIM, dc, RESET, text, zone))
    above_m = sum(1 for s, _, _ in results if s >= MATCH_THRESHOLD)
    above_w = sum(1 for s, _, _ in results if s >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {}{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w, RESET))
    print()


# --- Reranked display ---

def _show_corrections_rerank(results):
    if not results:
        return
    corrections = results[0][6]
    corrected = results[0][5]
    extraction = results[0][7] if len(results[0]) > 7 else None
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
        print("  {}Evaluating:{} \"{}\"".format(DIM, RESET, corrected))
    if extraction:
        _display_extraction_info(extraction)


def display_default_rerank(results, top_n=3):
    top_combined = results[0][0]
    neg_score = results[0][8] if len(results[0]) > 8 else 0.0
    _show_corrections_rerank(results)
    print("\n  {}\n".format(format_verdict(top_combined, neg_score)))
    print("  {:<6} {:>8} {:>10} {:>10}  {:<30} {}".format(
        "Rank", "Combined", "Cosine", "CrossEnc", "Category", "Nearest Anchor"))
    print("  " + "-" * 110)
    for rank, item in enumerate(results[:top_n], 1):
        combined, cosine, xenc, text, cat = item[0], item[1], item[2], item[3], item[4]
        cs = color_score(combined)
        dt = text if len(text) <= 45 else text[:42] + "..."
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        xe_s = "{:.4f}".format(xenc) if xenc > 0 else DIM + "  -- " + RESET
        print("  {:<6} {:>17} {:>10.4f} {:>10}  {}{:<30}{} \"{}\"".format(
            rank, cs, cosine, xe_s, DIM, dc, RESET, dt))
    print()


def display_verbose_rerank(results):
    top_combined = results[0][0]
    neg_score = results[0][8] if len(results[0]) > 8 else 0.0
    _show_corrections_rerank(results)
    print("\n  {}\n".format(format_verdict(top_combined, neg_score)))
    print("  {:<5} {:>8} {:>10} {:>10}  {:<30} {}".format(
        "#", "Combined", "Cosine", "CrossEnc", "Category", "Anchor Text"))
    print("  " + "=" * 115)
    for rank, item in enumerate(results, 1):
        combined, cosine, xenc, text, cat = item[0], item[1], item[2], item[3], item[4]
        cs = color_score(combined)
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        xe_s = "{:.4f}".format(xenc) if xenc > 0 else DIM + "  -- " + RESET
        zone = ""
        if combined >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif combined >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17} {:>10.4f} {:>10}  {}{:<30}{} \"{}\"{}".format(
            rank, cs, cosine, xe_s, DIM, dc, RESET, text, zone))
    above_m = sum(1 for r in results if r[0] >= MATCH_THRESHOLD)
    above_w = sum(1 for r in results if r[0] >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {} | "
          "Cross-encoder on top {}{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w,
        RERANK_TOP_K, RESET))
    print()


# --- NLI display ---

def display_default_nli(results, corrected, corrections, extraction=None, top_n=3, neg_score=0.0):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction)
    print("\n  {}\n".format(format_verdict(top_score, neg_score)))
    print("  {:<6} {:>8}  {:<30} {}".format(
        "Rank", "NLI", "Category", "Nearest Anchor"))
    print("  " + "-" * 100)
    for rank, (score, text, cat, best_src) in enumerate(results[:top_n], 1):
        cs = color_score(score)
        dt = text if len(text) <= 50 else text[:47] + "..."
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        print("  {:<6} {:>17}  {}{:<30}{} \"{}\"".format(
            rank, cs, DIM, dc, RESET, dt))
        if best_src and best_src not in (corrected, "[full message]"):
            bs = best_src if len(best_src) <= 65 else best_src[:62] + "..."
            print("  {}       matched via: {}{} ".format(DIM, bs, RESET))
    print()


def display_verbose_nli(results, corrected, corrections, extraction=None, neg_score=0.0):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction, verbose=True)
    print("\n  {}\n".format(format_verdict(top_score, neg_score)))
    print("  {:<5} {:>8}  {:<30} {}".format("#", "NLI", "Category", "Anchor Text"))
    print("  " + "=" * 105)
    for rank, (score, text, cat, best_src) in enumerate(results, 1):
        cs = color_score(score)
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        zone = ""
        if score >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif score >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17}  {}{:<30}{} \"{}\"{}".format(
            rank, cs, DIM, dc, RESET, text, zone))
    above_m = sum(1 for s, _, _, _ in results if s >= MATCH_THRESHOLD)
    above_w = sum(1 for s, _, _, _ in results if s >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {} | "
          "NLI on ALL anchors{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w,
        RESET))
    print()


# --- Hybrid display ---

def display_default_hybrid(results, corrected, corrections, extraction=None,
                           top_n=3, neg_score=0.0, debug=None):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction)

    # Show contrastive debug
    if debug:
        action = debug.get("action", "")
        neg_nli = debug.get("neg_nli", 0)
        gap = debug.get("gap", 0)
        detail = "gap={:.3f} action={}".format(gap, action)
        if neg_nli > 0:
            detail += " neg_nli={:.3f}".format(neg_nli)
        print("  {}Hybrid: {}{}".format(DIM, detail, RESET))

    print("\n  {}\n".format(format_verdict(top_score, neg_score)))
    print("  {:<6} {:>8}  {:<30} {}".format(
        "Rank", "NLI", "Category", "Nearest Anchor"))
    print("  " + "-" * 100)
    for rank, (score, text, cat, best_src) in enumerate(results[:top_n], 1):
        cs = color_score(score)
        dt = text if len(text) <= 50 else text[:47] + "..."
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        print("  {:<6} {:>17}  {}{:<30}{} \"{}\"".format(
            rank, cs, DIM, dc, RESET, dt))
        if best_src and best_src not in (corrected, "[full message]"):
            bs = best_src if len(best_src) <= 65 else best_src[:62] + "..."
            print("  {}       matched via: {}{} ".format(DIM, bs, RESET))
    print()


def display_verbose_hybrid(results, corrected, corrections, extraction=None,
                           neg_score=0.0, debug=None):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction, verbose=True)

    if debug:
        action = debug.get("action", "")
        neg_nli = debug.get("neg_nli", 0)
        gap = debug.get("gap", 0)
        detail = "pos_cos={:.3f} neg_cos={:.3f} gap={:.3f} action={}".format(
            debug.get("pos_cos", 0), debug.get("neg_cos", 0), gap, action)
        if neg_nli > 0:
            detail += " neg_nli={:.3f}".format(neg_nli)
        print("  {}Hybrid debug: {}{}".format(DIM, detail, RESET))

    print("\n  {}\n".format(format_verdict(top_score, neg_score)))
    print("  {:<5} {:>8}  {:<30} {}".format("#", "NLI", "Category", "Anchor Text"))
    print("  " + "=" * 105)
    for rank, (score, text, cat, best_src) in enumerate(results, 1):
        cs = color_score(score)
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        zone = ""
        if score >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif score >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17}  {}{:<30}{} \"{}\"{}".format(
            rank, cs, DIM, dc, RESET, text, zone))
    above_m = sum(1 for s, _, _, _ in results if s >= MATCH_THRESHOLD)
    above_w = sum(1 for s, _, _, _ in results if s >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {} | "
          "Hybrid NLI + smart contrastive{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w,
        RESET))
    print()


# --- LLM Judge display ---

def display_llm_result(llm_result, corrected, corrections, cosine_results=None, top_n=3):
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))

    verdict = llm_result.get("verdict", "ERROR")
    confidence = llm_result.get("confidence", 0.0)
    reasoning = llm_result.get("reasoning", "")

    if verdict == "MATCH":
        vcolor = RED
    elif verdict == "WARNING":
        vcolor = YELLOW
    elif verdict == "ERROR":
        vcolor = "\033[91m"
    else:
        vcolor = GREEN

    print("\n  {}{}\u25a0 {}{} (confidence: {:.2f})".format(
        vcolor, BOLD, verdict, RESET, confidence))
    if reasoning:
        # Word-wrap reasoning at ~80 chars
        words = reasoning.split()
        lines = []
        line = "  "
        for w in words:
            if len(line) + len(w) + 1 > 82:
                lines.append(line)
                line = "  "
            line += w + " "
        if line.strip():
            lines.append(line)
        print("  {}Reasoning:{} {}".format(DIM, RESET, lines[0].strip()))
        for l in lines[1:]:
            print("  {}".format(l.rstrip()))

    if cosine_results:
        print("\n  {}Cosine context (for reference):{}".format(DIM, RESET))
        print("  {:<6} {:>8}  {:<30} {}".format("Rank", "Cosine", "Category", "Nearest Anchor"))
        print("  " + "-" * 90)
        for rank, (score, text, cat, *rest) in enumerate(cosine_results[:top_n], 1):
            cs = color_score(score)
            dt = text if len(text) <= 50 else text[:47] + "..."
            dc = cat if len(cat) <= 28 else cat[:25] + "..."
            print("  {:<6} {:>17}  {}{:<30}{} \"{}\"".format(
                rank, cs, DIM, dc, RESET, dt))
    print()


# ---------------------------------------------------------------------------
# Compare mode — run all scoring methods side-by-side
# ---------------------------------------------------------------------------

def _verdict_for_score(score):
    """Return verdict string for a numeric score."""
    if score >= MATCH_THRESHOLD:
        return "MATCH"
    elif score >= WARNING_THRESHOLD:
        return "WARNING"
    return "NO MATCH"


def _verdict_color(verdict):
    if verdict == "MATCH":
        return RED
    elif verdict == "WARNING":
        return YELLOW
    elif verdict == "ERROR":
        return "\033[91m"
    return GREEN


def _format_top3(results, is_llm=False):
    """Format top-3 anchors as a compact string."""
    if is_llm or not results:
        return ""
    parts = []
    for score, text, cat, *rest in results[:3]:
        dt = text if len(text) <= 30 else text[:27] + "..."
        parts.append("[{:.3f}] \"{}\"".format(score, dt))
    return "  ".join(parts)


def score_all_modes(message, model, embs, texts, cats,
                    neg_embs=None, neg_texts=None, neutral_embs=None,
                    use_extraction=True, include_llm=False):
    """
    Run all scoring modes on a single message.

    Returns dict of mode_name -> {
        verdict: str, score: float, top3: list,
        reasoning: str (llm only), neg_cos: float
    }
    """
    results = {}

    # --- Cosine ---
    cos_results, cos_extraction, cos_neg = score_message(
        model, message, embs, texts, cats,
        neg_embeddings=neg_embs, neg_texts=neg_texts,
        neutral_embeddings=neutral_embs,
        use_extraction=use_extraction)
    top_cos = cos_results[0][0]
    results["cosine"] = {
        "verdict": _verdict_for_score(top_cos),
        "score": top_cos,
        "top3": cos_results[:3],
        "neg_cos": cos_neg,
    }

    # --- NLI ---
    nli_results, nli_corr, nli_corrections, nli_ext, nli_neg = score_message_nli(
        message, texts, cats, model=model, all_embeddings=embs,
        neg_texts=neg_texts, neg_embeddings=neg_embs,
        neutral_embeddings=neutral_embs,
        use_extraction=use_extraction)
    top_nli = nli_results[0][0]
    results["nli"] = {
        "verdict": _verdict_for_score(top_nli),
        "score": top_nli,
        "top3": nli_results[:3],
        "neg_cos": nli_neg,
        "corrections": nli_corrections,
    }

    # --- Hybrid ---
    hyb_results, hyb_corr, hyb_corrections, hyb_ext, hyb_neg, hyb_debug = score_message_hybrid(
        message, texts, cats, model=model, all_embeddings=embs,
        neg_texts=neg_texts, neg_embeddings=neg_embs,
        neutral_embeddings=neutral_embs,
        use_extraction=use_extraction)
    top_hyb = hyb_results[0][0]
    results["hybrid"] = {
        "verdict": _verdict_for_score(top_hyb),
        "score": top_hyb,
        "top3": hyb_results[:3],
        "neg_cos": hyb_neg,
        "debug": hyb_debug,
    }

    # --- LLM (optional) ---
    if include_llm:
        llm_result, llm_corr, llm_corrections, llm_cos = score_message_llm(
            message, PROPOSITION, model=model, all_embeddings=embs,
            all_texts=texts, all_categories=cats,
            neg_embeddings=neg_embs, neutral_embeddings=neutral_embs,
            use_extraction=use_extraction)
        results["llm"] = {
            "verdict": llm_result.get("verdict", "ERROR"),
            "score": llm_result.get("confidence", 0.0),
            "top3": [],
            "reasoning": llm_result.get("reasoning", ""),
            "neg_cos": 0.0,
        }

    return results


def display_compare(message, mode_results, index=None, total=None):
    """Display side-by-side comparison table for one message."""
    # Header
    if index is not None and total is not None:
        disp = message if len(message) <= 80 else message[:77] + "..."
        print("\n  {}[{}/{}]{} \"{}\"".format(BOLD, index, total, RESET, disp))
    else:
        disp = message if len(message) <= 90 else message[:87] + "..."
        print("\n  \"{}\"".format(disp))

    # Spell corrections (from NLI — same for all)
    corrections = mode_results.get("nli", {}).get("corrections", [])
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))

    # === Verdict summary row ===
    print()
    mode_order = ["cosine", "nli", "hybrid", "llm"]
    active_modes = [m for m in mode_order if m in mode_results]

    # Build column widths
    col_w = 26
    header = "  {:<12}".format("")
    for m in active_modes:
        header += " {:^{}}".format(m.upper(), col_w)
    print(BOLD + header + RESET)
    print("  " + "\u2500" * (12 + (col_w + 1) * len(active_modes)))

    # Verdict row
    row_verdict = "  {:<12}".format("Verdict")
    for m in active_modes:
        v = mode_results[m]["verdict"]
        vc = _verdict_color(v)
        cell = "{}{}\u25a0 {}{}".format(vc, BOLD, v, RESET)
        # Pad for alignment (color codes don't take visual space)
        pad = col_w - len("\u25a0 " + v)
        row_verdict += " " + cell + " " * max(0, pad)
    print(row_verdict)

    # Score row
    row_score = "  {:<12}".format("Score")
    for m in active_modes:
        s = mode_results[m]["score"]
        cell = "{:.4f}".format(s)
        row_score += " {:^{}}".format(cell, col_w)
    print(row_score)

    # Neg cosine row
    row_neg = "  {:<12}".format("Neg Cosine")
    for m in active_modes:
        nc = mode_results[m].get("neg_cos", 0.0)
        cell = "{:.3f}".format(nc) if nc > 0.01 else "--"
        row_neg += " {:^{}}".format(cell, col_w)
    print(row_neg)

    print("  " + "\u2500" * (12 + (col_w + 1) * len(active_modes)))

    # === Top 3 anchors per mode ===
    print("  {}Top 3 Anchors:{}".format(BOLD, RESET))

    for rank in range(3):
        row = "  {:<12}".format("  #{}".format(rank + 1))
        for m in active_modes:
            mr = mode_results[m]
            is_llm = (m == "llm")
            if is_llm:
                if rank == 0:
                    reasoning = mr.get("reasoning", "")
                    cell = reasoning if len(reasoning) <= (col_w - 2) else reasoning[:col_w - 5] + "..."
                else:
                    cell = ""
            else:
                top3 = mr.get("top3", [])
                if rank < len(top3):
                    s, t, c, *rest = top3[rank]
                    max_t = col_w - 9  # space for score "[0.XXX] "
                    dt = t if len(t) <= max_t else t[:max_t - 3] + "..."
                    cell = "{:.3f} \"{}\"".format(s, dt)
                    if len(cell) > col_w:
                        cell = cell[:col_w - 3] + "..."
                else:
                    cell = ""
            row += " {:<{}}".format(cell, col_w)
        print(row)

    # Category for top anchor per mode
    row_cat = "  {:<12}".format("  Category")
    for m in active_modes:
        if m == "llm":
            row_cat += " {:<{}}".format("", col_w)
        else:
            top3 = mode_results[m].get("top3", [])
            if top3:
                cat = top3[0][2]
                dc = cat if len(cat) <= (col_w - 2) else cat[:col_w - 5] + "..."
                row_cat += " {}{:<{}}{}".format(DIM, dc, col_w, RESET)
            else:
                row_cat += " {:<{}}".format("", col_w)
    print(row_cat)

    # Hybrid debug line
    hyb = mode_results.get("hybrid", {})
    debug = hyb.get("debug")
    if debug:
        action = debug.get("action", "")
        neg_nli = debug.get("neg_nli", 0)
        gap = debug.get("gap", 0)
        detail_parts = ["gap={:.3f}".format(gap), action]
        if neg_nli > 0:
            detail_parts.append("neg_nli={:.3f}".format(neg_nli))
        print("  {}Hybrid: {}{}".format(DIM, " ".join(detail_parts), RESET))

    print()


def display_compare_summary(all_mode_results, messages):
    """Display final summary grid across all messages and modes."""
    modes = ["cosine", "nli", "hybrid"]
    if any("llm" in mr for mr in all_mode_results):
        modes.append("llm")

    # Count per mode
    counts = {}
    for m in modes:
        counts[m] = {"MATCH": 0, "WARNING": 0, "NO MATCH": 0, "ERROR": 0}

    for mr in all_mode_results:
        for m in modes:
            if m in mr:
                v = mr[m]["verdict"]
                counts[m][v] = counts[m].get(v, 0) + 1

    total = len(messages)

    print("\n  {}Comparison Summary ({} messages){}".format(BOLD, total, RESET))
    print("  " + "=" * 75)
    print("  {}{:<10} {:>10} {:>10} {:>10} {:>10} {:>12}{}".format(
        BOLD, "Mode", "Matches", "Warnings", "No Match", "Errors", "Accuracy*", RESET))
    print("  " + "-" * 75)

    for m in modes:
        c = counts[m]
        matches = c["MATCH"]
        warns = c["WARNING"]
        clean = c["NO MATCH"]
        errs = c["ERROR"]

        # Flagged = matches + warnings
        flagged = matches + warns
        flagged_pct = flagged / total * 100 if total > 0 else 0

        print("  {:<10} {}{:>10}{} {}{:>10}{} {}{:>10}{} {:>10} {:>11.1f}%".format(
            m.upper(),
            RED, matches, RESET,
            YELLOW, warns, RESET,
            GREEN, clean, RESET,
            errs,
            flagged_pct))

    print()
    print("  {}* Accuracy = (matches + warnings) / total — compare with your ground truth{}".format(
        DIM, RESET))

    # Per-message agreement table
    print("\n  {}Per-Message Verdict Grid{}".format(BOLD, RESET))
    print("  " + "-" * 95)

    # Header row
    hdr = "  {:<4} {:<50}".format("#", "Message")
    for m in modes:
        hdr += " {:>10}".format(m.upper())
    print(hdr)
    print("  " + "-" * 95)

    for i, (msg, mr) in enumerate(zip(messages, all_mode_results), 1):
        disp = msg if len(msg) <= 48 else msg[:45] + "..."
        row = "  {:<4} {:<50}".format(i, disp)
        for m in modes:
            if m in mr:
                v = mr[m]["verdict"]
                vc = _verdict_color(v)
                short = v[:5] if v != "NO MATCH" else "CLEAN"
                row += " {}  {:>6}  {}".format(vc, short, RESET)
            else:
                row += " {:>10}".format("--")
        print(row)

    # Agreement stats
    if len(modes) >= 2:
        agree = 0
        for mr in all_mode_results:
            verdicts = set()
            for m in modes:
                if m in mr:
                    verdicts.add(mr[m]["verdict"])
            if len(verdicts) <= 1:
                agree += 1
        print("\n  {}All-mode agreement: {}/{} ({:.1f}%){}".format(
            DIM, agree, total, agree / total * 100 if total > 0 else 0, RESET))

    print()


def run_compare(filepath, model, embs, texts, cats,
                use_extraction=True,
                neg_embs=None, neg_texts=None, neutral_embs=None,
                include_llm=False):
    """Run all modes on a file and display comparison."""
    if not os.path.exists(filepath):
        print("  ERROR: File not found: {}".format(filepath)); sys.exit(1)
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    sentences = [s.strip() for s in content.split("###") if s.strip()]

    modes_str = "cosine, nli, hybrid"
    if include_llm:
        modes_str += ", llm"
    print("  {}Compare mode:{} {} on {} messages from: {}".format(
        BOLD, RESET, modes_str, len(sentences), filepath))
    print("  " + "=" * 100)

    all_mode_results = []
    for idx, sent in enumerate(sentences, 1):
        mr = score_all_modes(
            sent, model, embs, texts, cats,
            neg_embs=neg_embs, neg_texts=neg_texts, neutral_embs=neutral_embs,
            use_extraction=use_extraction, include_llm=include_llm)
        all_mode_results.append(mr)
        display_compare(sent, mr, index=idx, total=len(sentences))

    display_compare_summary(all_mode_results, sentences)


def run_compare_interactive(model, embs, texts, cats,
                            use_extraction=True,
                            neg_embs=None, neg_texts=None, neutral_embs=None,
                            include_llm=False):
    """Interactive compare mode — type messages, see all modes side-by-side."""
    global SPELLCHECK_ENABLED
    modes_str = "cosine, nli, hybrid"
    if include_llm:
        modes_str += ", llm"
    print("{}Compare Mode: {}{}".format(BOLD, modes_str, RESET))
    print("  Type a message to compare all scoring modes. Commands: /quit  /llm\n")
    print("  Thresholds: match={}{}{}  warning={}{}{}".format(
        RED, MATCH_THRESHOLD, RESET, YELLOW, WARNING_THRESHOLD, RESET))
    print("  " + "-" * 70)

    all_mode_results = []
    all_messages = []

    while True:
        try:
            msg = _read_message("\n  {}Message>{} ".format(BOLD, RESET))
        except (EOFError, KeyboardInterrupt):
            print("\n\n  Goodbye!")
            break
        if not msg:
            continue

        if msg.startswith("/"):
            c = msg.lower().split()
            if c[0] in ("/quit", "/exit", "/q"):
                break
            elif c[0] == "/llm":
                include_llm = not include_llm
                if include_llm:
                    try:
                        _load_llm_config()
                        print("  LLM: ON")
                    except SystemExit:
                        include_llm = False
                        print("  LLM: OFF (config not found)")
                else:
                    print("  LLM: OFF")
                continue
            elif c[0] == "/spellcheck":
                SPELLCHECK_ENABLED = not SPELLCHECK_ENABLED
                if SPELLCHECK_ENABLED:
                    _load_spellchecker()
                print("  Spell correction: {}".format(
                    "ON" if SPELLCHECK_ENABLED else "OFF"))
                continue
            elif c[0] == "/summary":
                if all_mode_results:
                    display_compare_summary(all_mode_results, all_messages)
                else:
                    print("  No messages scored yet.")
                continue
            elif c[0] == "/help":
                print("  /llm        \u2014 toggle LLM judge in comparison")
                print("  /spellcheck \u2014 toggle autocorrect spell checking")
                print("  /summary    \u2014 show summary of all messages so far")
                print("  /quit       \u2014 exit (shows summary)")
                continue
            else:
                print("  Unknown command. /help for options.")
                continue

        mr = score_all_modes(
            msg, model, embs, texts, cats,
            neg_embs=neg_embs, neg_texts=neg_texts, neutral_embs=neutral_embs,
            use_extraction=use_extraction, include_llm=include_llm)
        all_mode_results.append(mr)
        all_messages.append(msg)
        display_compare(msg, mr)

    if all_mode_results:
        display_compare_summary(all_mode_results, all_messages)


# ---------------------------------------------------------------------------
# 3D Visualization
# ---------------------------------------------------------------------------

def show_graph(model):
    """3D interactive scatter plot of anchor embeddings using PCA."""
    try:
        import matplotlib
        from sklearn.decomposition import PCA
        import numpy as np
    except ImportError as e:
        print("\n  ERROR: Missing dependency for --graph: {}".format(e))
        print("  Run: pip install matplotlib scikit-learn")
        sys.exit(1)

    backend_set = False
    for backend in ["macosx", "Qt5Agg", "TkAgg"]:
        try:
            matplotlib.use(backend)
            backend_set = True
            break
        except Exception:
            continue
    if not backend_set:
        matplotlib.use("Agg")

    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401

    using_agg = matplotlib.get_backend().lower() == "agg"

    all_texts = []
    all_cats = []
    for cat, examples in ANCHORS.items():
        for ex in examples:
            all_texts.append(ex)
            all_cats.append(cat)

    print("  Embedding anchors for visualization...")
    all_embs = model.encode(all_texts, show_progress_bar=False)
    prop_emb = model.encode([PROPOSITION])

    combined = np.vstack([all_embs, prop_emb])

    pca = PCA(n_components=3)
    coords_3d = pca.fit_transform(combined)
    anchor_coords = coords_3d[:-1]
    prop_coord = coords_3d[-1]

    explained = pca.explained_variance_ratio_
    print("  PCA variance explained: {:.1f}% + {:.1f}% + {:.1f}% = {:.1f}%".format(
        explained[0]*100, explained[1]*100, explained[2]*100, sum(explained)*100))

    unique_cats = list(dict.fromkeys(all_cats))
    try:
        cmap = matplotlib.colormaps.get_cmap("tab10")
    except AttributeError:
        cmap = plt.cm.get_cmap("tab10")
    cat_colors = {cat: cmap(i % 10) for i, cat in enumerate(unique_cats)}

    fig = plt.figure(figsize=(12, 9))
    ax = fig.add_subplot(111, projection="3d")

    for cat in unique_cats:
        mask = [i for i, c in enumerate(all_cats) if c == cat]
        xs = [anchor_coords[i, 0] for i in mask]
        ys = [anchor_coords[i, 1] for i in mask]
        zs = [anchor_coords[i, 2] for i in mask]
        ax.scatter(xs, ys, zs, c=[cat_colors[cat]], s=50, alpha=0.7,
                   edgecolors="white", linewidths=0.5, label=cat)

    ax.scatter([prop_coord[0]], [prop_coord[1]], [prop_coord[2]],
               c="red", s=300, marker="*", edgecolors="black",
               linewidths=1, label="PROPOSITION", zorder=10)

    for i in range(len(all_texts)):
        ax.plot([prop_coord[0], anchor_coords[i, 0]],
                [prop_coord[1], anchor_coords[i, 1]],
                [prop_coord[2], anchor_coords[i, 2]],
                color="red", alpha=0.08, linewidth=0.5)

    ax.set_xlabel("PC1 ({:.1f}%)".format(explained[0]*100))
    ax.set_ylabel("PC2 ({:.1f}%)".format(explained[1]*100))
    ax.set_zlabel("PC3 ({:.1f}%)".format(explained[2]*100))
    ax.set_title("Semantic Anchors: {} \u2014 {} anchors in 3D embedding space".format(
        SCRIPT_NAME, len(all_texts)), fontsize=13, fontweight="bold")
    ax.legend(loc="upper left", fontsize=8, framealpha=0.9)

    norms = np.linalg.norm(all_embs, axis=1, keepdims=True)
    normed = all_embs / (norms + 1e-10)
    sim_m = normed @ normed.T
    np.fill_diagonal(sim_m, 0)
    stats = "Avg pairwise sim: {:.4f} | Min: {:.4f} | Max: {:.4f}".format(
        sim_m.mean(), sim_m[sim_m > 0].min(), sim_m.max())
    ax.text2D(0.02, 0.96, stats, transform=ax.transAxes, fontsize=8,
              bbox=dict(boxstyle="round", facecolor="white", alpha=0.8))

    plt.tight_layout()

    if using_agg:
        out_file = "semantic_anchor_{}_3d.png".format(SCRIPT_NAME)
        plt.savefig(out_file, dpi=150)
        print("  Backend is non-interactive. Saved plot to: {}".format(out_file))
        import subprocess, platform
        try:
            if platform.system() == "Darwin":
                subprocess.Popen(["open", out_file])
            elif platform.system() == "Linux":
                subprocess.Popen(["xdg-open", out_file])
            elif platform.system() == "Windows":
                os.startfile(out_file)
        except Exception:
            pass
    else:
        print("  Showing 3D plot (drag to rotate)...")
        plt.show()


# ---------------------------------------------------------------------------
# Multi-line input handling
# ---------------------------------------------------------------------------

def _read_message(prompt):
    """
    Read a complete message from the user. Handles both single-line and
    multi-line input reliably without timing-based paste detection.

    Rules:
      - If the line ends with sentence-ending punctuation (. ? ! " ' ) ]),
        it's treated as complete and evaluated immediately.
      - If it doesn't (indicating a line break mid-sentence from a paste),
        a continuation prompt is shown. The user keeps pasting/typing until
        they enter a blank line.
      - Commands (starting with /) are always single-line.
    """
    first_line = input(prompt).strip()
    if not first_line:
        return first_line

    # Commands are always single-line
    if first_line.startswith("/"):
        return first_line

    # Check if the line looks complete (ends with sentence punctuation)
    if first_line and first_line[-1] in '.?!"\')]':
        # Try to drain any paste buffer quickly (best-effort)
        try:
            import select, sys
            lines = [first_line]
            while True:
                ready, _, _ = select.select([sys.stdin], [], [], 0.08)
                if ready:
                    line = sys.stdin.readline()
                    if not line:
                        break
                    line = line.strip()
                    if line:
                        lines.append(line)
                else:
                    break
            return " ".join(lines)
        except (ImportError, OSError):
            return first_line

    # Line doesn't end with punctuation — it's a multi-line paste
    # Show continuation prompt and read until blank line
    lines = [first_line]
    while True:
        try:
            line = input("  {}...>{} ".format(BOLD, RESET))
        except (EOFError, KeyboardInterrupt):
            break
        if not line.strip():
            break
        lines.append(line.strip())

    return " ".join(lines)


# ---------------------------------------------------------------------------
# Interactive mode
# ---------------------------------------------------------------------------

def run_interactive(model, embs, texts, cats, verbose=False, mode="cosine",
                    use_extraction=True,
                    neg_embs=None, neg_texts=None, neutral_embs=None):
    global SPELLCHECK_ENABLED
    mode_labels = {
        "cosine": "Cosine similarity",
        "nli": "NLI entailment (all anchors)",
        "hybrid": "Hybrid (NLI + smart contrastive)",
        "llm": "LLM Judge",
    }
    mode_label = mode_labels.get(mode, mode)
    extract_label = " + extraction" if use_extraction else ""
    neg_label = " + contrastive" if neg_texts else ""
    print("{}Interactive Mode: {}{}{}{}".format(BOLD, mode_label, extract_label, neg_label, RESET))
    print("  Type a message to evaluate. Commands: /verbose  /top N  /mode MODE  /extract  /quit\n")
    print("  Thresholds: match={}{}{}  warning={}{}{}".format(
        RED, MATCH_THRESHOLD, RESET, YELLOW, WARNING_THRESHOLD, RESET))
    print("  Mode: {}{}{}".format(GREEN, mode_label, RESET))
    print("  Extraction: {}{}{}  (long-message defense, tau={})".format(
        GREEN if use_extraction else YELLOW,
        "ON" if use_extraction else "OFF",
        RESET, EXTRACTION_RELEVANCE_TAU))
    if neg_texts:
        neutral_count = len(NEUTRAL_ANCHORS) if NEUTRAL_ANCHORS else 0
        print("  Contrastive: {}ON{} ({} negative, {} neutral anchors)".format(
            GREEN, RESET, len(neg_texts), neutral_count))
    else:
        print("  Contrastive: {}OFF{} (no negative anchors)".format(YELLOW, RESET))
    print("  " + "-" * 70)
    top_n = 3
    verbose_extract = False

    while True:
        try:
            msg = _read_message("\n  {}Message>{} ".format(BOLD, RESET))
        except (EOFError, KeyboardInterrupt):
            print("\n\n  Goodbye!")
            break
        if not msg:
            continue

        if msg.startswith("/"):
            c = msg.lower().split()
            if c[0] in ("/quit", "/exit", "/q"):
                print("  Goodbye!"); break
            elif c[0] == "/verbose":
                verbose = not verbose
                print("  Verbose: {}".format("ON" if verbose else "OFF")); continue
            elif c[0] == "/top" and len(c) > 1:
                try: top_n = int(c[1]); print("  Showing top {}".format(top_n))
                except ValueError: print("  Usage: /top N")
                continue
            elif c[0] == "/mode" and len(c) > 1:
                new_mode = c[1]
                if new_mode in ("cosine", "nli", "hybrid", "llm"):
                    mode = new_mode
                    if mode in ("nli", "hybrid"):
                        _load_cross_encoder()
                    if mode == "llm":
                        _load_llm_config()
                    print("  Mode: {}".format(mode_labels.get(mode, mode)))
                else:
                    print("  Available modes: cosine, nli, hybrid, llm")
                continue
            elif c[0] == "/extract":
                use_extraction = not use_extraction
                print("  Extraction: {} (tau={})".format(
                    "ON" if use_extraction else "OFF", EXTRACTION_RELEVANCE_TAU)); continue
            elif c[0] == "/extractv":
                verbose_extract = not verbose_extract
                print("  Verbose extraction: {}".format(
                    "ON" if verbose_extract else "OFF")); continue
            elif c[0] == "/spellcheck":
                SPELLCHECK_ENABLED = not SPELLCHECK_ENABLED
                if SPELLCHECK_ENABLED:
                    _load_spellchecker()
                print("  Spell correction: {}".format(
                    "ON" if SPELLCHECK_ENABLED else "OFF")); continue
            elif c[0] == "/help":
                print("  /verbose   \u2014 toggle full table")
                print("  /top N     \u2014 show top N results")
                print("  /mode MODE \u2014 switch scoring: cosine, nli, hybrid, llm")
                print("  /extract   \u2014 toggle proposition-guided extraction (long msgs)")
                print("  /extractv  \u2014 toggle verbose extraction diagnostics")
                print("  /spellcheck\u2014 toggle autocorrect spell checking")
                print("  /quit      \u2014 exit"); continue
            else:
                print("  Unknown command. /help for options."); continue

        # --- Dispatch by mode ---
        if mode == "llm":
            llm_result, corrected, corrections, cosine_results = score_message_llm(
                msg, PROPOSITION, model=model, all_embeddings=embs,
                all_texts=texts, all_categories=cats,
                neg_embeddings=neg_embs, neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            display_llm_result(llm_result, corrected, corrections, cosine_results, top_n)

        elif mode == "hybrid":
            results, corrected, corrections, extraction, neg_score, debug = score_message_hybrid(
                msg, texts, cats, model=model, all_embeddings=embs,
                neg_texts=neg_texts, neg_embeddings=neg_embs,
                neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            if verbose:
                display_verbose_hybrid(results, corrected, corrections, extraction,
                                       neg_score=neg_score, debug=debug)
            else:
                display_default_hybrid(results, corrected, corrections, extraction,
                                       top_n, neg_score=neg_score, debug=debug)

        elif mode == "nli":
            results, corrected, corrections, extraction, neg_score = score_message_nli(
                msg, texts, cats, model=model, all_embeddings=embs,
                neg_texts=neg_texts, neg_embeddings=neg_embs,
                neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            if verbose:
                display_verbose_nli(results, corrected, corrections, extraction, neg_score=neg_score)
            else:
                display_default_nli(results, corrected, corrections, extraction, top_n, neg_score=neg_score)

        else:  # cosine
            results, extraction, neg_score = score_message(model, msg, embs, texts, cats,
                                               neg_embeddings=neg_embs, neg_texts=neg_texts,
                                               neutral_embeddings=neutral_embs,
                                               use_extraction=use_extraction)
            if verbose:
                display_verbose(results, extraction, neg_score=neg_score)
            else:
                display_default(results, extraction, top_n, verbose_extract, neg_score=neg_score)


# ---------------------------------------------------------------------------
# File mode
# ---------------------------------------------------------------------------

def run_file(filepath, model, embs, texts, cats, verbose=False, mode="cosine",
             use_extraction=True,
             neg_embs=None, neg_texts=None, neutral_embs=None):
    if not os.path.exists(filepath):
        print("  ERROR: File not found: {}".format(filepath)); sys.exit(1)
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    sentences = [s.strip() for s in content.split("###") if s.strip()]
    mode_labels = {
        "cosine": "cosine", "nli": "NLI-only",
        "hybrid": "hybrid", "llm": "LLM judge",
    }
    mode_label = " ({})".format(mode_labels.get(mode, mode))
    extract_label = " + extraction" if use_extraction else ""
    neg_label = " + contrastive" if neg_texts else ""
    print("  Processing {} sentences from: {}{}{}{}\n".format(
        len(sentences), filepath, mode_label, extract_label, neg_label))
    print("  " + "=" * 100)
    all_top_scores = []
    all_verdicts = []

    for idx, sent in enumerate(sentences, 1):
        disp = sent[:80] + ("..." if len(sent) > 80 else "")
        print("\n  {}[{}/{}] Message:{} \"{}\"".format(BOLD, idx, len(sentences), RESET, disp))

        if mode == "llm":
            llm_result, corrected, corrections, cosine_results = score_message_llm(
                sent, PROPOSITION, model=model, all_embeddings=embs,
                all_texts=texts, all_categories=cats,
                neg_embeddings=neg_embs, neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            display_llm_result(llm_result, corrected, corrections, cosine_results)
            conf = llm_result.get("confidence", 0.0)
            verdict = llm_result.get("verdict", "ERROR")
            all_top_scores.append(conf if llm_result.get("match") else 0.0)
            all_verdicts.append(verdict)

        elif mode == "hybrid":
            results, corrected, corrections, extraction, neg_score, debug = score_message_hybrid(
                sent, texts, cats, model=model, all_embeddings=embs,
                neg_texts=neg_texts, neg_embeddings=neg_embs,
                neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose_hybrid(results, corrected, corrections, extraction,
                                       neg_score=neg_score, debug=debug)
            else:
                display_default_hybrid(results, corrected, corrections, extraction,
                                       neg_score=neg_score, debug=debug)
            all_top_scores.append(top_score)

        elif mode == "nli":
            results, corrected, corrections, extraction, neg_score = score_message_nli(
                sent, texts, cats, model=model, all_embeddings=embs,
                neg_texts=neg_texts, neg_embeddings=neg_embs,
                neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose_nli(results, corrected, corrections, extraction, neg_score=neg_score)
            else:
                display_default_nli(results, corrected, corrections, extraction, neg_score=neg_score)
            all_top_scores.append(top_score)

        else:  # cosine
            results, extraction, neg_score = score_message(model, sent, embs, texts, cats,
                                               neg_embeddings=neg_embs, neg_texts=neg_texts,
                                               neutral_embeddings=neutral_embs,
                                               use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose(results, extraction, neg_score=neg_score)
            else:
                display_default(results, extraction, neg_score=neg_score)
            all_top_scores.append(top_score)

    # Summary
    if mode == "llm":
        matches = sum(1 for v in all_verdicts if v == "MATCH")
        warns = sum(1 for v in all_verdicts if v == "WARNING")
        errors = sum(1 for v in all_verdicts if v == "ERROR")
        clean = sum(1 for v in all_verdicts if v == "NO MATCH")
    else:
        matches = sum(1 for s in all_top_scores if s >= MATCH_THRESHOLD)
        warns = sum(1 for s in all_top_scores if WARNING_THRESHOLD <= s < MATCH_THRESHOLD)
        clean = sum(1 for s in all_top_scores if s < WARNING_THRESHOLD)
        errors = 0

    print("\n  {}Summary{}".format(BOLD, RESET))
    print("  " + "=" * 60)
    print("  Total sentences:  {}".format(len(sentences)))
    print("  {}\u25a0 Matches:         {}{}".format(RED, matches, RESET))
    print("  {}\u25a0 Warnings:        {}{}".format(YELLOW, warns, RESET))
    print("  {}\u25a0 No match:        {}{}".format(GREEN, clean, RESET))
    if errors:
        print("  \u25a0 Errors:          {}".format(errors))
    print()


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Semantic Anchor Evaluator: " + SCRIPT_NAME)
    parser.add_argument("--file", "-f", type=str, default=None,
        help="Input file with sentences separated by ###")
    parser.add_argument("--verbose", "-v", action="store_true",
        help="Show full scored table (all anchors sorted)")
    parser.add_argument("--show-examples", action="store_true",
        help="Print all positive examples by category")
    parser.add_argument("--graph", "-g", action="store_true",
        help="Show 3D interactive visualization of anchor spread")
    parser.add_argument("--mode", "-m", type=str, default="cosine",
        choices=["cosine", "nli", "hybrid", "llm"],
        help="Scoring mode: cosine, nli, hybrid (NLI+smart contrastive), llm (LLM judge)")
    # Backward-compatible aliases
    parser.add_argument("--nli", action="store_true",
        help="Shortcut for --mode nli")
    parser.add_argument("--hybrid", action="store_true",
        help="Shortcut for --mode hybrid")
    parser.add_argument("--llm", action="store_true",
        help="Shortcut for --mode llm")
    parser.add_argument("--rerank", "-r", action="store_true",
        help="(Deprecated) Alias for --mode hybrid")
    parser.add_argument("--no-extract", action="store_true",
        help="Disable proposition-guided extraction for long messages")
    parser.add_argument("--spellcheck", action="store_true",
        help="Enable autocorrect spell checking (disabled by default)")
    parser.add_argument("--compare", "-c", action="store_true",
        help="Compare all scoring modes side-by-side (cosine, nli, hybrid, +llm)")
    args = parser.parse_args()

    # Resolve mode from flags
    mode = args.mode
    if args.compare:
        mode = "compare"
    elif args.nli:
        mode = "nli"
    elif args.hybrid or args.rerank:
        mode = "hybrid"
    elif args.llm:
        mode = "llm"

    use_extraction = not args.no_extract

    # --- Spellcheck: CLI flag or config_<name>.ini ---
    global SPELLCHECK_ENABLED
    if args.spellcheck:
        SPELLCHECK_ENABLED = True
    else:
        # Check config_<name>.ini for spellcheck setting
        try:
            import configparser as _cp
            _cfg = _cp.ConfigParser()
            _cfg_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                                     "config_{}.ini".format(SCRIPT_NAME))
            if os.path.exists(_cfg_path):
                _cfg.read(_cfg_path)
                if _cfg.has_option("options", "spellcheck"):
                    SPELLCHECK_ENABLED = _cfg.get("options", "spellcheck").lower() in ("true", "1", "yes", "on")
                elif _cfg.has_option("llm_judge", "spellcheck"):
                    SPELLCHECK_ENABLED = _cfg.get("llm_judge", "spellcheck").lower() in ("true", "1", "yes", "on")
        except Exception:
            pass

    print_banner()

    if args.show_examples:
        print_examples()
        if not args.file and not args.graph:
            try:
                r = input("  Enter interactive mode? [Y/n] ").strip().lower()
                if r == "n": return
            except (EOFError, KeyboardInterrupt):
                print(); return

    model = load_model()

    if args.graph:
        show_graph(model)
        return

    (all_texts, all_cats, all_embs,
     neg_texts, neg_cats, neg_embs,
     neutral_embs) = prepare_anchors(model)

    if use_extraction:
        print("  Extraction: {}ON{} (tau={}, min_words={})".format(
            GREEN, RESET, EXTRACTION_RELEVANCE_TAU, EXTRACTION_MIN_WORDS))

    # Log model sources
    if _model_sources:
        emb_name, emb_src = _model_sources.get("embedding", (EMBEDDING_MODEL, "default"))
        nli_name, nli_src = _model_sources.get("nli", (NLI_MODEL, "default"))
        print("  Embedding:  {} {}(from {}){}".format(emb_name, DIM, emb_src, RESET))
        print("  NLI model:  {} {}(from {}){}".format(nli_name, DIM, nli_src, RESET))

    spell_label = "{}ON{}".format(GREEN, RESET) if SPELLCHECK_ENABLED else "{}OFF{} (enable with --spellcheck)".format(DIM, RESET)
    print("  Spell correction: {}\n".format(spell_label))

    # Load cross-encoder for modes that need it
    if mode in ("nli", "hybrid", "compare"):
        _load_cross_encoder()
        total_anchors = sum(len(v) for v in ANCHORS.values())
        if mode == "compare":
            print("  Compare mode: {}ON{} (all scoring methods side-by-side)".format(
                GREEN, RESET))
        elif mode == "hybrid":
            print("  Hybrid mode: {}ON{} (NLI scoring + smart NLI-confirmed contrastive)".format(
                GREEN, RESET))
        else:
            print("  NLI-only mode: {}ON{}".format(GREEN, RESET))
            print("  Scoring: NLI entailment on ALL {} anchors".format(total_anchors))
            print("  Long messages: proposition-guided extraction + multi-view NLI\n")

    elif mode == "llm":
        _load_llm_config()
        print("  LLM Judge: {}ON{} ({} / {})".format(
            GREEN, RESET, _llm_config["provider"], _llm_config["model"]))
        prov = _llm_config["provider"].lower()
        if prov in ("ollama", "local", "lmstudio", "vllm"):
            print("  Base URL:   {}".format(_llm_config.get("base_url", "")))
        print()

    # Check if LLM is available for compare mode
    include_llm = False
    if mode == "compare":
        try:
            _load_llm_config(silent=True)
            if _llm_config.get("api_key", "").startswith("YOUR_"):
                print("  LLM Judge: {}SKIPPED{} (edit config_{}.ini [llm_judge] api_key)".format(
                    YELLOW, RESET, SCRIPT_NAME))
            else:
                include_llm = True
                print("  LLM Judge: {}ON{} ({} / {})".format(
                    GREEN, RESET, _llm_config["provider"], _llm_config["model"]))
        except SystemExit:
            print("  LLM Judge: {}SKIPPED{} (no config_{}.ini found)".format(
                YELLOW, RESET, SCRIPT_NAME))
        print()

    # Dispatch
    if mode == "compare":
        if args.file:
            run_compare(args.file, model, all_embs, all_texts, all_cats,
                        use_extraction, neg_embs=neg_embs, neg_texts=neg_texts,
                        neutral_embs=neutral_embs, include_llm=include_llm)
        else:
            run_compare_interactive(model, all_embs, all_texts, all_cats,
                                    use_extraction, neg_embs=neg_embs,
                                    neg_texts=neg_texts, neutral_embs=neutral_embs,
                                    include_llm=include_llm)
    elif args.file:
        run_file(args.file, model, all_embs, all_texts, all_cats,
                 args.verbose, mode, use_extraction,
                 neg_embs=neg_embs, neg_texts=neg_texts, neutral_embs=neutral_embs)
    else:
        run_interactive(model, all_embs, all_texts, all_cats,
                        args.verbose, mode, use_extraction,
                        neg_embs=neg_embs, neg_texts=neg_texts, neutral_embs=neutral_embs)


if __name__ == "__main__":
    main()
