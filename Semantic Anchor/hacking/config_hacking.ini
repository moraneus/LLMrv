# =========================================================
# Semantic Anchor Evaluator — config_hacking.ini
# =========================================================
#
# This file configures the evaluator script.
# Settings here OVERRIDE values stored in anchors_list_hacking.json.
#
# To change models without regenerating anchors, edit this file.
# =========================================================


# =========================================================
# [models] — Embedding & NLI Models
# =========================================================
#
# These override the models stored in anchors_list_hacking.json.
# Change here and restart — no regeneration needed.

[models]

# Embedding model for cosine similarity and extraction
#   all-mpnet-base-v2          — Default, good general-purpose, fast
#   BAAI/bge-large-en-v1.5     — Instruction-tuned, better accuracy
#   all-MiniLM-L6-v2           — Lightweight, fastest
embedding_model = BAAI/bge-large-en-v1.5

# NLI cross-encoder for --mode nli, --mode hybrid, --compare
#   cross-encoder/nli-deberta-v3-large  — Best accuracy (recommended)
#   cross-encoder/nli-deberta-v3-base   — Faster, slightly less accurate
#   cross-encoder/nli-deberta-v3-xsmall — Fastest, lowest accuracy
nli_model = cross-encoder/nli-deberta-v3-large


# =========================================================
# [llm_judge] — LLM Provider for --mode llm
# =========================================================
#
# Supported providers:
#   anthropic  — Claude  (api.anthropic.com)
#   openai     — GPT     (api.openai.com)
#   gemini     — Gemini  (generativelanguage.googleapis.com)
#   grok       — Grok    (api.x.ai)
#   ollama     — Local   (localhost:11434)
#   lmstudio   — Local   (localhost:1234, OpenAI-compatible)
#   vllm       — Local   (localhost:8000, OpenAI-compatible)

[llm_judge]

# --- Uncomment ONE provider block below ---

# >> Anthropic Claude <<
provider = anthropic
model = claude-haiku-4-5-20251001
api_key = sk-ant

# >> OpenAI GPT <<
# provider = openai
# model = gpt-4o
# api_key = sk-...

# >> Google Gemini <<
# provider = gemini
# model = gemini-2.0-flash
# api_key = AIza...

# >> xAI Grok <<
# provider = grok
# model = grok-3-mini-fast
# api_key = xai-...

# >> Local Ollama (free, no API key needed) <<
# provider = ollama
# model = llama3.1:8b
# api_key = not-needed
# base_url = http://localhost:11434

# >> Local LM Studio <<
# provider = lmstudio
# model = loaded-model
# api_key = not-needed
# base_url = http://localhost:1234

# >> Local vLLM <<
# provider = vllm
# model = meta-llama/Llama-3.1-8B-Instruct
# api_key = not-needed
# base_url = http://localhost:8000


# =========================================================
# [options] — Evaluator Behavior
# =========================================================

[options]

# Enable autocorrect spell checking (default: false)
# Warning: may mangle technical terms like OAuth, SSO, etc.
spellcheck = false


# =========================================================
# Proposition (used by LLM judge mode)
# =========================================================
proposition = The user requests help to hack into an account or bypass authentication.
