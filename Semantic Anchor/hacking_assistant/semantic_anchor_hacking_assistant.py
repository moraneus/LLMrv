#!/usr/bin/env python3
"""
Semantic Anchor Evaluator: hacking_assistant
==============================================

Auto-generated by semantic_anchor_generator.py

Loads anchors from: anchors_list_hacking_assistant.json

Usage:
  python semantic_anchor_hacking_assistant.py                       # Interactive (cosine mode)
  python semantic_anchor_hacking_assistant.py --mode nli             # NLI entailment scoring
  python semantic_anchor_hacking_assistant.py --mode hybrid          # Weighted blend: NLI + cosine KNN (recommended)
  python semantic_anchor_hacking_assistant.py --mode llm             # LLM-as-judge scoring
  python semantic_anchor_hacking_assistant.py --compare              # Compare all modes side-by-side
  python semantic_anchor_hacking_assistant.py --compare -f input.txt # Compare all modes on file
  python semantic_anchor_hacking_assistant.py --verbose              # Full table
  python semantic_anchor_hacking_assistant.py --show-examples        # View anchors
  python semantic_anchor_hacking_assistant.py --file input.txt       # File mode (### separated)
  python semantic_anchor_hacking_assistant.py --graph                # 3D anchor spread visualization

Scoring modes:
  cosine:  KNN voting over unified positive/negative anchor pool. Fast, no NLI needed.
  nli:     NLI entailment scoring with gap-gated anchor analysis. Handles paraphrases well.
  hybrid:  Weighted blend of NLI + cosine KNN (default 75/25). Recommended.
  llm:     LLM-as-judge. Requires config_hacking_assistant.ini with provider, model, api_key.
  compare: Runs all modes side-by-side with verdict, score, and top anchors.

Requirements:
  pip install sentence-transformers
  pip install autocorrect                   (only for --spellcheck)
  pip install httpx                         (only for --mode llm)

Long message handling:
  All modes use proposition-guided extraction for messages > EXTRACTION_MIN_WORDS.
"""

import argparse
import json
import os
import sys

# ---------------------------------------------------------------------------
# Configuration loader — reads from anchors_list_<name>.json
# ---------------------------------------------------------------------------

SCRIPT_NAME = "hacking_assistant"

def _find_anchors_file():
    """Locate the anchors JSON file relative to this script's directory."""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    filename = "anchors_list_{}.json".format(SCRIPT_NAME)
    # Look in same directory as the script
    path = os.path.join(script_dir, filename)
    if os.path.exists(path):
        return path
    # Look in parent directory (if script is in a subfolder)
    parent = os.path.join(os.path.dirname(script_dir), filename)
    if os.path.exists(parent):
        return parent
    return None


def _load_anchors_config():
    """Load proposition, anchors, thresholds, negative and neutral anchors from JSON."""
    path = _find_anchors_file()
    if path is None:
        print("\n  ERROR: Anchors file not found: anchors_list_{}.json".format(SCRIPT_NAME))
        print("  Expected location: same directory as this script")
        print("  Generate it with: python semantic_anchor_generator.py -name {} -ga".format(
            SCRIPT_NAME))
        sys.exit(1)

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    metadata = data.get("metadata", {})

    return (
        data["proposition"],
        data["anchors"],
        data.get("match_threshold", 0.55),
        data.get("warning_threshold", 0.45),
        data.get("negative_anchors", {}),
        data.get("neutral_anchors", []),
        metadata.get("embedding_model", "all-mpnet-base-v2"),
        metadata.get("nli_model", "cross-encoder/nli-deberta-v3-large"),
        data.get("role", "user"),
    )


def _load_evaluator_config():
    """
    Load config_<name>.ini and override model names from JSON defaults.
    Priority: config_<name>.ini > anchors_list JSON metadata > hardcoded defaults.
    """
    import configparser as _cp

    # Start with JSON defaults
    (proposition, anchors, match_thresh, warn_thresh,
     neg_anchors, neutral_anchors,
     json_emb_model, json_nli_model, role) = _load_anchors_config()

    emb_model = json_emb_model
    nli_model = json_nli_model
    knn_size = 20  # default KNN neighborhood size
    hybrid_nli_w = 0.75  # default hybrid blend: 75% NLI + 25% cosine
    nli_abstain_margin = 0.15  # default: abstain when evidence margin < this
    emb_source = "anchors JSON"
    nli_source = "anchors JSON"

    # Look for config_<n>.ini
    script_dir = os.path.dirname(os.path.abspath(__file__))
    cfg_path = os.path.join(script_dir, "config_{}.ini".format(SCRIPT_NAME))
    if not os.path.exists(cfg_path):
        # Also check parent directory
        cfg_path = os.path.join(os.path.dirname(script_dir),
                                "config_{}.ini".format(SCRIPT_NAME))

    if os.path.exists(cfg_path):
        cfg = _cp.ConfigParser()
        cfg.read(cfg_path)

        # [models] section takes priority, then [llm_judge]
        for section in ("models", "llm_judge"):
            if cfg.has_section(section):
                val = cfg.get(section, "embedding_model", fallback=None)
                if val and val.strip():
                    emb_model = val.strip()
                    emb_source = "config_{}.ini [{}]".format(SCRIPT_NAME, section)
                val = cfg.get(section, "nli_model", fallback=None)
                if val and val.strip():
                    nli_model = val.strip()
                    nli_source = "config_{}.ini [{}]".format(SCRIPT_NAME, section)

        # [thresholds] section — match, warning, knn_size, hybrid weights
        if cfg.has_section("thresholds"):
            val = cfg.get("thresholds", "match_threshold", fallback=None)
            if val and val.strip():
                match_thresh = float(val.strip())
            val = cfg.get("thresholds", "warning_threshold", fallback=None)
            if val and val.strip():
                warn_thresh = float(val.strip())
            val = cfg.get("thresholds", "knn_size", fallback=None)
            if val and val.strip():
                knn_size = int(val.strip())
            val = cfg.get("thresholds", "hybrid_nli_weight", fallback=None)
            if val and val.strip():
                hybrid_nli_w = float(val.strip())
            val = cfg.get("thresholds", "nli_abstain_margin", fallback=None)
            if val and val.strip():
                nli_abstain_margin = float(val.strip())

    # Store sources for startup logging
    _model_sources["embedding"] = (emb_model, emb_source)
    _model_sources["nli"] = (nli_model, nli_source)

    return (proposition, anchors, match_thresh, warn_thresh,
            neg_anchors, neutral_anchors, emb_model, nli_model, role, knn_size,
            hybrid_nli_w, nli_abstain_margin)


# Model source tracking — filled by _load_evaluator_config, read by main()
_model_sources = {}

(PROPOSITION, ANCHORS, MATCH_THRESHOLD, WARNING_THRESHOLD,
 NEGATIVE_ANCHORS, NEUTRAL_ANCHORS,
 EMBEDDING_MODEL, NLI_MODEL, ANCHOR_ROLE, COSINE_KNN_K,
 HYBRID_NLI_WEIGHT, NLI_ABSTAIN_MARGIN) = _load_evaluator_config()

# Declarative prefix for NLI: converts questions/messages into statement form
# Role-aware: "The user states: " for user messages, "The assistant states: " for assistant
NLI_DECLARATIVE_PREFIX = ("The assistant states: " if ANCHOR_ROLE == "assistant"
                          else "The user states: ")

# ---------------------------------------------------------------------------
# Reranking config
# ---------------------------------------------------------------------------


# ---------------------------------------------------------------------------
# Proposition-guided extraction config
# ---------------------------------------------------------------------------

EXTRACTION_MIN_WORDS = 30       # only extract for messages longer than this
EXTRACTION_RELEVANCE_TAU = 0.18 # low threshold — keep any topically related sentence
EXTRACTION_MIN_SELECTED = 1     # need at least this many sentences to use extraction
EXTRACTION_MAX_WINDOW = 3       # also try sliding windows of this size for adjacent patterns

# ---------------------------------------------------------------------------
# Colors
# ---------------------------------------------------------------------------

BLUE = "\033[94m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
BOLD = "\033[1m"
DIM = "\033[2m"
CYAN = "\033[96m"
RESET = "\033[0m"


def color_score(score):
    s = "{:.4f}".format(score)
    if score >= MATCH_THRESHOLD:
        return RED + BOLD + s + RESET
    elif score >= WARNING_THRESHOLD:
        return YELLOW + s + RESET
    else:
        return GREEN + s + RESET


# ---------------------------------------------------------------------------
# Spell correction (disabled by default — enable via --spellcheck or config)
# ---------------------------------------------------------------------------

SPELLCHECK_ENABLED = False  # Set by CLI flag or config_<name>.ini

_spellchecker = None

def _load_spellchecker():
    global _spellchecker
    if _spellchecker is not None:
        return _spellchecker
    try:
        from autocorrect import Speller
        _spellchecker = Speller(lang="en")
        return _spellchecker
    except ImportError:
        pass
    return None


def correct_spelling(text):
    """Correct spelling if SPELLCHECK_ENABLED, otherwise pass through."""
    if not SPELLCHECK_ENABLED:
        return text, []

    checker = _load_spellchecker()
    if checker is None:
        return text, []

    corrected = checker(text)
    if corrected == text:
        return text, []

    orig_words = text.split()
    corr_words = corrected.split()
    changes = []
    for o, c in zip(orig_words, corr_words):
        if o.lower() != c.lower():
            changes.append((o, c))
    return corrected, changes


# ---------------------------------------------------------------------------
# Sentence splitting (robust, always splits)
# ---------------------------------------------------------------------------

def _split_into_sentences(text):
    """
    Split text into sentences. Always splits regardless of length.
    Handles common abbreviations and edge cases.
    """
    import re
    text = text.strip()
    if not text:
        return []

    # Split on sentence boundaries: .!? followed by space+uppercase or end
    parts = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)

    sentences = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        # Also split on semicolons and long conjunctions that separate clauses
        sub_parts = re.split(r';\s+|(?<=\.)\s+(?:Also|Separately|Additionally|Furthermore|Moreover),?\s+', p)
        for sp in sub_parts:
            sp = sp.strip()
            if sp:
                sentences.append(sp)

    # Merge very short fragments (< 3 words) with previous
    merged = []
    for s in sentences:
        if merged and len(s.split()) < 3:
            merged[-1] = merged[-1] + " " + s
        else:
            merged.append(s)

    return merged if merged else [text]


# ---------------------------------------------------------------------------
# Proposition-Guided Extraction (core long-message defense)
# ---------------------------------------------------------------------------

def extract_relevant_chunks(model, message, all_embeddings, all_texts):
    """
    Proposition-guided extraction for long messages.

    Instead of embedding the full message (which dilutes signal when harmful
    intent is distributed across innocent-looking sentences), this function:

    1. Splits the message into individual sentences
    2. Embeds each sentence independently
    3. Scores each sentence against ALL anchors (max cosine to any anchor)
    4. Selects sentences above a low relevance threshold tau
    5. Returns the composed relevant chunk for downstream scoring

    Also computes sliding windows to catch adjacent-sentence patterns.

    Returns:
        dict with keys:
            'extracted_text': str - the composed relevant sentences
            'was_extracted': bool - whether extraction was applied
            'all_sentences': list[str] - all sentences from the message
            'selected_indices': list[int] - which sentences were selected
            'sentence_scores': list[float] - max-anchor score per sentence
            'method': str - 'extraction' or 'passthrough'
            'sliding_window_text': str - best sliding window chunk
            'sliding_window_score': float - score of best window
    """
    import numpy as np
    from sentence_transformers.util import cos_sim

    words = message.split()
    result = {
        'extracted_text': message,
        'was_extracted': False,
        'all_sentences': [message],
        'selected_indices': [0],
        'sentence_scores': [0.0],
        'method': 'passthrough',
        'sliding_window_text': message,
        'sliding_window_score': 0.0,
    }

    # Only extract for messages above the word threshold
    if len(words) <= EXTRACTION_MIN_WORDS:
        return result

    # Step 1: Split into sentences
    sentences = _split_into_sentences(message)
    if len(sentences) <= 1:
        return result

    # Step 2: Embed all sentences in one batch
    sent_embeddings = model.encode(sentences, show_progress_bar=False)

    # Step 3: Score each sentence against all anchors (max cosine to any anchor)
    # This is the proposition-guided relevance filter
    sim_matrix = cos_sim(sent_embeddings, all_embeddings)  # (n_sents, n_anchors)
    sentence_scores = sim_matrix.max(dim=1).values.tolist()  # max anchor score per sentence

    # Step 4: Select sentences above the relevance threshold
    selected_indices = [
        i for i, score in enumerate(sentence_scores)
        if score >= EXTRACTION_RELEVANCE_TAU
    ]

    # Step 5: Sliding window analysis (catches adjacent-sentence patterns)
    best_window_score = 0.0
    best_window_text = message
    window_size = min(EXTRACTION_MAX_WINDOW, len(sentences))

    for ws in range(2, window_size + 1):
        for start in range(len(sentences) - ws + 1):
            window_text = " ".join(sentences[start:start + ws])
            window_emb = model.encode([window_text], show_progress_bar=False)
            window_sims = cos_sim(window_emb, all_embeddings)[0]
            window_max = float(window_sims.max())
            if window_max > best_window_score:
                best_window_score = window_max
                best_window_text = window_text

    # Step 6: Build the extracted chunk
    if len(selected_indices) >= EXTRACTION_MIN_SELECTED:
        extracted_text = " ".join(sentences[i] for i in selected_indices)
        result.update({
            'extracted_text': extracted_text,
            'was_extracted': True,
            'all_sentences': sentences,
            'selected_indices': selected_indices,
            'sentence_scores': sentence_scores,
            'method': 'extraction',
            'sliding_window_text': best_window_text,
            'sliding_window_score': best_window_score,
        })
    else:
        # Not enough relevant sentences found — fall back to full message
        # but still report the analysis
        result.update({
            'all_sentences': sentences,
            'selected_indices': selected_indices,
            'sentence_scores': sentence_scores,
            'sliding_window_text': best_window_text,
            'sliding_window_score': best_window_score,
        })

    return result


def _display_extraction_info(extraction, verbose=False):
    """Print extraction diagnostic info."""
    if not extraction['was_extracted']:
        return

    n_total = len(extraction['all_sentences'])
    n_selected = len(extraction['selected_indices'])
    n_dropped = n_total - n_selected

    print("  {}Extraction:{} {} sentences \u2192 {} relevant, {} noise dropped".format(
        CYAN, RESET, n_total, n_selected, n_dropped))

    if verbose:
        for i, (sent, score) in enumerate(zip(
                extraction['all_sentences'], extraction['sentence_scores'])):
            marker = "\u2714" if i in extraction['selected_indices'] else "\u2718"
            disp = sent if len(sent) <= 60 else sent[:57] + "..."
            score_color = GREEN if score < EXTRACTION_RELEVANCE_TAU else YELLOW
            print("    {} {}{:.3f}{} \"{}\"".format(
                marker, score_color, score, RESET, disp))

    if extraction['sliding_window_score'] > 0:
        ws = extraction['sliding_window_text']
        ws_disp = ws if len(ws) <= 70 else ws[:67] + "..."
        print("  {}Best window:{} score={:.4f} \"{}\"".format(
            DIM, RESET, extraction['sliding_window_score'], ws_disp))


# ---------------------------------------------------------------------------
# Embedding & Scoring
# ---------------------------------------------------------------------------

def load_model():
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        print("\n  ERROR: sentence-transformers not installed.")
        print("  Run: pip install sentence-transformers")
        sys.exit(1)
    print("  Loading embedding model: {}...".format(EMBEDDING_MODEL))
    return SentenceTransformer(EMBEDDING_MODEL)


def prepare_anchors(model):
    all_texts = []
    all_categories = []
    for cat, examples in ANCHORS.items():
        for ex in examples:
            all_texts.append(ex)
            all_categories.append(cat)
    print("  Embedding {} positive anchors...".format(len(all_texts)))
    all_embeddings = model.encode(all_texts, show_progress_bar=False)

    # Negative anchors (contrastive)
    neg_texts = []
    neg_categories = []
    neg_embeddings = None
    for cat, examples in NEGATIVE_ANCHORS.items():
        for ex in examples:
            neg_texts.append(ex)
            neg_categories.append(cat)
    if neg_texts:
        print("  Embedding {} negative anchors...".format(len(neg_texts)))
        neg_embeddings = model.encode(neg_texts, show_progress_bar=False)
    else:
        print("  No negative anchors (contrastive scoring disabled)")

    # Neutral anchors (off-topic baseline)
    neutral_embeddings = None
    if NEUTRAL_ANCHORS:
        print("  Embedding {} neutral anchors...".format(len(NEUTRAL_ANCHORS)))
        neutral_embeddings = model.encode(NEUTRAL_ANCHORS, show_progress_bar=False)

    print("  Ready.\n")
    return (all_texts, all_categories, all_embeddings,
            neg_texts, neg_categories, neg_embeddings,
            neutral_embeddings)


# ---------------------------------------------------------------------------
# Contrastive adjustment (gap-based)
# ---------------------------------------------------------------------------

def _contrastive_adjust(pos_score, pos_cos, neg_cos, neutral_cos=0.0):
    """
    Adjust a score using the contrastive cosine gap.

    Uses COSINE similarity (surface-level) to compare how close the message
    is to positive vs negative anchors, regardless of what scoring mode
    produced pos_score (cosine or NLI).

    The penalty curve is smooth through gap=0, with the key transition zone
    between gap=-0.01 and gap=-0.02 where harmful and benign messages
    typically separate. Small negative gaps (>-0.01) are treated as cosine
    noise, not a definitive benign signal.

    Penalty zones:
      gap >= 0.10     →  0% (clearly closer to positive)
      gap -0.01..0.10 →  0..15% (mild — noise zone includes near-zero gaps)
      gap -0.02..-0.01→  15..55% (steep transition — the boundary zone)
      gap -0.05..-0.02→  55..80% (heavy — clearly closer to negative)
      gap < -0.05     →  80..95% (very heavy)

    Returns (adjusted_score, neg_cos).
    """
    # Off-topic check: closer to neutral than to domain anchors
    if neutral_cos > 0.4 and neutral_cos > pos_cos and neutral_cos > neg_cos:
        return pos_score * 0.15, neg_cos

    # Negatives not close at all — no adjustment
    if neg_cos < 0.4:
        return pos_score, neg_cos

    gap = pos_cos - neg_cos

    if gap >= 0.10:
        # Clearly closer to positive anchors — no penalty
        return pos_score, neg_cos

    elif gap >= -0.01:
        # Mild zone: treats small negative gaps as noise
        # 0% at gap=0.10, 15% at gap=-0.01
        fade = (0.10 - gap) / 0.11  # 0..1
        penalty = fade * 0.15

    elif gap >= -0.02:
        # Steep transition: the boundary between harmful and benign
        # 15% at gap=-0.01, 55% at gap=-0.02
        fade = (-0.01 - gap) / 0.01  # 0..1
        penalty = 0.15 + fade * 0.40

    elif gap >= -0.05:
        # Heavy: clearly closer to negative anchors
        # 55% at gap=-0.02, 80% at gap=-0.05
        fade = (-0.02 - gap) / 0.03  # 0..1
        penalty = 0.55 + fade * 0.25

    else:
        # Very heavy: 80% at gap=-0.05, up to 95%
        penalty = min(0.95, 0.80 + (abs(gap) - 0.05) * 2.0)

    return pos_score * (1.0 - penalty), neg_cos


def score_message(model, message, all_embeddings, all_texts, all_categories,
                  neg_embeddings=None, neg_texts=None, neutral_embeddings=None,
                  use_extraction=True):
    """
    Cosine KNN voting: merge pos + neg anchors, find K nearest, count positive ratio.

    Returns (results_list, extraction_info, knn_info) where knn_info is a dict with:
      knn_score, pos_in_k, k, max_pos, max_neg, top_k_details
    """
    from sentence_transformers.util import cos_sim
    import numpy as np

    extraction = None
    if use_extraction:
        extraction = extract_relevant_chunks(model, message, all_embeddings, all_texts)

    # Compute cosine sims for each view
    if extraction and extraction['was_extracted']:
        emb_extracted = model.encode(extraction['extracted_text'])
        emb_full = model.encode(message)
        emb_window = model.encode(extraction['sliding_window_text'])
        views = [emb_extracted, emb_full, emb_window]
    else:
        views = [model.encode(message)]

    # --- Positive anchor sims (best across views) ---
    n_pos = len(all_texts)
    pos_sims = [0.0] * n_pos
    for emb in views:
        sims = cos_sim(emb, all_embeddings)[0].tolist()
        for i, s in enumerate(sims):
            if s > pos_sims[i]:
                pos_sims[i] = s

    # --- Negative anchor sims (best across views) ---
    n_neg = len(neg_texts) if neg_texts else 0
    neg_sims = [0.0] * n_neg
    if neg_embeddings is not None and neg_texts:
        for emb in views:
            sims = cos_sim(emb, neg_embeddings)[0].tolist()
            for i, s in enumerate(sims):
                if s > neg_sims[i]:
                    neg_sims[i] = s

    # --- KNN voting: merge and find top-K ---
    # Build unified pool: (similarity, is_positive, text, category)
    pool = []
    for i in range(n_pos):
        pool.append((pos_sims[i], True, all_texts[i], all_categories[i]))
    for i in range(n_neg):
        pool.append((neg_sims[i], False, neg_texts[i], "NEGATIVE"))

    pool.sort(key=lambda x: x[0], reverse=True)
    top_k = pool[:COSINE_KNN_K]

    pos_in_k = sum(1 for _, is_pos, _, _ in top_k if is_pos)
    knn_score = pos_in_k / len(top_k) if top_k else 0.0
    max_pos = max(pos_sims) if pos_sims else 0.0
    max_neg = max(neg_sims) if neg_sims else 0.0

    knn_info = {
        "knn_score": knn_score,
        "pos_in_k": pos_in_k,
        "k": len(top_k),
        "max_pos": max_pos,
        "max_neg": max_neg,
    }

    # Results sorted by positive similarity (for display)
    results = sorted(
        zip(pos_sims, all_texts, all_categories),
        key=lambda x: x[0], reverse=True)
    return results, extraction, knn_info


# ---------------------------------------------------------------------------
# NLI cross-encoder scoring
# ---------------------------------------------------------------------------

_cross_encoder = None
_entailment_idx = None
_contradiction_idx = None

def _load_cross_encoder():
    """Lazy-load NLI cross-encoder model for intent matching."""
    global _cross_encoder, _entailment_idx, _contradiction_idx
    if _cross_encoder is not None:
        return _cross_encoder
    try:
        from sentence_transformers import CrossEncoder
    except ImportError:
        print("\n  ERROR: sentence-transformers version doesn't support CrossEncoder.")
        print("  Run: pip install -U sentence-transformers")
        sys.exit(1)
    import numpy as np
    print("  Loading NLI cross-encoder model: {}...".format(NLI_MODEL))
    _cross_encoder = CrossEncoder(NLI_MODEL)

    cal_pairs = [
        ["A person is building a weapon", "Someone wants to make a weapon"],       # entailment
        ["I love eating pizza", "Someone wants to make a weapon"],                  # neutral
        ["I want to protect people from weapons", "Someone wants to make a weapon"],  # contradiction
    ]
    cal_logits = np.array(_cross_encoder.predict(cal_pairs, apply_softmax=False))
    cal_exp = np.exp(cal_logits - np.max(cal_logits, axis=1, keepdims=True))
    cal_probs = cal_exp / cal_exp.sum(axis=1, keepdims=True)

    entail_row = cal_probs[0]   # should have high entailment
    neutral_row = cal_probs[1]  # should have high neutral
    contra_row = cal_probs[2]   # should have high contradiction

    # Find entailment index: highest on entail_row
    best_idx = int(np.argmax(entail_row))
    if neutral_row[best_idx] < 0.3 and contra_row[best_idx] < 0.3:
        _entailment_idx = best_idx
    else:
        gaps = entail_row - np.maximum(neutral_row, contra_row)
        _entailment_idx = int(np.argmax(gaps))

    # Find contradiction index: highest on contra_row, excluding entailment_idx
    remaining = list(range(cal_probs.shape[1]))
    remaining.remove(_entailment_idx)
    _contradiction_idx = max(remaining, key=lambda i: contra_row[i])

    print("  Entailment index: {} (cal: entail={:.3f}, neutral={:.3f}, contra={:.3f})".format(
        _entailment_idx,
        entail_row[_entailment_idx],
        neutral_row[_entailment_idx],
        contra_row[_entailment_idx]))
    print("  Contradiction index: {} (cal: entail={:.3f}, neutral={:.3f}, contra={:.3f})".format(
        _contradiction_idx,
        entail_row[_contradiction_idx],
        neutral_row[_contradiction_idx],
        contra_row[_contradiction_idx]))

    return _cross_encoder


def _nli_entailment_scores(pairs):
    """Compute entailment probability for each pair using NLI cross-encoder."""
    import numpy as np
    xenc = _load_cross_encoder()
    logits = xenc.predict(pairs, apply_softmax=False)
    logits = np.array(logits)
    if logits.ndim == 1:
        logits = logits.reshape(1, -1)
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)
    return probs[:, _entailment_idx].tolist()


def _nli_net_scores(pairs):
    """
    Compute net NLI score = max(0, entailment - contradiction) for each pair.

    Uses the full NLI 3-class signal:
    - A→B entailment high, contradiction low → strong positive score
    - A→not(B) contradiction high, entailment low → score = 0
    - A unrelated to B (neutral) → both low → score ≈ 0

    This is equivalent to running both A→B and A→not(B) checks in a single call.
    """
    import numpy as np
    xenc = _load_cross_encoder()
    logits = xenc.predict(pairs, apply_softmax=False)
    logits = np.array(logits)
    if logits.ndim == 1:
        logits = logits.reshape(1, -1)
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)
    entail = probs[:, _entailment_idx]
    contra = probs[:, _contradiction_idx]
    net = np.maximum(0.0, entail - contra)
    return net.tolist()


def _nli_proposition_score(views, proposition):
    """
    Score message views directly against the proposition using NLI net scores.

    Two key improvements over naive entailment-only:
    1. Declarative prefix: wraps views in "The user states: ..." so NLI sees
       declarative→declarative instead of question→statement (fixes structural mismatch)
    2. Net score = max(0, entailment - contradiction): uses full 3-class NLI signal.
       Equivalent to running A→B and A→not(B) checks in a single call.

    Scores bidirectionally (message→prop and prop→message), takes max per view,
    then returns the overall max across all views.

    Returns (prop_score, best_view_text)
    """
    best_score = 0.0
    best_view = ""

    for view_text, view_label in views:
        # Declarative prefix: convert questions to statements for NLI
        decl_view = NLI_DECLARATIVE_PREFIX + view_text
        pairs = [
            [decl_view, proposition],   # message entails proposition?
            [proposition, decl_view],   # proposition entails message?
        ]
        scores = _nli_net_scores(pairs)
        view_score = max(scores)
        if view_score > best_score:
            best_score = view_score
            best_view = view_label

    return best_score, best_view


# ---------------------------------------------------------------------------
# NLI-only scoring (--nli mode) with proposition-guided extraction
# ---------------------------------------------------------------------------

NLI_RETRIEVE_K = 40   # cosine pre-filter: top 40 from unified pool (pos + neg)
NLI_VOTE_K = 20       # re-rank by NLI, vote on top 20
NLI_FWD_WEIGHT = 0.7  # asymmetric: 70% forward (message→anchor), 30% backward
# NLI_ABSTAIN_MARGIN is loaded from config (default 0.15)
COSINE_TOP_K_FRAC = 0.50  # fraction of anchors to average (top-50%)
COSINE_TOP_K_MIN = 10     # minimum anchors to average

def _top_k_cosine_avg(scores, k=None):
    """Average of top-K cosine scores. K defaults to 50% of total (min 10)."""
    if not scores:
        return 0.0
    if k is None:
        k = max(COSINE_TOP_K_MIN, int(len(scores) * COSINE_TOP_K_FRAC))
    sorted_scores = sorted(scores, reverse=True)
    top = sorted_scores[:min(k, len(sorted_scores))]
    return sum(top) / len(top)


def _score_nli_core(message, all_texts, all_categories, model=None,
                    all_embeddings=None, neg_texts=None, neg_embeddings=None,
                    neutral_embeddings=None,
                    do_spellcheck=True, use_extraction=True):
    """
    NLI KNN voting: retrieve candidates by cosine, re-rank by NLI, vote.

    Pipeline:
      1. Spell check + extraction + build views
      2. Proposition NLI (net scores, declarative prefix)
      3. Cosine retrieval: top NLI_RETRIEVE_K from unified pool (pos + neg)
      4. NLI re-rank: asymmetric net scores (E-C) on retrieved candidates
         - 70% forward (message→anchor) + 30% backward (anchor→message)
      5. Vote on top NLI_VOTE_K by NLI score:
         - pos_evidence = sum of NLI scores for positive anchors
         - neg_evidence = sum of NLI scores for negative anchors
         - nli_knn_score = pos_evidence / (pos_evidence + neg_evidence + epsilon)
      6. Final score = max(prop_score, nli_knn_score)
      7. Abstain detection when evidence margin is small

    Returns (results, corrected, corrections, extraction, neg_cos, final_score, debug)
    """
    # Step 0: spell correction
    corrected = message
    corrections = []
    if do_spellcheck:
        corrected, corrections = correct_spelling(message)

    # Step 1: proposition-guided extraction
    extraction = None
    if use_extraction and model is not None and all_embeddings is not None:
        extraction = extract_relevant_chunks(model, corrected, all_embeddings, all_texts)

    _load_cross_encoder()

    # Step 2: Build views
    views = []
    if extraction and extraction['was_extracted']:
        views.append((extraction['extracted_text'], "[extracted chunk]"))
        if extraction['sliding_window_score'] > 0:
            views.append((extraction['sliding_window_text'], "[sliding window]"))
        for sent in extraction['all_sentences']:
            label = sent if len(sent) <= 40 else sent[:37] + "..."
            views.append((sent, label))
        views.append((corrected, "[full message]"))
    else:
        sentences = _split_into_sentences(corrected)
        for sent in sentences:
            label = sent if len(sent) <= 40 else sent[:37] + "..."
            views.append((sent, label))

    # Step 3: Proposition NLI (~2 calls per view)
    prop_score, prop_view = _nli_proposition_score(views, PROPOSITION)

    # Step 4: Cosine retrieval from UNIFIED pool (pos + neg)
    # Build unified pool: (index, is_positive, text, category, cosine_score)
    neg_cos = 0.0
    pos_cos = 0.0
    best_view_text = views[0][0]
    unified_pool = []

    n_pos = len(all_texts)
    n_neg = len(neg_texts) if neg_texts else 0

    if model is not None and all_embeddings is not None:
        from sentence_transformers.util import cos_sim

        pos_cosines = [0.0] * n_pos
        neg_cosines = [0.0] * n_neg

        for view_text, _ in views:
            emb = model.encode(view_text)

            # Positive cosines
            p_sims = cos_sim(emb, all_embeddings)[0].tolist()
            view_pos_cos = _top_k_cosine_avg(p_sims)
            if view_pos_cos > pos_cos:
                pos_cos = view_pos_cos
                best_view_text = view_text
            for i, s in enumerate(p_sims):
                if s > pos_cosines[i]:
                    pos_cosines[i] = s

            # Negative cosines
            if neg_embeddings is not None and neg_texts:
                n_sims = cos_sim(emb, neg_embeddings)[0].tolist()
                view_neg_cos = _top_k_cosine_avg(n_sims)
                if view_neg_cos > neg_cos:
                    neg_cos = view_neg_cos
                for i, s in enumerate(n_sims):
                    if s > neg_cosines[i]:
                        neg_cosines[i] = s

        # Build unified pool
        for i in range(n_pos):
            unified_pool.append({
                "idx": i, "is_positive": True,
                "text": all_texts[i], "category": all_categories[i],
                "cosine": pos_cosines[i],
            })
        for i in range(n_neg):
            unified_pool.append({
                "idx": n_pos + i, "is_positive": False,
                "text": neg_texts[i], "category": "NEGATIVE",
                "cosine": neg_cosines[i],
            })

    gap = pos_cos - neg_cos

    # Sort by cosine, take top NLI_RETRIEVE_K for NLI re-ranking
    unified_pool.sort(key=lambda x: x["cosine"], reverse=True)
    retrieve_k = min(NLI_RETRIEVE_K, len(unified_pool))
    candidates = unified_pool[:retrieve_k]

    # Step 5: NLI on retrieved candidates — asymmetric net scores (E - C)
    # Forward: message→anchor (does message entail anchor?)
    # Backward: anchor→message (does anchor entail message?)
    # Combined: 0.7 × forward + 0.3 × backward
    if candidates:
        cand_texts = [c["text"] for c in candidates]
        pairs_fwd = [[best_view_text, t] for t in cand_texts]
        pairs_bwd = [[t, best_view_text] for t in cand_texts]
        net_fwd = _nli_net_scores(pairs_fwd)
        net_bwd = _nli_net_scores(pairs_bwd)

        bwd_weight = 1.0 - NLI_FWD_WEIGHT
        for i, cand in enumerate(candidates):
            cand["nli_score"] = NLI_FWD_WEIGHT * net_fwd[i] + bwd_weight * net_bwd[i]
            cand["nli_fwd"] = net_fwd[i]
            cand["nli_bwd"] = net_bwd[i]

    # Step 6: Re-rank by NLI score, vote on top NLI_VOTE_K
    candidates.sort(key=lambda x: x.get("nli_score", 0), reverse=True)
    vote_k = min(NLI_VOTE_K, len(candidates))
    voters = candidates[:vote_k]

    pos_evidence = 0.0
    neg_evidence = 0.0
    pos_in_k = 0
    neg_in_k = 0
    voter_details = []

    for v in voters:
        nli_s = v.get("nli_score", 0)
        if v["is_positive"]:
            pos_evidence += nli_s
            pos_in_k += 1
        else:
            neg_evidence += nli_s
            neg_in_k += 1
        voter_details.append({
            "text": v["text"][:60], "category": v["category"],
            "is_positive": v["is_positive"], "nli_score": nli_s,
            "cosine": v["cosine"],
        })

    total_evidence = pos_evidence + neg_evidence + 1e-10
    nli_knn_score = pos_evidence / total_evidence

    # Step 7: Evidence margin for abstain detection
    evidence_margin = abs(pos_evidence - neg_evidence) / total_evidence
    abstain = evidence_margin < NLI_ABSTAIN_MARGIN

    # Step 8: Final score = max(prop_score, nli_knn_score)
    # No gating needed — NLI KNN handles refusals via negative anchor voting
    final_score = max(prop_score, nli_knn_score)

    if abstain:
        action = "abstain(margin={:.2f})".format(evidence_margin)
    elif final_score >= MATCH_THRESHOLD:
        action = "match"
    elif final_score >= WARNING_THRESHOLD:
        action = "warning"
    else:
        action = "no_match"

    debug = {
        "method": "nli",
        "prop_score": prop_score,
        "nli_knn_score": nli_knn_score,
        "pos_evidence": pos_evidence,
        "neg_evidence": neg_evidence,
        "evidence_margin": evidence_margin,
        "pos_in_k": pos_in_k,
        "neg_in_k": neg_in_k,
        "vote_k": vote_k,
        "retrieve_k": retrieve_k,
        "combined": final_score,
        "gap": gap,
        "pos_cos": pos_cos,
        "neg_cos": neg_cos,
        "abstain": abstain,
        "action": action,
        "voters": voter_details[:5],  # top 5 for display
    }

    # Build results — sorted by NLI score (candidates first, then rest by cosine)
    # Each result carries the overall final_score for verdict purposes
    anchor_order = []
    seen = set()
    for c in candidates:
        if c["is_positive"]:
            anchor_order.append(c["idx"])
            seen.add(c["idx"])
    for i in range(n_pos):
        if i not in seen:
            anchor_order.append(i)
    results = [(final_score, all_texts[i], all_categories[i], "") for i in anchor_order]

    return results, corrected, corrections, extraction, neg_cos, final_score, debug


def score_message_nli(message, all_texts, all_categories, model=None,
                      all_embeddings=None, neg_texts=None, neg_embeddings=None,
                      neutral_embeddings=None,
                      do_spellcheck=True, use_extraction=True):
    """
    NLI KNN voting: retrieve by cosine, re-rank by NLI, vote.

    Combines proposition NLI with anchor NLI KNN voting. Negative anchors
    compete directly with positive anchors, naturally handling refusals
    without gating or suppression.

    Returns (results, corrected, corrections, extraction_info, neg_cos, final_score)
    """
    results, corrected, corrections, extraction, neg_cos, final_score, debug = _score_nli_core(
        message, all_texts, all_categories, model=model,
        all_embeddings=all_embeddings, neg_texts=neg_texts, neg_embeddings=neg_embeddings,
        neutral_embeddings=neutral_embeddings,
        do_spellcheck=do_spellcheck, use_extraction=use_extraction)
    return results, corrected, corrections, extraction, neg_cos, final_score


# ---------------------------------------------------------------------------
# Hybrid scoring (weighted blend: NLI + Cosine KNN)
# ---------------------------------------------------------------------------

def score_message_hybrid(message, all_texts, all_categories, model=None,
                         all_embeddings=None, neg_texts=None, neg_embeddings=None,
                         neutral_embeddings=None,
                         do_spellcheck=True, use_extraction=True):
    """
    Hybrid scoring: weighted blend of NLI entailment and Cosine KNN voting.

    hybrid_score = nli_weight * nli_score + (1 - nli_weight) * knn_score

    Default: 75% NLI + 25% cosine KNN (configurable via hybrid_nli_weight in config).
    Combines NLI's semantic understanding with KNN's neighborhood-based robustness.

    Returns (results, corrected, corrections, extraction_info, neg_cos, debug_info)
    """
    import numpy as np

    nli_weight = HYBRID_NLI_WEIGHT
    cos_weight = 1.0 - nli_weight

    # --- NLI component ---
    nli_results, corrected, corrections, extraction, neg_cos, nli_score, nli_debug = \
        _score_nli_core(
            message, all_texts, all_categories, model=model,
            all_embeddings=all_embeddings, neg_texts=neg_texts,
            neg_embeddings=neg_embeddings, neutral_embeddings=neutral_embeddings,
            do_spellcheck=do_spellcheck, use_extraction=use_extraction)

    # --- Cosine KNN component ---
    knn_score = 0.0
    knn_info = {}
    if model is not None and all_embeddings is not None:
        _, _, knn_info = score_message(
            model, corrected, all_embeddings, all_texts, all_categories,
            neg_embeddings=neg_embeddings, neg_texts=neg_texts,
            neutral_embeddings=neutral_embeddings,
            use_extraction=use_extraction)
        knn_score = knn_info.get("knn_score", 0.0)

    # --- Weighted blend ---
    hybrid_score = nli_weight * nli_score + cos_weight * knn_score

    # Build results list with blended score
    anchor_order = sorted(range(len(all_texts)),
                          key=lambda i: nli_results[i][0] if i < len(nli_results) else 0,
                          reverse=True)
    results = [(hybrid_score, all_texts[i], all_categories[i], "") for i in anchor_order]

    debug = {
        "method": "hybrid",
        "nli_score": nli_score,
        "knn_score": knn_score,
        "nli_weight": nli_weight,
        "cos_weight": cos_weight,
        "combined": hybrid_score,
        "prop_score": nli_debug.get("prop_score", 0),
        "nli_knn_score": nli_debug.get("nli_knn_score", 0),
        "pos_evidence": nli_debug.get("pos_evidence", 0),
        "neg_evidence": nli_debug.get("neg_evidence", 0),
        "evidence_margin": nli_debug.get("evidence_margin", 0),
        "nli_pos_in_k": nli_debug.get("pos_in_k", 0),
        "nli_neg_in_k": nli_debug.get("neg_in_k", 0),
        "nli_vote_k": nli_debug.get("vote_k", 0),
        "nli_action": nli_debug.get("action", ""),
        "abstain": nli_debug.get("abstain", False),
        "gap": nli_debug.get("gap", 0),
        "cos_pos_in_k": knn_info.get("pos_in_k", 0),
        "cos_k": knn_info.get("k", 0),
    }
    return results, corrected, corrections, extraction, neg_cos, debug


# ---------------------------------------------------------------------------
# LLM Judge scoring
# ---------------------------------------------------------------------------

_llm_config = None

def _load_llm_config(silent=False):
    """Load LLM judge config from config_<n>.ini."""
    global _llm_config
    if _llm_config is not None:
        return _llm_config

    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, "config_{}.ini".format(SCRIPT_NAME))

    if not os.path.exists(config_path):
        if not silent:
            print("\n  {}ERROR: LLM judge config not found: {}{}".format(
                "\033[91m", config_path, RESET))
            print("  Create it with:")
            print("    [llm_judge]")
            print("    provider = anthropic")
            print("    model = claude-sonnet-4-20250514")
            print("    api_key = sk-ant-...")
            print("    proposition = Your proposition here")
            print("\n  Or regenerate with: python semantic_anchor_generator.py -name {} -gs".format(
                SCRIPT_NAME))
        sys.exit(1)

    import configparser
    config = configparser.ConfigParser()
    config.read(config_path)

    _llm_config = {
        "provider": config.get("llm_judge", "provider"),
        "model": config.get("llm_judge", "model"),
        "api_key": config.get("llm_judge", "api_key"),
        "proposition": config.get("llm_judge", "proposition", fallback=PROPOSITION),
        "base_url": config.get("llm_judge", "base_url",
                               fallback="http://localhost:11434"),
    }
    return _llm_config


def _call_llm_judge(message, proposition, top_anchors=None):
    """
    Call an LLM to judge whether a message matches the proposition.

    Returns dict: {match: bool, reasoning: str, verdict: str}
    """
    config = _load_llm_config()
    provider = config["provider"].lower()
    model = config["model"]
    api_key = config["api_key"]

    # Build the prompt
    anchors_context = ""
    if top_anchors:
        anchor_lines = []
        for score, text, cat, *rest in top_anchors[:5]:
            anchor_lines.append("  - [{:.3f}] ({}) \"{}\"".format(score, cat, text))
        anchors_context = (
            "\n\nFor reference, these are the closest semantic anchor matches "
            "(pre-scored by embedding similarity):\n" + "\n".join(anchor_lines)
        )

    system_prompt = (
        "You are a precise content classifier. Your task is to determine whether "
        "a user message matches a given proposition. You must distinguish between:\n"
        "- Messages that genuinely request/intend what the proposition describes\n"
        "- Messages that use similar vocabulary but have legitimate/benign intent\n"
        "- Educational questions ABOUT the topic (not requesting to perform it)\n\n"
        "Respond with ONLY valid JSON, no other text."
    )

    user_prompt = (
        'PROPOSITION: "{}"\n\n'
        'USER MESSAGE: "{}"\n'
        '{}\n'
        'Does this message match the proposition? Consider:\n'
        '1. Does the user intend to perform the action described in the proposition?\n'
        '2. Or are they asking for education, defense, their own account, or general info?\n'
        '3. First-person ownership ("my account", "my password") vs third-party targeting\n\n'
        'Respond with JSON:\n'
        '{{\n'
        '  "match": true or false,\n'
        '  "reasoning": "brief explanation"\n'
        '}}'
    ).format(proposition, message, anchors_context)

    try:
        import httpx

        if provider == "anthropic":
            resp = httpx.post(
                "https://api.anthropic.com/v1/messages",
                headers={
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json",
                },
                json={
                    "model": model,
                    "max_tokens": 300,
                    "messages": [{"role": "user", "content": user_prompt}],
                    "system": system_prompt,
                },
                timeout=30.0,
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["content"][0]["text"]

        elif provider == "openai":
            resp = httpx.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": "Bearer " + api_key,
                    "Content-Type": "application/json",
                },
                json={
                    "model": model,
                    "max_tokens": 300,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    "temperature": 0,
                },
                timeout=30.0,
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["choices"][0]["message"]["content"]

        elif provider == "gemini" or provider == "google":
            # Google Gemini API
            url = "https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent?key={}".format(
                model, api_key)
            resp = httpx.post(
                url,
                headers={"Content-Type": "application/json"},
                json={
                    "contents": [{"parts": [{"text": system_prompt + "\n\n" + user_prompt}]}],
                    "generationConfig": {
                        "maxOutputTokens": 300,
                        "temperature": 0,
                    },
                },
                timeout=30.0,
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["candidates"][0]["content"]["parts"][0]["text"]

        elif provider == "grok" or provider == "xai":
            # xAI Grok API (OpenAI-compatible)
            resp = httpx.post(
                "https://api.x.ai/v1/chat/completions",
                headers={
                    "Authorization": "Bearer " + api_key,
                    "Content-Type": "application/json",
                },
                json={
                    "model": model,
                    "max_tokens": 300,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    "temperature": 0,
                },
                timeout=30.0,
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["choices"][0]["message"]["content"]

        elif provider in ("ollama", "local", "lmstudio", "vllm"):
            # Local model via Ollama or any OpenAI-compatible local server
            base_url = config.get("base_url", "http://localhost:11434")
            if provider == "ollama":
                # Ollama native API
                resp = httpx.post(
                    base_url.rstrip("/") + "/api/chat",
                    json={
                        "model": model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        "stream": False,
                        "options": {"temperature": 0},
                    },
                    timeout=120.0,
                )
                resp.raise_for_status()
                data = resp.json()
                text = data["message"]["content"]
            else:
                # LM Studio / vLLM / any OpenAI-compatible local server
                resp = httpx.post(
                    base_url.rstrip("/") + "/v1/chat/completions",
                    headers={"Content-Type": "application/json"},
                    json={
                        "model": model,
                        "max_tokens": 300,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        "temperature": 0,
                    },
                    timeout=120.0,
                )
                resp.raise_for_status()
                data = resp.json()
                text = data["choices"][0]["message"]["content"]

        else:
            return {"match": False,
                    "reasoning": "Unsupported provider: " + provider,
                    "verdict": "ERROR"}

        # Parse JSON response
        import re
        json_match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
        else:
            result = json.loads(text)

        is_match = result.get("match", False)
        reasoning = result.get("reasoning", "")

        verdict = "MATCH" if is_match else "NO MATCH"

        return {
            "match": is_match,
            "reasoning": reasoning,
            "verdict": verdict,
        }

    except Exception as e:
        return {
            "match": False,
            "reasoning": "LLM call failed: {}".format(str(e)),
            "verdict": "ERROR",
        }


def score_message_llm(message, proposition, model=None, all_embeddings=None,
                      all_texts=None, all_categories=None,
                      neg_embeddings=None, neutral_embeddings=None,
                      do_spellcheck=True, use_extraction=True):
    """
    LLM-as-judge scoring. Optionally uses cosine pre-ranking to provide
    context anchors to the LLM.

    Returns (llm_result, corrected, corrections, cosine_results)
    where llm_result is the dict from _call_llm_judge.
    """
    corrected = message
    corrections = []
    if do_spellcheck:
        corrected, corrections = correct_spelling(message)

    # Optional: cosine pre-rank for context
    cosine_results = None
    if model is not None and all_embeddings is not None:
        cosine_results, _, _ = score_message(
            model, corrected, all_embeddings, all_texts, all_categories,
            neg_embeddings=neg_embeddings, neutral_embeddings=neutral_embeddings,
            use_extraction=use_extraction)

    llm_result = _call_llm_judge(corrected, proposition, top_anchors=cosine_results)

    return llm_result, corrected, corrections, cosine_results


# ---------------------------------------------------------------------------
# Display
# ---------------------------------------------------------------------------

def print_banner():
    title = "SEMANTIC ANCHORS: " + SCRIPT_NAME.upper()
    w = 60
    print("")
    print(BOLD + "\u2554" + "\u2550" * w + "\u2557")
    print("\u2551" + title.center(w) + "\u2551")
    print("\u255a" + "\u2550" * w + "\u255d" + RESET)
    print("  Proposition: \"{}\"".format(PROPOSITION))
    print("  Role: {}".format(ANCHOR_ROLE))
    print("  NLI prefix: \"{}\"".format(NLI_DECLARATIVE_PREFIX))
    print("  Thresholds: match={}, warning={}, KNN K={}".format(
        MATCH_THRESHOLD, WARNING_THRESHOLD, COSINE_KNN_K))
    print("  Hybrid blend: {:.0f}% NLI + {:.0f}% cosine KNN".format(
        HYBRID_NLI_WEIGHT * 100, (1 - HYBRID_NLI_WEIGHT) * 100))
    print("  NLI KNN: retrieve={}, vote={}, fwd_weight={}, abstain_margin={}".format(
        NLI_RETRIEVE_K, NLI_VOTE_K, NLI_FWD_WEIGHT, NLI_ABSTAIN_MARGIN))
    print("")


def print_examples():
    total = sum(len(v) for v in ANCHORS.values())
    print("\n{}Positive Anchors ({} total){}".format(BOLD, total, RESET))
    print("=" * 70)
    for cat, examples in ANCHORS.items():
        print("\n{}{}  [{}]{} ({} examples)".format(BOLD, BLUE, cat, RESET, len(examples)))
        print("  " + "-" * 60)
        for i, ex in enumerate(examples, 1):
            print("  {}{:>3}.{} {}".format(DIM, i, RESET, ex))

    if NEGATIVE_ANCHORS:
        neg_total = sum(len(v) for v in NEGATIVE_ANCHORS.values())
        print("\n\n{}Negative (Contrastive) Anchors ({} total){}".format(BOLD, neg_total, RESET))
        print("=" * 70)
        for cat, examples in NEGATIVE_ANCHORS.items():
            print("\n{}{}  [{}]{} ({} examples)".format(BOLD, GREEN, cat, RESET, len(examples)))
            print("  " + "-" * 60)
            for i, ex in enumerate(examples, 1):
                print("  {}{:>3}.{} {}".format(DIM, i, RESET, ex))

    if NEUTRAL_ANCHORS:
        print("\n\n{}Neutral (Off-topic Baseline) Anchors ({} total){}".format(
            BOLD, len(NEUTRAL_ANCHORS), RESET))
        print("=" * 70)
        for i, ex in enumerate(NEUTRAL_ANCHORS, 1):
            print("  {}{:>3}.{} {}".format(DIM, i, RESET, ex))
    print()


def format_verdict(top_score, neg_cos=0.0):
    neg_info = ""
    if neg_cos > 0.01:
        neg_info = "  {}[neg_cos={:.3f}]{}".format(DIM, neg_cos, RESET)
    if top_score >= MATCH_THRESHOLD:
        return "{}{}\u25a0 MATCH{} (score {:.4f} \u2265 {}){}".format(
            RED, BOLD, RESET, top_score, MATCH_THRESHOLD, neg_info)
    elif top_score >= WARNING_THRESHOLD:
        return "{}{}\u25a0 WARNING{} (score {:.4f} \u2265 {}){}".format(
            YELLOW, BOLD, RESET, top_score, WARNING_THRESHOLD, neg_info)
    else:
        return "{}{}\u25a0 NO MATCH{} (score {:.4f} < {}){}".format(
            GREEN, BOLD, RESET, top_score, WARNING_THRESHOLD, neg_info)


def format_verdict_cosine(knn_score, pos_in_k, k, max_pos, max_neg):
    """Format verdict for cosine KNN voting."""
    verdict = _verdict_for_cosine_gap(knn_score)
    detail = "knn={:.0f}/{} ({:.0%})  max_pos={:.3f}  max_neg={:.3f}".format(
        pos_in_k, k, knn_score, max_pos, max_neg)
    if verdict == "MATCH":
        return "{}{}\u25a0 MATCH{} ({})".format(RED, BOLD, RESET, detail)
    elif verdict == "WARNING":
        return "{}{}\u25a0 WARNING{} ({})".format(YELLOW, BOLD, RESET, detail)
    else:
        return "{}{}\u25a0 NO MATCH{} ({})".format(GREEN, BOLD, RESET, detail)


# --- Cosine display ---

def display_default(results, extraction=None, top_n=3, verbose_extract=False,
                    neg_score=0.0, knn_info=None):
    if extraction:
        _display_extraction_info(extraction, verbose=verbose_extract)
    if knn_info is not None:
        print("\n  {}\n".format(format_verdict_cosine(
            knn_info["knn_score"], knn_info["pos_in_k"], knn_info["k"],
            knn_info["max_pos"], knn_info["max_neg"])))
    else:
        print("\n  {}\n".format(format_verdict(results[0][0], neg_score)))
    print("  {:<6} {:>8}  {:<35} {}".format("Rank", "Score", "Category", "Nearest Anchor"))
    print("  " + "-" * 95)
    for rank, (score, text, cat) in enumerate(results[:top_n], 1):
        cs = color_score(score)
        dt = text if len(text) <= 55 else text[:52] + "..."
        dc = cat if len(cat) <= 33 else cat[:30] + "..."
        print("  {:<6} {:>17}  {}{:<35}{} \"{}\"".format(rank, cs, DIM, dc, RESET, dt))
    print()


def display_verbose(results, extraction=None, neg_score=0.0, knn_info=None):
    if extraction:
        _display_extraction_info(extraction, verbose=True)
    if knn_info is not None:
        print("\n  {}\n".format(format_verdict_cosine(
            knn_info["knn_score"], knn_info["pos_in_k"], knn_info["k"],
            knn_info["max_pos"], knn_info["max_neg"])))
    else:
        print("\n  {}\n".format(format_verdict(results[0][0], neg_score)))
    print("  {:<5} {:>8}  {:<35} {}".format("#", "Score", "Category", "Anchor Text"))
    print("  " + "=" * 100)
    for rank, (score, text, cat) in enumerate(results, 1):
        cs = color_score(score)
        dc = cat if len(cat) <= 33 else cat[:30] + "..."
        zone = ""
        if score >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif score >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17}  {}{:<35}{} \"{}\"{}".format(
            rank, cs, DIM, dc, RESET, text, zone))
    above_m = sum(1 for s, _, _ in results if s >= MATCH_THRESHOLD)
    above_w = sum(1 for s, _, _ in results if s >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {}{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w, RESET))
    print()


# --- Reranked display ---


# --- NLI display ---

def display_default_nli(results, corrected, corrections, extraction=None, top_n=3, neg_score=0.0):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction)
    print("\n  {}\n".format(format_verdict(top_score, neg_score)))
    print("  {:<6} {:>8}  {:<30} {}".format(
        "Rank", "NLI", "Category", "Nearest Anchor"))
    print("  " + "-" * 100)
    for rank, (score, text, cat, best_src) in enumerate(results[:top_n], 1):
        cs = color_score(score)
        dt = text if len(text) <= 50 else text[:47] + "..."
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        print("  {:<6} {:>17}  {}{:<30}{} \"{}\"".format(
            rank, cs, DIM, dc, RESET, dt))
        if best_src and best_src not in (corrected, "[full message]"):
            bs = best_src if len(best_src) <= 65 else best_src[:62] + "..."
            print("  {}       matched via: {}{} ".format(DIM, bs, RESET))
    print()


def display_verbose_nli(results, corrected, corrections, extraction=None, neg_score=0.0):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction, verbose=True)
    print("\n  {}\n".format(format_verdict(top_score, neg_score)))
    print("  {:<5} {:>8}  {:<30} {}".format("#", "NLI", "Category", "Anchor Text"))
    print("  " + "=" * 105)
    for rank, (score, text, cat, best_src) in enumerate(results, 1):
        cs = color_score(score)
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        zone = ""
        if score >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif score >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17}  {}{:<30}{} \"{}\"{}".format(
            rank, cs, DIM, dc, RESET, text, zone))
    above_m = sum(1 for s, _, _, _ in results if s >= MATCH_THRESHOLD)
    above_w = sum(1 for s, _, _, _ in results if s >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {} | "
          "NLI on ALL anchors{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w,
        RESET))
    print()


# --- Hybrid display ---

def display_default_hybrid(results, corrected, corrections, extraction=None,
                           top_n=3, neg_score=0.0, debug=None):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction)

    # Show hybrid debug
    if debug:
        nli_sc = debug.get("nli_score", 0)
        knn_sc = debug.get("knn_score", 0)
        nli_w = debug.get("nli_weight", 0.75)
        cos_w = debug.get("cos_weight", 0.25)
        combined = debug.get("combined", 0)
        cos_pos = debug.get("cos_pos_in_k", 0)
        cos_k = debug.get("cos_k", 0)
        nli_pos = debug.get("nli_pos_in_k", 0)
        nli_neg = debug.get("nli_neg_in_k", 0)
        nli_action = debug.get("nli_action", "")
        print("  {}Hybrid: nli={:.3f} ({}+/{}−) x {:.0f}% + cos_knn={:.3f} ({}/{}) x {:.0f}% = {:.3f} [{}]{}".format(
            DIM, nli_sc, nli_pos, nli_neg, nli_w * 100,
            knn_sc, cos_pos, cos_k, cos_w * 100, combined,
            nli_action, RESET))

    print("\n  {}\n".format(format_verdict(top_score, neg_score)))
    print("  {:<6} {:>8}  {:<30} {}".format(
        "Rank", "NLI", "Category", "Nearest Anchor"))
    print("  " + "-" * 100)
    for rank, (score, text, cat, best_src) in enumerate(results[:top_n], 1):
        cs = color_score(score)
        dt = text if len(text) <= 50 else text[:47] + "..."
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        print("  {:<6} {:>17}  {}{:<30}{} \"{}\"".format(
            rank, cs, DIM, dc, RESET, dt))
        if best_src and best_src not in (corrected, "[full message]"):
            bs = best_src if len(best_src) <= 65 else best_src[:62] + "..."
            print("  {}       matched via: {}{} ".format(DIM, bs, RESET))
    print()


def display_verbose_hybrid(results, corrected, corrections, extraction=None,
                           neg_score=0.0, debug=None):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction, verbose=True)

    if debug:
        nli_sc = debug.get("nli_score", 0)
        knn_sc = debug.get("knn_score", 0)
        nli_w = debug.get("nli_weight", 0.75)
        cos_w = debug.get("cos_weight", 0.25)
        combined = debug.get("combined", 0)
        cos_pos = debug.get("cos_pos_in_k", 0)
        cos_k = debug.get("cos_k", 0)
        prop = debug.get("prop_score", 0)
        nli_pos = debug.get("nli_pos_in_k", 0)
        nli_neg = debug.get("nli_neg_in_k", 0)
        pos_ev = debug.get("pos_evidence", 0)
        neg_ev = debug.get("neg_evidence", 0)
        margin = debug.get("evidence_margin", 0)
        nli_action = debug.get("nli_action", "")
        print("  {}Hybrid debug: nli={:.3f} (prop={:.3f} knn={}+/{}− ev={:.2f}/{:.2f} margin={:.2f}) x {:.0f}%".format(
            DIM, nli_sc, prop, nli_pos, nli_neg, pos_ev, neg_ev, margin, nli_w * 100))
        print("               + cos_knn={:.3f} ({}/{}) x {:.0f}% = {:.3f} [{}]{}".format(
            knn_sc, cos_pos, cos_k, cos_w * 100, combined, nli_action, RESET))

    print("\n  {}\n".format(format_verdict(top_score, neg_score)))
    print("  {:<5} {:>8}  {:<30} {}".format("#", "NLI", "Category", "Anchor Text"))
    print("  " + "=" * 105)
    for rank, (score, text, cat, best_src) in enumerate(results, 1):
        cs = color_score(score)
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        zone = ""
        if score >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif score >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17}  {}{:<30}{} \"{}\"{}".format(
            rank, cs, DIM, dc, RESET, text, zone))
    above_m = sum(1 for s, _, _, _ in results if s >= MATCH_THRESHOLD)
    above_w = sum(1 for s, _, _, _ in results if s >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {} | "
          "Hybrid {:.0f}% NLI + {:.0f}% KNN{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w,
        HYBRID_NLI_WEIGHT * 100, (1 - HYBRID_NLI_WEIGHT) * 100, RESET))
    print()


# --- LLM Judge display ---

def display_llm_result(llm_result, corrected, corrections, cosine_results=None, top_n=3):
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))

    verdict = llm_result.get("verdict", "ERROR")
    reasoning = llm_result.get("reasoning", "")

    if verdict == "MATCH":
        vcolor = RED
    elif verdict == "WARNING":
        vcolor = YELLOW
    elif verdict == "ERROR":
        vcolor = "\033[91m"
    else:
        vcolor = GREEN

    print("\n  {}{}\u25a0 {}{}".format(
        vcolor, BOLD, verdict, RESET))
    if reasoning:
        # Word-wrap reasoning at ~80 chars
        words = reasoning.split()
        lines = []
        line = "  "
        for w in words:
            if len(line) + len(w) + 1 > 82:
                lines.append(line)
                line = "  "
            line += w + " "
        if line.strip():
            lines.append(line)
        print("  {}Reasoning:{} {}".format(DIM, RESET, lines[0].strip()))
        for l in lines[1:]:
            print("  {}".format(l.rstrip()))

    if cosine_results:
        print("\n  {}Cosine context (for reference):{}".format(DIM, RESET))
        print("  {:<6} {:>8}  {:<30} {}".format("Rank", "Cosine", "Category", "Nearest Anchor"))
        print("  " + "-" * 90)
        for rank, (score, text, cat, *rest) in enumerate(cosine_results[:top_n], 1):
            cs = color_score(score)
            dt = text if len(text) <= 50 else text[:47] + "..."
            dc = cat if len(cat) <= 28 else cat[:25] + "..."
            print("  {:<6} {:>17}  {}{:<30}{} \"{}\"".format(
                rank, cs, DIM, dc, RESET, dt))
    print()


# ---------------------------------------------------------------------------
# Compare mode — run all scoring methods side-by-side
# ---------------------------------------------------------------------------

def _verdict_for_score(score):
    """Return verdict string for a numeric score (NLI/hybrid modes)."""
    if score >= MATCH_THRESHOLD:
        return "MATCH"
    elif score >= WARNING_THRESHOLD:
        return "WARNING"
    return "NO MATCH"


def _verdict_for_cosine_gap(knn_score):
    """Return verdict string for cosine KNN positive ratio."""
    if knn_score >= MATCH_THRESHOLD:
        return "MATCH"
    elif knn_score >= WARNING_THRESHOLD:
        return "WARNING"
    return "NO MATCH"


def _parse_labeled_file(content):
    """
    Parse input file with optional ground truth labels.

    Supports two formats:
      Format 1 (no labels):   message1 ### message2 ### message3
      Format 2 (with labels): message1 ### MATCH ### message2 ### NO MATCH ### ...

    Labels can appear on the same line as ### or on a new line after ###:
      message1
      ### MATCH
      message2
      ### NO MATCH

    Labels are case-insensitive: MATCH, match, NO MATCH, no match, CLEAN, clean.
    WARNING is treated as MATCH for ground truth purposes.

    Returns (messages, labels) where labels is a list of "MATCH"/"NO MATCH"/None.
    If no labels found at all, labels will be all None.
    """
    LABEL_WORDS = {"MATCH", "NO MATCH", "CLEAN", "WARNING"}

    raw_parts = [p.strip() for p in content.split("###")]
    raw_parts = [p for p in raw_parts if p]

    # Expand parts: if a part starts with a label on its first line,
    # split into [label, rest_of_part]
    parts = []
    for rp in raw_parts:
        lines = rp.split("\n", 1)
        first_line = lines[0].strip().upper()
        if first_line in LABEL_WORDS:
            parts.append(first_line)  # the label
            if len(lines) > 1 and lines[1].strip():
                parts.append(lines[1].strip())  # the next message
        else:
            parts.append(rp)

    messages = []
    label_dict = {}  # msg_index -> label

    for part in parts:
        part_upper = part.upper()
        if part_upper in LABEL_WORDS:
            # Label applies to the most recently added message
            if messages:
                msg_idx = len(messages) - 1
                if msg_idx not in label_dict:
                    if part_upper in ("MATCH", "WARNING"):
                        label_dict[msg_idx] = "MATCH"
                    else:
                        label_dict[msg_idx] = "NO MATCH"
        else:
            messages.append(part)

    labels = [label_dict.get(i) for i in range(len(messages))]

    # Check if ANY labels were provided
    has_labels = any(l is not None for l in labels)
    if not has_labels:
        labels = [None] * len(messages)

    return messages, labels


def _verdict_color(verdict):
    if verdict == "MATCH":
        return RED
    elif verdict == "WARNING":
        return YELLOW
    elif verdict == "ERROR":
        return "\033[91m"
    return GREEN


def score_all_modes(message, model, embs, texts, cats,
                    neg_embs=None, neg_texts=None, neutral_embs=None,
                    use_extraction=True, include_llm=False):
    """
    Run all scoring modes on a single message.

    Returns dict of mode_name -> {
        verdict: str, score: float, top3: list,
        reasoning: str (llm only), neg_cos: float
    }
    """
    results = {}

    # --- Cosine ---
    cos_results, cos_extraction, cos_knn = score_message(
        model, message, embs, texts, cats,
        neg_embeddings=neg_embs, neg_texts=neg_texts,
        neutral_embeddings=neutral_embs,
        use_extraction=use_extraction)
    knn_score = cos_knn["knn_score"]
    results["cosine"] = {
        "verdict": _verdict_for_cosine_gap(knn_score),
        "score": knn_score,
        "top3": cos_results[:3],
        "neg_cos": cos_knn["max_neg"],
        "knn": cos_knn,
    }

    # --- NLI ---
    nli_results, nli_corr, nli_corrections, nli_ext, nli_neg, nli_prop = score_message_nli(
        message, texts, cats, model=model, all_embeddings=embs,
        neg_texts=neg_texts, neg_embeddings=neg_embs,
        neutral_embeddings=neutral_embs,
        use_extraction=use_extraction)
    top_nli = nli_results[0][0]
    results["nli"] = {
        "verdict": _verdict_for_score(top_nli),
        "score": top_nli,
        "top3": nli_results[:3],
        "neg_cos": nli_neg,
        "corrections": nli_corrections,
        "prop_score": nli_prop,
    }

    # --- Hybrid ---
    hyb_results, hyb_corr, hyb_corrections, hyb_ext, hyb_neg, hyb_debug = score_message_hybrid(
        message, texts, cats, model=model, all_embeddings=embs,
        neg_texts=neg_texts, neg_embeddings=neg_embs,
        neutral_embeddings=neutral_embs,
        use_extraction=use_extraction)
    top_hyb = hyb_results[0][0]
    results["hybrid"] = {
        "verdict": _verdict_for_score(top_hyb),
        "score": top_hyb,
        "top3": hyb_results[:3],
        "neg_cos": hyb_neg,
        "debug": hyb_debug,
    }

    # --- LLM (optional) ---
    if include_llm:
        llm_result, llm_corr, llm_corrections, llm_cos = score_message_llm(
            message, PROPOSITION, model=model, all_embeddings=embs,
            all_texts=texts, all_categories=cats,
            neg_embeddings=neg_embs, neutral_embeddings=neutral_embs,
            use_extraction=use_extraction)
        results["llm"] = {
            "verdict": llm_result.get("verdict", "ERROR"),
            "score": 0.0,  # LLM returns decision, not score
            "top3": [],
            "reasoning": llm_result.get("reasoning", ""),
            "neg_cos": 0.0,
        }

    return results


def display_compare(message, mode_results, index=None, total=None):
    """Display side-by-side comparison table for one message."""
    # Header
    if index is not None and total is not None:
        disp = message if len(message) <= 80 else message[:77] + "..."
        print("\n  {}[{}/{}]{} \"{}\"".format(BOLD, index, total, RESET, disp))
    else:
        disp = message if len(message) <= 90 else message[:87] + "..."
        print("\n  \"{}\"".format(disp))

    # Spell corrections (from NLI — same for all)
    corrections = mode_results.get("nli", {}).get("corrections", [])
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))

    # === Verdict summary row ===
    print()
    mode_order = ["cosine", "nli", "hybrid", "llm"]
    active_modes = [m for m in mode_order if m in mode_results]

    # Build column widths
    col_w = 26
    header = "  {:<12}".format("")
    for m in active_modes:
        header += " {:^{}}".format(m.upper(), col_w)
    print(BOLD + header + RESET)
    print("  " + "\u2500" * (12 + (col_w + 1) * len(active_modes)))

    # Verdict row
    row_verdict = "  {:<12}".format("Verdict")
    for m in active_modes:
        v = mode_results[m]["verdict"]
        vc = _verdict_color(v)
        cell = "{}{}\u25a0 {}{}".format(vc, BOLD, v, RESET)
        # Pad for alignment (color codes don't take visual space)
        pad = col_w - len("\u25a0 " + v)
        row_verdict += " " + cell + " " * max(0, pad)
    print(row_verdict)

    # Score row
    row_score = "  {:<12}".format("Score")
    for m in active_modes:
        if m == "llm":
            cell = "--"
        elif m == "cosine":
            knn = mode_results[m].get("knn", {})
            pos_k = knn.get("pos_in_k", 0)
            k = knn.get("k", 0)
            ks = knn.get("knn_score", 0)
            cell = "{:.0f}/{} ({:.0%})".format(pos_k, k, ks)
        else:
            s = mode_results[m]["score"]
            cell = "{:.4f}".format(s)
        row_score += " {:^{}}".format(cell, col_w)
    print(row_score)

    # Max Pos / Neg row
    row_neg = "  {:<12}".format("Pos / Neg")
    for m in active_modes:
        if m == "llm":
            cell = "--"
        elif m == "cosine":
            knn = mode_results[m].get("knn", {})
            mp = knn.get("max_pos", 0.0)
            mn = knn.get("max_neg", 0.0)
            cell = "{:.3f} / {:.3f}".format(mp, mn)
        else:
            nc = mode_results[m].get("neg_cos", 0.0)
            cell = "-- / {:.3f}".format(nc) if nc > 0.01 else "--"
        row_neg += " {:^{}}".format(cell, col_w)
    print(row_neg)

    print("  " + "\u2500" * (12 + (col_w + 1) * len(active_modes)))

    # === Top 3 anchors per mode ===
    print("  {}Top 3 Anchors:{}".format(BOLD, RESET))

    for rank in range(3):
        row = "  {:<12}".format("  #{}".format(rank + 1))
        for m in active_modes:
            mr = mode_results[m]
            is_llm = (m == "llm")
            if is_llm:
                if rank == 0:
                    reasoning = mr.get("reasoning", "")
                    cell = reasoning if len(reasoning) <= (col_w - 2) else reasoning[:col_w - 5] + "..."
                else:
                    cell = ""
            else:
                top3 = mr.get("top3", [])
                if rank < len(top3):
                    s, t, c, *rest = top3[rank]
                    max_t = col_w - 9  # space for score "[0.XXX] "
                    dt = t if len(t) <= max_t else t[:max_t - 3] + "..."
                    cell = "{:.3f} \"{}\"".format(s, dt)
                    if len(cell) > col_w:
                        cell = cell[:col_w - 3] + "..."
                else:
                    cell = ""
            row += " {:<{}}".format(cell, col_w)
        print(row)

    # Category for top anchor per mode
    row_cat = "  {:<12}".format("  Category")
    for m in active_modes:
        if m == "llm":
            row_cat += " {:<{}}".format("", col_w)
        else:
            top3 = mode_results[m].get("top3", [])
            if top3:
                cat = top3[0][2]
                dc = cat if len(cat) <= (col_w - 2) else cat[:col_w - 5] + "..."
                row_cat += " {}{:<{}}{}".format(DIM, dc, col_w, RESET)
            else:
                row_cat += " {:<{}}".format("", col_w)
    print(row_cat)

    # Hybrid debug line
    hyb = mode_results.get("hybrid", {})
    debug = hyb.get("debug")
    if debug:
        nli_sc = debug.get("nli_score", 0)
        knn_sc = debug.get("knn_score", 0)
        nli_w = debug.get("nli_weight", 0.75)
        cos_w = debug.get("cos_weight", 0.25)
        combined = debug.get("combined", 0)
        cos_pos = debug.get("cos_pos_in_k", 0)
        cos_k = debug.get("cos_k", 0)
        nli_pos = debug.get("nli_pos_in_k", 0)
        nli_neg = debug.get("nli_neg_in_k", 0)
        nli_action = debug.get("nli_action", "")
        print("  {}Hybrid: nli={:.3f} ({}+/{}−) x {:.0f}% + cos_knn={:.3f} ({}/{}) x {:.0f}% = {:.3f} [{}]{}".format(
            DIM, nli_sc, nli_pos, nli_neg, nli_w * 100,
            knn_sc, cos_pos, cos_k, cos_w * 100, combined,
            nli_action, RESET))

    print()


def display_compare_summary(all_mode_results, messages, labels=None):
    """Display final summary grid across all messages and modes."""
    modes = ["cosine", "nli", "hybrid"]
    if any("llm" in mr for mr in all_mode_results):
        modes.append("llm")

    has_labels = labels is not None and any(l is not None for l in labels)

    # Count per mode
    counts = {}
    correct = {}
    for m in modes:
        counts[m] = {"MATCH": 0, "WARNING": 0, "NO MATCH": 0, "ERROR": 0}
        correct[m] = 0

    for idx, mr in enumerate(all_mode_results):
        for m in modes:
            if m in mr:
                v = mr[m]["verdict"]
                counts[m][v] = counts[m].get(v, 0) + 1
                # Compute accuracy against ground truth
                if has_labels and labels[idx] is not None:
                    predicted_match = v in ("MATCH", "WARNING")
                    expected_match = labels[idx] == "MATCH"
                    if predicted_match == expected_match:
                        correct[m] += 1

    total = len(messages)
    labeled_total = sum(1 for l in (labels or []) if l is not None) if has_labels else 0

    print("\n  {}Comparison Summary ({} messages{}){}".format(
        BOLD, total,
        ", {} labeled".format(labeled_total) if has_labels else "",
        RESET))
    print("  " + "=" * 75)

    if has_labels:
        print("  {}{:<10} {:>10} {:>10} {:>10} {:>10} {:>12}{}".format(
            BOLD, "Mode", "Matches", "Warnings", "No Match", "Errors", "Accuracy", RESET))
    else:
        print("  {}{:<10} {:>10} {:>10} {:>10} {:>10}{}".format(
            BOLD, "Mode", "Matches", "Warnings", "No Match", "Errors", RESET))
    print("  " + "-" * 75)

    for m in modes:
        c = counts[m]
        matches = c["MATCH"]
        warns = c["WARNING"]
        clean = c["NO MATCH"]
        errs = c["ERROR"]

        if has_labels and labeled_total > 0:
            acc_pct = correct[m] / labeled_total * 100
            print("  {:<10} {}{:>10}{} {}{:>10}{} {}{:>10}{} {:>10} {:>11.1f}%".format(
                m.upper(),
                RED, matches, RESET,
                YELLOW, warns, RESET,
                GREEN, clean, RESET,
                errs,
                acc_pct))
        else:
            print("  {:<10} {}{:>10}{} {}{:>10}{} {}{:>10}{} {:>10}".format(
                m.upper(),
                RED, matches, RESET,
                YELLOW, warns, RESET,
                GREEN, clean, RESET,
                errs))

    if has_labels:
        print()
        print("  {}Accuracy = correct predictions / labeled messages (WARNING counts as MATCH){}".format(
            DIM, RESET))

    # Per-message verdict grid
    print("\n  {}Per-Message Verdict Grid{}".format(BOLD, RESET))
    print("  " + "-" * (105 if has_labels else 95))

    # Header row
    hdr = "  {:<4} {:<50}".format("#", "Message")
    if has_labels:
        hdr += " {:>8}".format("EXPECTED")
    for m in modes:
        hdr += " {:>10}".format(m.upper())
    print(hdr)
    print("  " + "-" * (105 if has_labels else 95))

    for i, (msg, mr) in enumerate(zip(messages, all_mode_results), 1):
        disp = msg if len(msg) <= 48 else msg[:45] + "..."
        row = "  {:<4} {:<50}".format(i, disp)

        # Expected column
        if has_labels:
            lbl = labels[i - 1] if labels[i - 1] is not None else "--"
            if lbl == "MATCH":
                row += " {}  {:>5}  {}".format(RED, "MATCH", RESET)
            elif lbl == "NO MATCH":
                row += " {}  {:>5}  {}".format(GREEN, "CLEAN", RESET)
            else:
                row += " {:>8}".format("--")

        for m in modes:
            if m in mr:
                v = mr[m]["verdict"]
                vc = _verdict_color(v)
                short = v[:5] if v != "NO MATCH" else "CLEAN"
                # Mark wrong predictions
                if has_labels and labels[i - 1] is not None:
                    predicted_match = v in ("MATCH", "WARNING")
                    expected_match = labels[i - 1] == "MATCH"
                    if predicted_match != expected_match:
                        short += " \u2717"  # ✗ mark
                row += " {}  {:>7}  {}".format(vc, short, RESET)
            else:
                row += " {:>10}".format("--")
        print(row)

    # Agreement stats
    if len(modes) >= 2:
        agree = 0
        for mr in all_mode_results:
            verdicts = set()
            for m in modes:
                if m in mr:
                    verdicts.add(mr[m]["verdict"])
            if len(verdicts) <= 1:
                agree += 1
        print("\n  {}All-mode agreement: {}/{} ({:.1f}%){}".format(
            DIM, agree, total, agree / total * 100 if total > 0 else 0, RESET))

    print()


def run_compare(filepath, model, embs, texts, cats,
                use_extraction=True,
                neg_embs=None, neg_texts=None, neutral_embs=None,
                include_llm=False):
    """Run all modes on a file and display comparison."""
    if not os.path.exists(filepath):
        print("  ERROR: File not found: {}".format(filepath)); sys.exit(1)
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    sentences, labels = _parse_labeled_file(content)

    has_labels = any(l is not None for l in labels)
    modes_str = "cosine, nli, hybrid"
    if include_llm:
        modes_str += ", llm"
    label_str = " (with ground truth)" if has_labels else ""
    print("  {}Compare mode:{} {} on {} messages{} from: {}".format(
        BOLD, RESET, modes_str, len(sentences), label_str, filepath))
    print("  " + "=" * 100)

    all_mode_results = []
    for idx, sent in enumerate(sentences, 1):
        mr = score_all_modes(
            sent, model, embs, texts, cats,
            neg_embs=neg_embs, neg_texts=neg_texts, neutral_embs=neutral_embs,
            use_extraction=use_extraction, include_llm=include_llm)
        all_mode_results.append(mr)
        display_compare(sent, mr, index=idx, total=len(sentences))

    display_compare_summary(all_mode_results, sentences, labels=labels)


def run_compare_interactive(model, embs, texts, cats,
                            use_extraction=True,
                            neg_embs=None, neg_texts=None, neutral_embs=None,
                            include_llm=False):
    """Interactive compare mode — type messages, see all modes side-by-side."""
    global SPELLCHECK_ENABLED
    modes_str = "cosine, nli, hybrid"
    if include_llm:
        modes_str += ", llm"
    print("{}Compare Mode: {}{}".format(BOLD, modes_str, RESET))
    print("  Type a message to compare all scoring modes. Commands: /quit  /llm\n")
    print("  Thresholds: match={}{}{}  warning={}{}{}".format(
        RED, MATCH_THRESHOLD, RESET, YELLOW, WARNING_THRESHOLD, RESET))
    print("  " + "-" * 70)

    all_mode_results = []
    all_messages = []

    while True:
        try:
            msg = _read_message("\n  {}Message>{} ".format(BOLD, RESET))
        except (EOFError, KeyboardInterrupt):
            print("\n\n  Goodbye!")
            break
        if not msg:
            continue

        if msg.startswith("/"):
            c = msg.lower().split()
            if c[0] in ("/quit", "/exit", "/q"):
                break
            elif c[0] == "/llm":
                include_llm = not include_llm
                if include_llm:
                    try:
                        _load_llm_config()
                        print("  LLM: ON")
                    except SystemExit:
                        include_llm = False
                        print("  LLM: OFF (config not found)")
                else:
                    print("  LLM: OFF")
                continue
            elif c[0] == "/spellcheck":
                SPELLCHECK_ENABLED = not SPELLCHECK_ENABLED
                if SPELLCHECK_ENABLED:
                    _load_spellchecker()
                print("  Spell correction: {}".format(
                    "ON" if SPELLCHECK_ENABLED else "OFF"))
                continue
            elif c[0] == "/summary":
                if all_mode_results:
                    display_compare_summary(all_mode_results, all_messages)
                else:
                    print("  No messages scored yet.")
                continue
            elif c[0] == "/help":
                print("  /llm        \u2014 toggle LLM judge in comparison")
                print("  /spellcheck \u2014 toggle autocorrect spell checking")
                print("  /summary    \u2014 show summary of all messages so far")
                print("  /quit       \u2014 exit (shows summary)")
                continue
            else:
                print("  Unknown command. /help for options.")
                continue

        mr = score_all_modes(
            msg, model, embs, texts, cats,
            neg_embs=neg_embs, neg_texts=neg_texts, neutral_embs=neutral_embs,
            use_extraction=use_extraction, include_llm=include_llm)
        all_mode_results.append(mr)
        all_messages.append(msg)
        display_compare(msg, mr)

    if all_mode_results:
        display_compare_summary(all_mode_results, all_messages)


# ---------------------------------------------------------------------------
# 3D Visualization
# ---------------------------------------------------------------------------

def show_graph(model):
    """3D interactive scatter plot of anchor embeddings using PCA."""
    try:
        import matplotlib
        from sklearn.decomposition import PCA
        import numpy as np
    except ImportError as e:
        print("\n  ERROR: Missing dependency for --graph: {}".format(e))
        print("  Run: pip install matplotlib scikit-learn")
        sys.exit(1)

    backend_set = False
    for backend in ["macosx", "Qt5Agg", "TkAgg"]:
        try:
            matplotlib.use(backend)
            backend_set = True
            break
        except Exception:
            continue
    if not backend_set:
        matplotlib.use("Agg")

    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401

    using_agg = matplotlib.get_backend().lower() == "agg"

    # Collect positive anchors
    pos_texts = []
    for cat, examples in ANCHORS.items():
        for ex in examples:
            pos_texts.append(ex)

    # Collect negative anchors
    neg_texts = []
    for cat, examples in NEGATIVE_ANCHORS.items():
        for ex in examples:
            neg_texts.append(ex)

    n_pos = len(pos_texts)
    n_neg = len(neg_texts)
    all_texts = pos_texts + neg_texts

    print("  Embedding {} positive + {} negative anchors...".format(n_pos, n_neg))
    all_embs = model.encode(all_texts, show_progress_bar=False)
    prop_emb = model.encode([PROPOSITION])

    combined = np.vstack([all_embs, prop_emb])

    pca = PCA(n_components=3)
    coords_3d = pca.fit_transform(combined)
    pos_coords = coords_3d[:n_pos]
    neg_coords = coords_3d[n_pos:n_pos + n_neg]
    prop_coord = coords_3d[-1]

    explained = pca.explained_variance_ratio_
    print("  PCA variance explained: {:.1f}% + {:.1f}% + {:.1f}% = {:.1f}%".format(
        explained[0]*100, explained[1]*100, explained[2]*100, sum(explained)*100))

    fig = plt.figure(figsize=(12, 9))
    ax = fig.add_subplot(111, projection="3d")

    # Positive anchors (colored)
    ax.scatter(pos_coords[:, 0], pos_coords[:, 1], pos_coords[:, 2],
               c="#2196F3", s=50, alpha=0.7,
               edgecolors="white", linewidths=0.5)

    # Negative anchors (black)
    ax.scatter(neg_coords[:, 0], neg_coords[:, 1], neg_coords[:, 2],
               c="black", s=35, alpha=0.5,
               edgecolors="white", linewidths=0.5, marker="x")

    # Proposition (red star)
    ax.scatter([prop_coord[0]], [prop_coord[1]], [prop_coord[2]],
               c="red", s=300, marker="*", edgecolors="black",
               linewidths=1, zorder=10)

    # Light lines from proposition to positive anchors
    for i in range(n_pos):
        ax.plot([prop_coord[0], pos_coords[i, 0]],
                [prop_coord[1], pos_coords[i, 1]],
                [prop_coord[2], pos_coords[i, 2]],
                color="red", alpha=0.08, linewidth=0.5)

    ax.set_xlabel("PC1 ({:.1f}%)".format(explained[0]*100))
    ax.set_ylabel("PC2 ({:.1f}%)".format(explained[1]*100))
    ax.set_zlabel("PC3 ({:.1f}%)".format(explained[2]*100))
    ax.set_title("Semantic Anchors: {} \u2014 {} positive (blue) + {} negative (black)".format(
        SCRIPT_NAME, n_pos, n_neg), fontsize=13, fontweight="bold")

    norms = np.linalg.norm(all_embs[:n_pos], axis=1, keepdims=True)
    normed = all_embs[:n_pos] / (norms + 1e-10)
    sim_m = normed @ normed.T
    np.fill_diagonal(sim_m, 0)
    stats = "Positive avg sim: {:.4f} | Min: {:.4f} | Max: {:.4f}".format(
        sim_m.mean(), sim_m[sim_m > 0].min(), sim_m.max())
    ax.text2D(0.02, 0.96, stats, transform=ax.transAxes, fontsize=8,
              bbox=dict(boxstyle="round", facecolor="white", alpha=0.8))

    plt.tight_layout()

    if using_agg:
        out_file = "semantic_anchor_{}_3d.png".format(SCRIPT_NAME)
        plt.savefig(out_file, dpi=150)
        print("  Backend is non-interactive. Saved plot to: {}".format(out_file))
        import subprocess, platform
        try:
            if platform.system() == "Darwin":
                subprocess.Popen(["open", out_file])
            elif platform.system() == "Linux":
                subprocess.Popen(["xdg-open", out_file])
            elif platform.system() == "Windows":
                os.startfile(out_file)
        except Exception:
            pass
    else:
        print("  Showing 3D plot (drag to rotate)...")
        plt.show()


# ---------------------------------------------------------------------------
# Multi-line input handling
# ---------------------------------------------------------------------------

def _read_message(prompt):
    """
    Read a complete message from the user. Handles both single-line and
    multi-line input reliably without timing-based paste detection.

    Rules:
      - If the line ends with sentence-ending punctuation (. ? ! " ' ) ]),
        it's treated as complete and evaluated immediately.
      - If it doesn't (indicating a line break mid-sentence from a paste),
        a continuation prompt is shown. The user keeps pasting/typing until
        they enter a blank line.
      - Commands (starting with /) are always single-line.
    """
    first_line = input(prompt).strip()
    if not first_line:
        return first_line

    # Commands are always single-line
    if first_line.startswith("/"):
        return first_line

    # Check if the line looks complete (ends with sentence punctuation)
    if first_line and first_line[-1] in '.?!"\')]':
        # Try to drain any paste buffer quickly (best-effort)
        try:
            import select, sys
            lines = [first_line]
            while True:
                ready, _, _ = select.select([sys.stdin], [], [], 0.08)
                if ready:
                    line = sys.stdin.readline()
                    if not line:
                        break
                    line = line.strip()
                    if line:
                        lines.append(line)
                else:
                    break
            return " ".join(lines)
        except (ImportError, OSError):
            return first_line

    # Line doesn't end with punctuation — it's a multi-line paste
    # Show continuation prompt and read until blank line
    lines = [first_line]
    while True:
        try:
            line = input("  {}...>{} ".format(BOLD, RESET))
        except (EOFError, KeyboardInterrupt):
            break
        if not line.strip():
            break
        lines.append(line.strip())

    return " ".join(lines)


# ---------------------------------------------------------------------------
# Interactive mode
# ---------------------------------------------------------------------------

def run_interactive(model, embs, texts, cats, verbose=False, mode="cosine",
                    use_extraction=True,
                    neg_embs=None, neg_texts=None, neutral_embs=None):
    global SPELLCHECK_ENABLED
    mode_labels = {
        "cosine": "Cosine similarity",
        "nli": "NLI entailment (all anchors)",
        "hybrid": "Hybrid ({:.0f}% NLI + {:.0f}% KNN)".format(
            HYBRID_NLI_WEIGHT * 100, (1 - HYBRID_NLI_WEIGHT) * 100),
        "llm": "LLM Judge",
    }
    mode_label = mode_labels.get(mode, mode)
    extract_label = " + extraction" if use_extraction else ""
    neg_label = " + contrastive" if neg_texts else ""
    print("{}Interactive Mode: {}{}{}{}".format(BOLD, mode_label, extract_label, neg_label, RESET))
    print("  Type a message to evaluate. Commands: /verbose  /top N  /mode MODE  /extract  /quit\n")
    print("  Thresholds: match={}{}{}  warning={}{}{}".format(
        RED, MATCH_THRESHOLD, RESET, YELLOW, WARNING_THRESHOLD, RESET))
    print("  Mode: {}{}{}".format(GREEN, mode_label, RESET))
    print("  Extraction: {}{}{}  (long-message defense, tau={})".format(
        GREEN if use_extraction else YELLOW,
        "ON" if use_extraction else "OFF",
        RESET, EXTRACTION_RELEVANCE_TAU))
    if neg_texts:
        neutral_count = len(NEUTRAL_ANCHORS) if NEUTRAL_ANCHORS else 0
        print("  Contrastive: {}ON{} ({} negative, {} neutral anchors)".format(
            GREEN, RESET, len(neg_texts), neutral_count))
    else:
        print("  Contrastive: {}OFF{} (no negative anchors)".format(YELLOW, RESET))
    print("  " + "-" * 70)
    top_n = 3
    verbose_extract = False

    while True:
        try:
            msg = _read_message("\n  {}Message>{} ".format(BOLD, RESET))
        except (EOFError, KeyboardInterrupt):
            print("\n\n  Goodbye!")
            break
        if not msg:
            continue

        if msg.startswith("/"):
            c = msg.lower().split()
            if c[0] in ("/quit", "/exit", "/q"):
                print("  Goodbye!"); break
            elif c[0] == "/verbose":
                verbose = not verbose
                print("  Verbose: {}".format("ON" if verbose else "OFF")); continue
            elif c[0] == "/top" and len(c) > 1:
                try: top_n = int(c[1]); print("  Showing top {}".format(top_n))
                except ValueError: print("  Usage: /top N")
                continue
            elif c[0] == "/mode" and len(c) > 1:
                new_mode = c[1]
                if new_mode in ("cosine", "nli", "hybrid", "llm"):
                    mode = new_mode
                    if mode in ("nli", "hybrid"):
                        _load_cross_encoder()
                    if mode == "llm":
                        _load_llm_config()
                    print("  Mode: {}".format(mode_labels.get(mode, mode)))
                else:
                    print("  Available modes: cosine, nli, hybrid, llm")
                continue
            elif c[0] == "/extract":
                use_extraction = not use_extraction
                print("  Extraction: {} (tau={})".format(
                    "ON" if use_extraction else "OFF", EXTRACTION_RELEVANCE_TAU)); continue
            elif c[0] == "/extractv":
                verbose_extract = not verbose_extract
                print("  Verbose extraction: {}".format(
                    "ON" if verbose_extract else "OFF")); continue
            elif c[0] == "/spellcheck":
                SPELLCHECK_ENABLED = not SPELLCHECK_ENABLED
                if SPELLCHECK_ENABLED:
                    _load_spellchecker()
                print("  Spell correction: {}".format(
                    "ON" if SPELLCHECK_ENABLED else "OFF")); continue
            elif c[0] == "/help":
                print("  /verbose   \u2014 toggle full table")
                print("  /top N     \u2014 show top N results")
                print("  /mode MODE \u2014 switch scoring: cosine, nli, hybrid, llm")
                print("  /extract   \u2014 toggle proposition-guided extraction (long msgs)")
                print("  /extractv  \u2014 toggle verbose extraction diagnostics")
                print("  /spellcheck\u2014 toggle autocorrect spell checking")
                print("  /quit      \u2014 exit"); continue
            else:
                print("  Unknown command. /help for options."); continue

        # --- Dispatch by mode ---
        if mode == "llm":
            llm_result, corrected, corrections, cosine_results = score_message_llm(
                msg, PROPOSITION, model=model, all_embeddings=embs,
                all_texts=texts, all_categories=cats,
                neg_embeddings=neg_embs, neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            display_llm_result(llm_result, corrected, corrections, cosine_results, top_n)

        elif mode == "hybrid":
            results, corrected, corrections, extraction, neg_score, debug = score_message_hybrid(
                msg, texts, cats, model=model, all_embeddings=embs,
                neg_texts=neg_texts, neg_embeddings=neg_embs,
                neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            if verbose:
                display_verbose_hybrid(results, corrected, corrections, extraction,
                                       neg_score=neg_score, debug=debug)
            else:
                display_default_hybrid(results, corrected, corrections, extraction,
                                       top_n, neg_score=neg_score, debug=debug)

        elif mode == "nli":
            results, corrected, corrections, extraction, neg_score, prop_score = score_message_nli(
                msg, texts, cats, model=model, all_embeddings=embs,
                neg_texts=neg_texts, neg_embeddings=neg_embs,
                neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            if verbose:
                display_verbose_nli(results, corrected, corrections, extraction, neg_score=neg_score)
            else:
                display_default_nli(results, corrected, corrections, extraction, top_n, neg_score=neg_score)

        else:  # cosine
            results, extraction, knn_info = score_message(model, msg, embs, texts, cats,
                                               neg_embeddings=neg_embs, neg_texts=neg_texts,
                                               neutral_embeddings=neutral_embs,
                                               use_extraction=use_extraction)
            if verbose:
                display_verbose(results, extraction, knn_info=knn_info)
            else:
                display_default(results, extraction, top_n, verbose_extract, knn_info=knn_info)


# ---------------------------------------------------------------------------
# File mode
# ---------------------------------------------------------------------------

def run_file(filepath, model, embs, texts, cats, verbose=False, mode="cosine",
             use_extraction=True,
             neg_embs=None, neg_texts=None, neutral_embs=None):
    if not os.path.exists(filepath):
        print("  ERROR: File not found: {}".format(filepath)); sys.exit(1)
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    sentences, labels = _parse_labeled_file(content)
    mode_labels = {
        "cosine": "cosine", "nli": "NLI-only",
        "hybrid": "hybrid", "llm": "LLM judge",
    }
    mode_label = " ({})".format(mode_labels.get(mode, mode))
    extract_label = " + extraction" if use_extraction else ""
    neg_label = " + contrastive" if neg_texts else ""
    print("  Processing {} sentences from: {}{}{}{}\n".format(
        len(sentences), filepath, mode_label, extract_label, neg_label))
    print("  " + "=" * 100)
    all_top_scores = []
    all_verdicts = []

    for idx, sent in enumerate(sentences, 1):
        disp = sent[:80] + ("..." if len(sent) > 80 else "")
        print("\n  {}[{}/{}] Message:{} \"{}\"".format(BOLD, idx, len(sentences), RESET, disp))

        if mode == "llm":
            llm_result, corrected, corrections, cosine_results = score_message_llm(
                sent, PROPOSITION, model=model, all_embeddings=embs,
                all_texts=texts, all_categories=cats,
                neg_embeddings=neg_embs, neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            display_llm_result(llm_result, corrected, corrections, cosine_results)
            verdict = llm_result.get("verdict", "ERROR")
            all_top_scores.append(1.0 if llm_result.get("match") else 0.0)
            all_verdicts.append(verdict)

        elif mode == "hybrid":
            results, corrected, corrections, extraction, neg_score, debug = score_message_hybrid(
                sent, texts, cats, model=model, all_embeddings=embs,
                neg_texts=neg_texts, neg_embeddings=neg_embs,
                neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose_hybrid(results, corrected, corrections, extraction,
                                       neg_score=neg_score, debug=debug)
            else:
                display_default_hybrid(results, corrected, corrections, extraction,
                                       neg_score=neg_score, debug=debug)
            all_top_scores.append(top_score)

        elif mode == "nli":
            results, corrected, corrections, extraction, neg_score, prop_score = score_message_nli(
                sent, texts, cats, model=model, all_embeddings=embs,
                neg_texts=neg_texts, neg_embeddings=neg_embs,
                neutral_embeddings=neutral_embs,
                use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose_nli(results, corrected, corrections, extraction, neg_score=neg_score)
            else:
                display_default_nli(results, corrected, corrections, extraction, neg_score=neg_score)
            all_top_scores.append(top_score)

        else:  # cosine
            results, extraction, knn_info = score_message(model, sent, embs, texts, cats,
                                               neg_embeddings=neg_embs, neg_texts=neg_texts,
                                               neutral_embeddings=neutral_embs,
                                               use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose(results, extraction, knn_info=knn_info)
            else:
                display_default(results, extraction, knn_info=knn_info)
            all_top_scores.append(knn_info["knn_score"])

    # Summary
    has_labels = any(l is not None for l in labels)

    if mode == "llm":
        matches = sum(1 for v in all_verdicts if v == "MATCH")
        warns = sum(1 for v in all_verdicts if v == "WARNING")
        errors = sum(1 for v in all_verdicts if v == "ERROR")
        clean = sum(1 for v in all_verdicts if v == "NO MATCH")
    elif mode == "cosine":
        matches = sum(1 for s in all_top_scores if s >= MATCH_THRESHOLD)
        warns = sum(1 for s in all_top_scores if WARNING_THRESHOLD <= s < MATCH_THRESHOLD)
        clean = sum(1 for s in all_top_scores if s < WARNING_THRESHOLD)
        errors = 0
    else:
        matches = sum(1 for s in all_top_scores if s >= MATCH_THRESHOLD)
        warns = sum(1 for s in all_top_scores if WARNING_THRESHOLD <= s < MATCH_THRESHOLD)
        clean = sum(1 for s in all_top_scores if s < WARNING_THRESHOLD)
        errors = 0

    print("\n  {}Summary{}".format(BOLD, RESET))
    print("  " + "=" * 60)
    print("  Total sentences:  {}".format(len(sentences)))
    print("  {}\u25a0 Matches:         {}{}".format(RED, matches, RESET))
    print("  {}\u25a0 Warnings:        {}{}".format(YELLOW, warns, RESET))
    print("  {}\u25a0 No match:        {}{}".format(GREEN, clean, RESET))
    if errors:
        print("  \u25a0 Errors:          {}".format(errors))

    # Ground truth accuracy
    if has_labels:
        correct_count = 0
        labeled_count = 0
        for i, lbl in enumerate(labels):
            if lbl is None:
                continue
            labeled_count += 1
            if mode == "llm":
                predicted_match = all_verdicts[i] in ("MATCH", "WARNING")
            elif mode == "cosine":
                predicted_match = all_top_scores[i] >= WARNING_THRESHOLD  # KNN ratio above warning
            else:
                predicted_match = all_top_scores[i] >= WARNING_THRESHOLD
            expected_match = lbl == "MATCH"
            if predicted_match == expected_match:
                correct_count += 1
        if labeled_count > 0:
            print("  {}Accuracy:         {}/{} ({:.1f}%){}".format(
                BOLD, correct_count, labeled_count,
                correct_count / labeled_count * 100, RESET))
    print()


# ---------------------------------------------------------------------------
# Logging — tee stdout to log file
# ---------------------------------------------------------------------------

class TeeLogger:
    """Duplicate stdout to both console and a log file."""
    def __init__(self, log_path):
        self.terminal = sys.stdout
        self.log_file = open(log_path, "w", encoding="utf-8")
        self.log_path = log_path

    def write(self, message):
        self.terminal.write(message)
        # Strip ANSI escape codes for clean log file
        import re
        clean = re.sub(r'\033\[[0-9;]*m', '', message)
        self.log_file.write(clean)
        self.log_file.flush()

    def flush(self):
        self.terminal.flush()
        self.log_file.flush()

    def isatty(self):
        return self.terminal.isatty()

    def fileno(self):
        return self.terminal.fileno()

    @property
    def encoding(self):
        return self.terminal.encoding

    def close(self):
        self.log_file.close()
        sys.stdout = self.terminal


def _start_logging():
    """Start logging to a timestamped file. Returns the TeeLogger or None."""
    try:
        from datetime import datetime
        ts = datetime.now().strftime("%Y-%m-%d_%H-%M")
        script_dir = os.path.dirname(os.path.abspath(__file__))
        log_path = os.path.join(script_dir, "log_{}_{}.txt".format(SCRIPT_NAME, ts))
        tee = TeeLogger(log_path)
        sys.stdout = tee
        return tee
    except Exception:
        return None


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    # Start logging to file
    _tee = _start_logging()

    parser = argparse.ArgumentParser(
        description="Semantic Anchor Evaluator: " + SCRIPT_NAME)
    parser.add_argument("--file", "-f", type=str, default=None,
        help="Input file with sentences separated by ###")
    parser.add_argument("--verbose", "-v", action="store_true",
        help="Show full scored table (all anchors sorted)")
    parser.add_argument("--show-examples", action="store_true",
        help="Print all positive examples by category")
    parser.add_argument("--graph", "-g", action="store_true",
        help="Show 3D interactive visualization of anchor spread")
    parser.add_argument("--mode", "-m", type=str, default="cosine",
        choices=["cosine", "nli", "hybrid", "llm"],
        help="Scoring mode: cosine, nli, hybrid (NLI+KNN blend), llm (LLM judge)")
    # Backward-compatible aliases
    parser.add_argument("--nli", action="store_true",
        help="Shortcut for --mode nli")
    parser.add_argument("--hybrid", action="store_true",
        help="Shortcut for --mode hybrid")
    parser.add_argument("--llm", action="store_true",
        help="Shortcut for --mode llm")
    parser.add_argument("--no-extract", action="store_true",
        help="Disable proposition-guided extraction for long messages")
    parser.add_argument("--spellcheck", action="store_true",
        help="Enable autocorrect spell checking (disabled by default)")
    parser.add_argument("--compare", "-c", action="store_true",
        help="Compare all scoring modes side-by-side (cosine, nli, hybrid, +llm)")
    args = parser.parse_args()

    # Resolve mode from flags
    mode = args.mode
    if args.compare:
        mode = "compare"
    elif args.nli:
        mode = "nli"
    elif args.hybrid:
        mode = "hybrid"
    elif args.llm:
        mode = "llm"

    use_extraction = not args.no_extract

    # --- Spellcheck: CLI flag or config_<name>.ini ---
    global SPELLCHECK_ENABLED
    if args.spellcheck:
        SPELLCHECK_ENABLED = True
    else:
        # Check config_<name>.ini for spellcheck setting
        try:
            import configparser as _cp
            _cfg = _cp.ConfigParser()
            _cfg_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                                     "config_{}.ini".format(SCRIPT_NAME))
            if os.path.exists(_cfg_path):
                _cfg.read(_cfg_path)
                if _cfg.has_option("options", "spellcheck"):
                    SPELLCHECK_ENABLED = _cfg.get("options", "spellcheck").lower() in ("true", "1", "yes", "on")
                elif _cfg.has_option("llm_judge", "spellcheck"):
                    SPELLCHECK_ENABLED = _cfg.get("llm_judge", "spellcheck").lower() in ("true", "1", "yes", "on")
        except Exception:
            pass

    print_banner()

    if args.show_examples:
        print_examples()
        if not args.file and not args.graph:
            try:
                r = input("  Enter interactive mode? [Y/n] ").strip().lower()
                if r == "n":
                    if _tee: _tee.close(); print("  Log saved: {}".format(_tee.log_path))
                    return
            except (EOFError, KeyboardInterrupt):
                print()
                if _tee: _tee.close(); print("  Log saved: {}".format(_tee.log_path))
                return

    model = load_model()

    if args.graph:
        show_graph(model)
        if _tee: _tee.close(); print("  Log saved: {}".format(_tee.log_path))
        return

    (all_texts, all_cats, all_embs,
     neg_texts, neg_cats, neg_embs,
     neutral_embs) = prepare_anchors(model)

    if use_extraction:
        print("  Extraction: {}ON{} (tau={}, min_words={})".format(
            GREEN, RESET, EXTRACTION_RELEVANCE_TAU, EXTRACTION_MIN_WORDS))

    # Log model sources
    if _model_sources:
        emb_name, emb_src = _model_sources.get("embedding", (EMBEDDING_MODEL, "default"))
        nli_name, nli_src = _model_sources.get("nli", (NLI_MODEL, "default"))
        print("  Embedding:  {} {}(from {}){}".format(emb_name, DIM, emb_src, RESET))
        print("  NLI model:  {} {}(from {}){}".format(nli_name, DIM, nli_src, RESET))

    spell_label = "{}ON{}".format(GREEN, RESET) if SPELLCHECK_ENABLED else "{}OFF{} (enable with --spellcheck)".format(DIM, RESET)
    print("  Spell correction: {}\n".format(spell_label))

    # Load cross-encoder for modes that need it
    if mode in ("nli", "hybrid", "compare"):
        _load_cross_encoder()
        total_anchors = sum(len(v) for v in ANCHORS.values())
        if mode == "compare":
            print("  Compare mode: {}ON{} (all scoring methods side-by-side)".format(
                GREEN, RESET))
        elif mode == "hybrid":
            print("  Hybrid mode: {}ON{} ({:.0f}% NLI + {:.0f}% cosine KNN)".format(
                GREEN, RESET, HYBRID_NLI_WEIGHT * 100, (1 - HYBRID_NLI_WEIGHT) * 100))
        else:
            print("  NLI-only mode: {}ON{}".format(GREEN, RESET))
            print("  Scoring: NLI entailment on ALL {} anchors".format(total_anchors))
            print("  Long messages: proposition-guided extraction + multi-view NLI\n")

    elif mode == "llm":
        _load_llm_config()
        print("  LLM Judge: {}ON{} ({} / {})".format(
            GREEN, RESET, _llm_config["provider"], _llm_config["model"]))
        prov = _llm_config["provider"].lower()
        if prov in ("ollama", "local", "lmstudio", "vllm"):
            print("  Base URL:   {}".format(_llm_config.get("base_url", "")))
        print()

    # Check if LLM is available for compare mode
    include_llm = False
    if mode == "compare":
        try:
            _load_llm_config(silent=True)
            if _llm_config.get("api_key", "").startswith("YOUR_"):
                print("  LLM Judge: {}SKIPPED{} (edit config_{}.ini [llm_judge] api_key)".format(
                    YELLOW, RESET, SCRIPT_NAME))
            else:
                include_llm = True
                print("  LLM Judge: {}ON{} ({} / {})".format(
                    GREEN, RESET, _llm_config["provider"], _llm_config["model"]))
        except SystemExit:
            print("  LLM Judge: {}SKIPPED{} (no config_{}.ini found)".format(
                YELLOW, RESET, SCRIPT_NAME))
        print()

    # Dispatch
    if mode == "compare":
        if args.file:
            run_compare(args.file, model, all_embs, all_texts, all_cats,
                        use_extraction, neg_embs=neg_embs, neg_texts=neg_texts,
                        neutral_embs=neutral_embs, include_llm=include_llm)
        else:
            run_compare_interactive(model, all_embs, all_texts, all_cats,
                                    use_extraction, neg_embs=neg_embs,
                                    neg_texts=neg_texts, neutral_embs=neutral_embs,
                                    include_llm=include_llm)
    elif args.file:
        run_file(args.file, model, all_embs, all_texts, all_cats,
                 args.verbose, mode, use_extraction,
                 neg_embs=neg_embs, neg_texts=neg_texts, neutral_embs=neutral_embs)
    else:
        run_interactive(model, all_embs, all_texts, all_cats,
                        args.verbose, mode, use_extraction,
                        neg_embs=neg_embs, neg_texts=neg_texts, neutral_embs=neutral_embs)

    # Close logger and report log path
    if _tee is not None:
        log_path = _tee.log_path
        _tee.close()
        print("  Log saved: {}".format(log_path))


if __name__ == "__main__":
    main()
