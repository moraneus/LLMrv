# =============================================================================
# Semantic Anchor Generator — Configuration
# =============================================================================
#
# This file configures the LLM used to GENERATE anchors.
# (The evaluator uses a separate config_<name>.ini for --mode llm)
#
# Usage:
#   python semantic_anchor_generator.py -p "Your proposition" -name myproject
#


# =============================================================================
# [llm] — LLM Provider for Anchor Generation
# =============================================================================
#
# Supported providers:
#   anthropic   — Claude  (api.anthropic.com)
#   openai      — GPT     (api.openai.com)
#   gemini      — Gemini  (generativelanguage.googleapis.com)
#   grok        — Grok    (api.x.ai)
#   ollama      — Local   (localhost:11434, native API)
#   lmstudio    — Local   (localhost:1234, OpenAI-compatible)
#   vllm        — Local   (localhost:8000, OpenAI-compatible)
#
# Aliases: claude → anthropic, google → gemini, xai → grok, local → lmstudio

[llm]

# --- Uncomment ONE provider block below ---

# >> Anthropic Claude <<
provider = anthropic
model = claude-haiku-4-5-20251001
api_key = sk-ant-

# >> OpenAI GPT <<
# provider = openai
# model = gpt-4o
# api_key = sk-...

# >> Google Gemini <<
# provider = gemini
# model = gemini-2.0-flash
# api_key = AIza...

# >> xAI Grok <<
# provider = grok
# model = grok-3-mini-fast
# api_key = xai-...

# >> Local Ollama (free, no API key needed) <<
# provider = ollama
# model = llama3.1:8b
# api_key = not-needed

# >> Local LM Studio (free, OpenAI-compatible) <<
# provider = lmstudio
# model = loaded-model
# api_key = not-needed

# >> Local vLLM (free, OpenAI-compatible) <<
# provider = vllm
# model = meta-llama/Llama-3.1-8B-Instruct
# api_key = not-needed

# For local providers, set base URL via environment variable:
#   OLLAMA_BASE_URL=http://localhost:11434       (ollama)
#   LOCAL_LLM_BASE_URL=http://localhost:1234     (lmstudio, vllm)



# =============================================================================
# [anchors] — Anchor Generation Settings
# =============================================================================

[anchors]

# Number of anchors to generate per round (final count set by -n flag)
num_examples = 20

# Negative-to-positive anchor ratio (2.0 = generate 2x more negatives than positives)
negative_ratio = 2.0

# Hard positive ratio (0.3 = generate 30% of target count as hard positives)
# Hard positives sound benign/legitimate but genuinely match the proposition.
# They fill the boundary region between positive and negative anchor clouds,
# critical for NLI KNN accuracy on polite or professionally-phrased harmful messages.
# Set to 0 to disable hard positive generation.
hard_positive_ratio = 0.3

# Hard negative ratio (0.3 = generate 30% of target count as hard negatives)
# Hard negatives sound harmful/suspicious but are actually legitimate.
# They fill the boundary from the negative side — critical for reducing
# false positives on definitional, educational, and defensive queries
# that share vocabulary with positive anchors.
# Set to 0 to disable hard negative generation.
hard_negative_ratio = 0.3

# --- Adversarial Semantic Mapping ---

# Orthogonal axes: auto-generate domain-specific semantic axes that ensure
# the generation pipeline covers the FULL semantic space of the proposition.
# Instead of generic categories like "polite" or "direct", the LLM generates
# domain-specific axes (e.g., for "hacking": IoT, Web, Social Engineering,
# Cryptography). These are merged with static categories for generation.
# Set to false to skip and use only static categories.
orthogonal_axes = true

# Annealing MMR: dynamic lambda that starts high (relevance-focused) and
# decays to low (diversity-focused) as anchor slots fill up. This ensures
# the "core" of the proposition is well-defined while the boundary fringes
# get dense coverage — exactly where the classifier is most likely to fail.
# Set to false to use fixed lambda_param=0.5.
mmr_anneal = true

# Adversarial filtering: score hard negative candidates against the actual
# KNN voting mechanism (positive+negative anchor pool) and prioritize the
# benign messages that would produce the highest false-positive rates.
# These are the most valuable hard negatives for boundary definition.
# Set to false to skip adversarial scoring.
adversarial_filter = true

# Intra-category variance threshold: after MMR selection, compute pairwise
# cosine within each category. If variance drops below this threshold
# (meaning the LLM generated near-identical examples for that category),
# trigger a high-creativity regeneration round to force more "jitter".
# Range: 0.05–0.30 (lower = more tolerant of similar examples).
# Set to 0 to disable variance monitoring.
variance_threshold = 0.15

# Embedding model for similarity scoring and MMR diversity selection
# Used at both generation and evaluation time
#
# Options:
#   all-mpnet-base-v2          — Default, good general-purpose, fast
#   BAAI/bge-large-en-v1.5     — Instruction-tuned, better accuracy, larger
#   all-MiniLM-L6-v2           — Lightweight, fastest, lower accuracy
#
# Any model from https://huggingface.co/models?library=sentence-transformers
embedding_model = BAAI/bge-large-en-v1.5

# NLI cross-encoder model for --mode nli, --mode hybrid, and --compare
# Determines semantic entailment between messages and anchors
#
# Options:
#   cross-encoder/nli-deberta-v3-large  — Best accuracy (default)
#   cross-encoder/nli-deberta-v3-base   — Faster, slightly less accurate
#   cross-encoder/nli-deberta-v3-xsmall — Fastest, lowest accuracy
nli_model = cross-encoder/nli-deberta-v3-large

# Anchor categories — the LLM generates examples for each category
# to ensure diverse coverage. Comma-separated list.
categories_user = Direct explicit requests,
    Indirect or euphemistic requests,
    Implicit or inferred requests,
    Hidden intention within a long text (at least a paragraph),
    Wrapped in fictional context,
    Wrapped in academic or analytical context,
    Third-person framing,
    Hypothetical framing,
    Demanding or insistent phrasing,
    Polite or collaborative phrasing,
    Question format variations (open-ended, yes/no, leading, compound),
    Slang or coded language,
    Obfuscated or intentionally vague wording,
    Information-seeking requests,
    Procedural or instructional requests,
    Transformational requests (rewrite, summarize, translate),
    Creative generation requests,
    Analytical or evaluative requests,
    Decision-support requests,
    Emotional or social-support requests,
    Single clear task requests,
    Multi-step or compound requests,
    Iterative refinement requests,
    Constraint-based requests (format, tone, length, style),
    Clearly benign requests,
    Ambiguous-intent requests,
    Adversarial or evasive requests,
    High-risk or policy-sensitive requests

categories_assistant =
    Informational responses,
    Instructional or procedural responses,
    Analytical or reasoning responses,
    Creative or generative responses,
    Transformational responses,
    Decision support responses,
    Conversational or social responses,
    Emotional support responses,
    Meta level or self referential responses,
    Structured data outputs,
    Step by step formatted responses,
    Bullet point or list based responses,
    Long form explanatory responses,
    Concise or direct responses,
    Clarification or follow up questions,
    Assumption based completion,
    Iterative refinement responses,
    Constraint aware formatted responses,
    Safe completion with guidance,
    Refusal or boundary setting responses,
    Risk mitigation or harm reduction responses



# =============================================================================
# [thresholds] — Scoring Thresholds
# =============================================================================
#
# These are written into anchors_list_<name>.json and used by the evaluator.
#
#   score > match_threshold    → MATCH   (red)
#   score > warning_threshold → WARNING (yellow)
#   score <= warning_threshold  → CLEAN   (green)
#
# Tune these based on your domain. Use --compare mode to benchmark.

[thresholds]
match_threshold = 0.70
warning_threshold = 0.50
knn_size = 30
