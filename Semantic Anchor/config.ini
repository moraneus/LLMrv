# =============================================================================
# Semantic Anchor Generator — Configuration
# =============================================================================
#
# This file configures the LLM used to GENERATE anchors.
# (The evaluator uses a separate config_<name>.ini for --mode llm)
#
# Usage:
#   python semantic_anchor_generator.py -p "Your proposition" -name myproject
#


# =============================================================================
# [llm] — LLM Provider for Anchor Generation
# =============================================================================
#
# Supported providers:
#   anthropic   — Claude  (api.anthropic.com)
#   openai      — GPT     (api.openai.com)
#   gemini      — Gemini  (generativelanguage.googleapis.com)
#   grok        — Grok    (api.x.ai)
#   ollama      — Local   (localhost:11434, native API)
#   lmstudio    — Local   (localhost:1234, OpenAI-compatible)
#   vllm        — Local   (localhost:8000, OpenAI-compatible)
#
# Aliases: claude → anthropic, google → gemini, xai → grok, local → lmstudio

[llm]

# --- Uncomment ONE provider block below ---

# >> Anthropic Claude <<
provider = anthropic
model = claude-haiku-4-5-20251001
api_key = sk-ant

# >> OpenAI GPT <<
# provider = openai
# model = gpt-4o
# api_key = sk-...

# >> Google Gemini <<
# provider = gemini
# model = gemini-2.0-flash
# api_key = AIza...

# >> xAI Grok <<
# provider = grok
# model = grok-3-mini-fast
# api_key = xai-...

# >> Local Ollama (free, no API key needed) <<
# provider = ollama
# model = llama3.1:8b
# api_key = not-needed

# >> Local LM Studio (free, OpenAI-compatible) <<
# provider = lmstudio
# model = loaded-model
# api_key = not-needed

# >> Local vLLM (free, OpenAI-compatible) <<
# provider = vllm
# model = meta-llama/Llama-3.1-8B-Instruct
# api_key = not-needed

# For local providers, set base URL via environment variable:
#   OLLAMA_BASE_URL=http://localhost:11434       (ollama)
#   LOCAL_LLM_BASE_URL=http://localhost:1234     (lmstudio, vllm)



# =============================================================================
# [anchors] — Anchor Generation Settings
# =============================================================================

[anchors]

# Number of anchors to generate per round (final count set by -n flag)
num_examples = 20

# Negative-to-positive anchor ratio (2.0 = generate 2x more negatives than positives)
negative_ratio = 2.0

# Hard positive ratio (0.3 = generate 30% of target count as hard positives)
# Hard positives sound benign/legitimate but genuinely match the proposition.
# They fill the boundary region between positive and negative anchor clouds,
# critical for NLI KNN accuracy on polite or professionally-phrased harmful messages.
# Set to 0 to disable hard positive generation.
hard_positive_ratio = 0.3

# Hard negative ratio (0.3 = generate 30% of target count as hard negatives)
# Hard negatives sound harmful/suspicious but are actually legitimate.
# They fill the boundary from the negative side — critical for reducing
# false positives on definitional, educational, and defensive queries
# that share vocabulary with positive anchors.
# Set to 0 to disable hard negative generation.
hard_negative_ratio = 0.3

# --- Adversarial Semantic Mapping ---

# Orthogonal axes: auto-generate domain-specific semantic axes that ensure
# the generation pipeline covers the FULL semantic space of the proposition.
# Instead of generic categories like "polite" or "direct", the LLM generates
# domain-specific axes (e.g., for "hacking": IoT, Web, Social Engineering,
# Cryptography). These are merged with static categories for generation.
# Set to false to skip and use only static categories.
orthogonal_axes = true

# Annealing MMR: dynamic lambda that starts high (relevance-focused) and
# decays to low (diversity-focused) as anchor slots fill up. This ensures
# the "core" of the proposition is well-defined while the boundary fringes
# get dense coverage — exactly where the classifier is most likely to fail.
# Set to false to use fixed lambda_param=0.5.
mmr_anneal = true

# Adversarial filtering: score hard negative candidates against the actual
# KNN voting mechanism (positive+negative anchor pool) and prioritize the
# benign messages that would produce the highest false-positive rates.
# These are the most valuable hard negatives for boundary definition.
# Set to false to skip adversarial scoring.
adversarial_filter = true

# Intra-category variance threshold: after MMR selection, compute pairwise
# cosine within each category. If variance drops below this threshold
# (meaning the LLM generated near-identical examples for that category),
# trigger a high-creativity regeneration round to force more "jitter".
# Range: 0.05–0.30 (lower = more tolerant of similar examples).
# Set to 0 to disable variance monitoring.
variance_threshold = 0.15

# Embedding model for similarity scoring and MMR diversity selection
# Used at both generation and evaluation time
#
# Options:
#   all-mpnet-base-v2          — Default, good general-purpose, fast
#   BAAI/bge-large-en-v1.5     — Instruction-tuned, better accuracy, larger
#   all-MiniLM-L6-v2           — Lightweight, fastest, lower accuracy
#
# Any model from https://huggingface.co/models?library=sentence-transformers
embedding_model = BAAI/bge-large-en-v1.5

# NLI cross-encoder model for --mode nli, --mode hybrid, and --compare
# Determines semantic entailment between messages and anchors
#
# Options:
#   cross-encoder/nli-deberta-v3-large  — Best accuracy (default)
#   cross-encoder/nli-deberta-v3-base   — Faster, slightly less accurate
#   cross-encoder/nli-deberta-v3-xsmall — Fastest, lowest accuracy
nli_model = cross-encoder/nli-deberta-v3-large

# ---------------------------------------------------------------------------
# Categories — controls the diversity dimensions for anchor generation.
#
# Priority: categories_{role} > categories (generic fallback).
# When --role user is set, uses categories_user.
# When --role assistant is set, uses categories_assistant.
#
# Commas inside parentheses are preserved:
#   "Question format variations (open-ended, yes/no)" = one category
# ---------------------------------------------------------------------------

# Generic fallback (used when no role-specific key matches)
categories = Direct explicit requests,
    Indirect or euphemistic requests,
    Implicit or inferred requests,
    Hidden intention within a long text (at least a paragraph),
    Wrapped in fictional context,
    Wrapped in academic or analytical context,
    Third-person framing,
    Hypothetical framing,
    Demanding or insistent phrasing,
    Polite or collaborative phrasing,
    Question format variations (open-ended / yes-no / leading / compound),
    Slang or coded language

# Used when --role user
# Focuses on HOW PEOPLE ASK: request styles, framing techniques, intent levels.
categories_user = Direct explicit requests,
    Indirect or euphemistic requests,
    Implicit or inferred requests,
    Hidden intention within a long text (at least a paragraph),
    Wrapped in fictional context,
    Wrapped in academic or analytical context,
    Third-person framing,
    Hypothetical framing,
    Demanding or insistent phrasing,
    Polite or collaborative phrasing,
    Question format variations (open-ended, yes/no, leading, compound),
    Slang or coded language,
    Obfuscated or intentionally vague wording,
    Information-seeking requests,
    Procedural or instructional requests,
    Transformational requests (rewrite, summarize, translate),
    Creative generation requests,
    Analytical or evaluative requests,
    Decision-support requests,
    Emotional or social-support requests,
    Single clear task requests,
    Multi-step or compound requests,
    Iterative refinement requests,
    Constraint-based requests (format, tone, length, style),
    Clearly benign requests,
    Ambiguous-intent requests,
    Adversarial or evasive requests,
    High-risk or policy-sensitive requests

# Used when --role assistant
# Focuses on HOW THE AI RESPONDS: response types, formatting, safety behavior.
categories_assistant =
    Informational responses,
    Instructional or procedural responses,
    Analytical or reasoning responses,
    Creative or generative responses,
    Transformational responses,
    Decision support responses,
    Conversational or social responses,
    Emotional support responses,
    Meta level or self referential responses,
    Structured data outputs,
    Step by step formatted responses,
    Bullet point or list based responses,
    Long form explanatory responses,
    Concise or direct responses,
    Clarification or follow up questions,
    Assumption based completion,
    Iterative refinement responses,
    Constraint aware formatted responses,
    Safe completion with guidance,
    Refusal or boundary setting responses,
    Risk mitigation or harm reduction responses



# =========================================================
# [thresholds] — Scoring Thresholds
# =========================================================
#
# Used by the evaluator for verdict classification.
# These are initial values — use --auto-calibrate or --train to optimize.

[thresholds]

# KNN voting thresholds:
#   score <= warning_threshold       → NO MATCH
#   warning < score <= match         → WARNING
#   score > match_threshold          → MATCH
match_threshold = 0.70
warning_threshold = 0.50


# =========================================================
# [training] — Anchor Manifold Optimization (AMO)
# =========================================================
#
# Used by --train N to optimize anchor embeddings via gradient descent.
# The training loop nudges anchor vectors in embedding space so that:
#   - Positive anchors move closer to true positive messages
#   - Negative anchors move closer to false-positive (near-miss) messages
#
# Run: python semantic_anchor_generator.py -name NAME --train 100

[training]

# --- Optimizer parameters ---

# Step size for the Adam optimizer.
# Controls how far anchor vectors move per gradient update.
# Too high (>0.01): anchors overshoot, oscillate, or drift too far.
# Too low (<0.001): training converges slowly, may need more epochs.
# Start with 0.005 and adjust based on training_report loss curve.
learning_rate = 0.001

# Number of full passes over the training set.
# Each epoch processes every synthetic example once.
# Early stopping (patience=5) will halt if dev accuracy plateaus,
# so setting this high (30-50) is safe — it won't waste time.
# Too few (<10): may underfit, especially with larger anchor sets.
epochs = 20

# L2 regularization weight on anchor drift.
# Penalizes the squared distance between current and original vectors.
# Acts as a "spring" pulling anchors back toward their starting position.
# Higher (0.5-1.0): anchors barely move, preserves original meaning strongly.
# Lower (0.01-0.05): anchors move freely, may overfit to training data.
# 0.1 is a balanced default — enough freedom to improve, enough pull to stay grounded.
regularization = 0.01

# Temperature for soft top-K KNN voting (the τ in softmax(sim/τ)).
# Applied to the K=20 nearest neighbors (not all anchors).
# Controls how sharply the scoring function weights nearer vs. farther neighbors.
# Lower (0.1-0.2): nearest neighbor dominates — sharp boundary, noisier gradients.
# Higher (1.0-2.0): all K neighbors weighted equally — stable but blurry boundary.
# 0.5 gives a good gradient signal while keeping the boundary reasonably sharp.
temperature = 0.05

# Number of training examples per gradient update.
# The optimizer accumulates loss over this many examples before stepping.
# Smaller (1-4): noisier gradients but more frequent updates.
# Larger (16-32): smoother gradients but fewer updates per epoch.
# 8 works well for typical anchor sets (100-300 anchors).
# Scale up if you have >500 synthetic examples.
batch_size = 8

# --- Drift constraints ---

# Maximum allowed drift from original embedding position.
# Measured as 1 - cosine_similarity(new, original).
# 0.15 means anchors can drift up to 15% in cosine distance.
# This is a soft reference — the hard constraint is min_similarity below.
drift_limit = 0.15

# Cosine anchoring constraint — the hard floor.
# After every epoch, any anchor whose cosine similarity to its original
# vector drops below this threshold is "projected" back (interpolated
# toward the original until similarity >= min_similarity).
#
# This is the critical safety valve that keeps anchors meaningful:
# without it, gradient descent could push an anchor about "phishing emails"
# into an abstract region that scores well but no longer represents phishing.
#
# Higher (0.90-0.95): very conservative, anchors barely move.
#   Use for narrow propositions where the original anchors are already good.
# Lower (0.75-0.80): more freedom, bigger potential improvement.
#   Use for broad propositions where anchors need significant repositioning.
# 0.85 keeps anchors semantically recognizable while allowing meaningful optimization.
min_similarity = 0.85

# --- Synthetic generation controls ---
# These control how the LLM generates adversarial training pairs for --train.
# The LLM creates two types of examples:
#   STEALTH: Harmful intent disguised with benign vocabulary (label = positive)
#   TRAP:    Benign intent loaded with dangerous-sounding keywords (label = negative)

# LLM creativity for adversarial edge case generation.
# Higher values produce more unusual and creative scenarios.
# Lower values stick to more conventional examples.
# Range: 0.0 (deterministic) to 1.0 (maximum creativity)
diversity_temp = 0.9

# How adversarial the LLM-generated training pairs should be.
# This controls the prompt instructions given to the LLM:
#   low:    Straightforward edge cases. Most are relatively clear-cut.
#           Good for initial testing or when the proposition is already narrow.
#   medium: Moderately tricky. At least half require careful reading to classify.
#           Uses some misdirection and vocabulary cross-contamination.
#   high:   Maximally adversarial. Every example is deliberately designed to fool
#           a classifier. Heavy misdirection, plausible deniability, nested framing,
#           and vocabulary cross-contamination. A human reviewer would need to think
#           carefully about each one. Use this for production-grade training.
adversarial_depth = high

# Force varied message formats across synthetic examples.
# When true, the LLM distributes examples across: casual chat, formal emails,
# code comments, academic excerpts, forum posts, customer support tickets,
# social media posts, internal memos, technical docs, creative writing,
# interview transcripts, and legal/compliance text.
# When false, the LLM picks whatever format feels natural (usually chat-like).
context_variety = true


# Soft top-K neighborhood size for KNN voting during training.
# During each forward pass, the scorer finds the K nearest anchors to
# the query and takes a softmax-weighted vote among them. This is the
# differentiable approximation of the evaluator's hard KNN voting.
#   low  (10-15): Sharper decision boundary. Only the closest anchors
#                 vote, so training focuses on the immediate neighborhood.
#                 More sensitive to local structure, noisier gradients.
#   default (20): Matches the evaluator's default knn_size. Good baseline.
#   high (30-50): Smoother boundary. More anchors participate in each
#                 vote, so training considers broader context. Gradients
#                 are more stable but the boundary is less precise.
# Tip: if your anchor set is large (200+), try 30-50 for better
# gradient signal. Must not exceed total anchors (pos + neg);
# clamped automatically if it does.
train_knn_k = 20

kfold = 5

