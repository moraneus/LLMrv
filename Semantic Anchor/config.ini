# =============================================================================
# Semantic Anchor Generator — Configuration
# =============================================================================
#
# This file configures the LLM used to GENERATE anchors.
# (The evaluator uses a separate config_<name>.ini for --mode llm)
#
# Usage:
#   python semantic_anchor_generator.py -p "Your proposition" -name myproject
#


# =============================================================================
# [llm] — LLM Provider for Anchor Generation
# =============================================================================
#
# Supported providers:
#   anthropic   — Claude  (api.anthropic.com)
#   openai      — GPT     (api.openai.com)
#   gemini      — Gemini  (generativelanguage.googleapis.com)
#   grok        — Grok    (api.x.ai)
#   ollama      — Local   (localhost:11434, native API)
#   lmstudio    — Local   (localhost:1234, OpenAI-compatible)
#   vllm        — Local   (localhost:8000, OpenAI-compatible)
#
# Aliases: claude → anthropic, google → gemini, xai → grok, local → lmstudio

[llm]

# --- Uncomment ONE provider block below ---

# >> Anthropic Claude <<
provider = anthropic
model = claude-haiku-4-5-20251001
api_key = sk-ant

# >> OpenAI GPT <<
# provider = openai
# model = gpt-4o
# api_key = sk-...

# >> Google Gemini <<
# provider = gemini
# model = gemini-2.0-flash
# api_key = AIza...

# >> xAI Grok <<
# provider = grok
# model = grok-3-mini-fast
# api_key = xai-...

# >> Local Ollama (free, no API key needed) <<
# provider = ollama
# model = llama3.1:8b
# api_key = not-needed

# >> Local LM Studio (free, OpenAI-compatible) <<
# provider = lmstudio
# model = loaded-model
# api_key = not-needed

# >> Local vLLM (free, OpenAI-compatible) <<
# provider = vllm
# model = meta-llama/Llama-3.1-8B-Instruct
# api_key = not-needed

# For local providers, set base URL via environment variable:
#   OLLAMA_BASE_URL=http://localhost:11434       (ollama)
#   LOCAL_LLM_BASE_URL=http://localhost:1234     (lmstudio, vllm)


# =============================================================================
# [anchors] — Anchor Generation Settings
# =============================================================================

[anchors]

# Number of anchors to generate per round (final count set by -n flag)
num_examples = 20

# Embedding model for similarity scoring and MMR diversity selection
# Used at both generation and evaluation time
#
# Options:
#   all-mpnet-base-v2          — Default, good general-purpose, fast
#   BAAI/bge-large-en-v1.5     — Instruction-tuned, better accuracy, larger
#   all-MiniLM-L6-v2           — Lightweight, fastest, lower accuracy
#
# Any model from https://huggingface.co/models?library=sentence-transformers
embedding_model = BAAI/bge-large-en-v1.5

# NLI cross-encoder model for --mode nli, --mode hybrid, and --compare
# Determines semantic entailment between messages and anchors
#
# Options:
#   cross-encoder/nli-deberta-v3-large  — Best accuracy (default)
#   cross-encoder/nli-deberta-v3-base   — Faster, slightly less accurate
#   cross-encoder/nli-deberta-v3-xsmall — Fastest, lowest accuracy
nli_model = cross-encoder/nli-deberta-v3-large

# Anchor categories — the LLM generates examples for each category
# to ensure diverse coverage. Comma-separated list.
categories = Direct explicit requests,
    Indirect or euphemistic,
    Wrapped in fictional or academic context,
    Demanding or insistent phrasing,
    Question format variations,
    Slang or coded language,
    Hidden intention within a long text (at least a paragraph)


# =============================================================================
# [thresholds] — Scoring Thresholds
# =============================================================================
#
# These are written into anchors_list_<name>.json and used by the evaluator.
#
#   score >= match_threshold   → MATCH   (red)
#   score >= warning_threshold → WARNING (yellow)
#   score < warning_threshold  → CLEAN   (green)
#
# Tune these based on your domain. Use --compare mode to benchmark.

[thresholds]
match_threshold = 0.85
warning_threshold = 0.65
