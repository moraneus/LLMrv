#!/usr/bin/env python3
"""
Semantic Anchor Evaluator: weapons
====================================

Auto-generated by semantic_anchor_generator.py

Loads anchors from: anchors_list_weapons.json

Usage:
  python semantic_anchor_weapons.py                    # Interactive
  python semantic_anchor_weapons.py --verbose           # Full table
  python semantic_anchor_weapons.py --show-examples     # View anchors
  python semantic_anchor_weapons.py --file input.txt    # File mode (### separated)
  python semantic_anchor_weapons.py --graph             # 3D anchor spread visualization
  python semantic_anchor_weapons.py --rerank            # Enable cross-encoder reranking
  python semantic_anchor_weapons.py --nli               # Pure NLI on ALL anchors (most accurate)

Requirements:
  pip install sentence-transformers
  pip install matplotlib scikit-learn       (only for --graph)
  pip install autocorrect                   (only for spell correction in --rerank/--nli)

Scoring modes:
  Default:  Cosine similarity (bi-encoder). Fast, good for clean text.
  --rerank: Cosine prefilter + NLI cross-encoder on top-5. Balanced speed/accuracy.
  --nli:    Pure NLI entailment on ALL anchors. Most accurate, handles paraphrases
            and long multi-sentence messages (auto-splits, scores each sentence).

Long message handling:
  All modes use proposition-guided extraction for messages > EXTRACTION_MIN_WORDS.
  Instead of embedding the full diluted message, the system:
    1. Splits into sentences and embeds each
    2. Selects only sentences relevant to the proposition space (above tau)
    3. Scores the composed relevant chunk
  This defeats distributed attacks where harmful intent is spread across
  innocent-looking sentences interleaved with noise.
"""

import argparse
import json
import os
import sys

# ---------------------------------------------------------------------------
# Configuration loader — reads from anchors_list_<name>.json
# ---------------------------------------------------------------------------

SCRIPT_NAME = "weapons"

def _find_anchors_file():
    """Locate the anchors JSON file relative to this script's directory."""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    filename = "anchors_list_{}.json".format(SCRIPT_NAME)
    # Look in same directory as the script
    path = os.path.join(script_dir, filename)
    if os.path.exists(path):
        return path
    # Look in parent directory (if script is in a subfolder)
    parent = os.path.join(os.path.dirname(script_dir), filename)
    if os.path.exists(parent):
        return parent
    return None


def _load_anchors_config():
    """Load proposition, anchors, and thresholds from the JSON file."""
    path = _find_anchors_file()
    if path is None:
        print("\n  ERROR: Anchors file not found: anchors_list_{}.json".format(SCRIPT_NAME))
        print("  Expected location: same directory as this script")
        print("  Generate it with: python semantic_anchor_generator.py -name {} -ga".format(
            SCRIPT_NAME))
        sys.exit(1)

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    return (
        data["proposition"],
        data["anchors"],
        data.get("match_threshold", 0.55),
        data.get("warning_threshold", 0.45),
    )


PROPOSITION, ANCHORS, MATCH_THRESHOLD, WARNING_THRESHOLD = _load_anchors_config()

# ---------------------------------------------------------------------------
# Reranking config
# ---------------------------------------------------------------------------

RERANK_ALPHA = 0.5         # weight for cosine in combined score (0.5 = equal weight)
RERANK_TOP_K = 5           # number of top anchors to run cross-encoder on
RERANK_SKIP_BELOW = 0.25   # skip reranking if top cosine is below this

# ---------------------------------------------------------------------------
# Proposition-guided extraction config
# ---------------------------------------------------------------------------

EXTRACTION_MIN_WORDS = 30       # only extract for messages longer than this
EXTRACTION_RELEVANCE_TAU = 0.18 # low threshold — keep any topically related sentence
EXTRACTION_MIN_SELECTED = 1     # need at least this many sentences to use extraction
EXTRACTION_MAX_WINDOW = 3       # also try sliding windows of this size for adjacent patterns

# ---------------------------------------------------------------------------
# Colors
# ---------------------------------------------------------------------------

BLUE = "\033[94m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
BOLD = "\033[1m"
DIM = "\033[2m"
CYAN = "\033[96m"
RESET = "\033[0m"


def color_score(score):
    s = "{:.4f}".format(score)
    if score >= MATCH_THRESHOLD:
        return RED + BOLD + s + RESET
    elif score >= WARNING_THRESHOLD:
        return YELLOW + s + RESET
    else:
        return GREEN + s + RESET


# ---------------------------------------------------------------------------
# Spell correction
# ---------------------------------------------------------------------------

_spellchecker = None

def _load_spellchecker():
    global _spellchecker
    if _spellchecker is not None:
        return _spellchecker
    try:
        from autocorrect import Speller
        _spellchecker = Speller(lang="en")
        return _spellchecker
    except ImportError:
        pass
    return None


def correct_spelling(text):
    """Correct spelling, return (corrected_text, list_of_corrections)."""
    checker = _load_spellchecker()
    if checker is None:
        return text, []

    corrected = checker(text)
    if corrected == text:
        return text, []

    orig_words = text.split()
    corr_words = corrected.split()
    changes = []
    for o, c in zip(orig_words, corr_words):
        if o.lower() != c.lower():
            changes.append((o, c))
    return corrected, changes


# ---------------------------------------------------------------------------
# Sentence splitting (robust, always splits)
# ---------------------------------------------------------------------------

def _split_into_sentences(text):
    """
    Split text into sentences. Always splits regardless of length.
    Handles common abbreviations and edge cases.
    """
    import re
    text = text.strip()
    if not text:
        return []

    # Split on sentence boundaries: .!? followed by space+uppercase or end
    parts = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)

    sentences = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        # Also split on semicolons and long conjunctions that separate clauses
        sub_parts = re.split(r';\s+|(?<=\.)\s+(?:Also|Separately|Additionally|Furthermore|Moreover),?\s+', p)
        for sp in sub_parts:
            sp = sp.strip()
            if sp:
                sentences.append(sp)

    # Merge very short fragments (< 3 words) with previous
    merged = []
    for s in sentences:
        if merged and len(s.split()) < 3:
            merged[-1] = merged[-1] + " " + s
        else:
            merged.append(s)

    return merged if merged else [text]


def _split_sentences_legacy(text):
    """Legacy sentence splitter for backward compatibility (skips short texts)."""
    import re
    text = text.strip()
    if not text:
        return []
    if len(text.split()) < 60:
        return [text]
    parts = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
    sentences = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        if sentences and len(p.split()) < 4:
            sentences[-1] = sentences[-1] + " " + p
        else:
            sentences.append(p)
    return sentences if sentences else [text]


# ---------------------------------------------------------------------------
# Proposition-Guided Extraction (core long-message defense)
# ---------------------------------------------------------------------------

def extract_relevant_chunks(model, message, all_embeddings, all_texts):
    """
    Proposition-guided extraction for long messages.

    Instead of embedding the full message (which dilutes signal when harmful
    intent is distributed across innocent-looking sentences), this function:

    1. Splits the message into individual sentences
    2. Embeds each sentence independently
    3. Scores each sentence against ALL anchors (max cosine to any anchor)
    4. Selects sentences above a low relevance threshold tau
    5. Returns the composed relevant chunk for downstream scoring

    Also computes sliding windows to catch adjacent-sentence patterns.

    Returns:
        dict with keys:
            'extracted_text': str - the composed relevant sentences
            'was_extracted': bool - whether extraction was applied
            'all_sentences': list[str] - all sentences from the message
            'selected_indices': list[int] - which sentences were selected
            'sentence_scores': list[float] - max-anchor score per sentence
            'method': str - 'extraction' or 'passthrough'
            'sliding_window_text': str - best sliding window chunk
            'sliding_window_score': float - score of best window
    """
    import numpy as np
    from sentence_transformers.util import cos_sim

    words = message.split()
    result = {
        'extracted_text': message,
        'was_extracted': False,
        'all_sentences': [message],
        'selected_indices': [0],
        'sentence_scores': [0.0],
        'method': 'passthrough',
        'sliding_window_text': message,
        'sliding_window_score': 0.0,
    }

    # Only extract for messages above the word threshold
    if len(words) <= EXTRACTION_MIN_WORDS:
        return result

    # Step 1: Split into sentences
    sentences = _split_into_sentences(message)
    if len(sentences) <= 1:
        return result

    # Step 2: Embed all sentences in one batch
    sent_embeddings = model.encode(sentences, show_progress_bar=False)

    # Step 3: Score each sentence against all anchors (max cosine to any anchor)
    # This is the proposition-guided relevance filter
    sim_matrix = cos_sim(sent_embeddings, all_embeddings)  # (n_sents, n_anchors)
    sentence_scores = sim_matrix.max(dim=1).values.tolist()  # max anchor score per sentence

    # Step 4: Select sentences above the relevance threshold
    selected_indices = [
        i for i, score in enumerate(sentence_scores)
        if score >= EXTRACTION_RELEVANCE_TAU
    ]

    # Step 5: Sliding window analysis (catches adjacent-sentence patterns)
    best_window_score = 0.0
    best_window_text = message
    window_size = min(EXTRACTION_MAX_WINDOW, len(sentences))

    for ws in range(2, window_size + 1):
        for start in range(len(sentences) - ws + 1):
            window_text = " ".join(sentences[start:start + ws])
            window_emb = model.encode([window_text], show_progress_bar=False)
            window_sims = cos_sim(window_emb, all_embeddings)[0]
            window_max = float(window_sims.max())
            if window_max > best_window_score:
                best_window_score = window_max
                best_window_text = window_text

    # Step 6: Build the extracted chunk
    if len(selected_indices) >= EXTRACTION_MIN_SELECTED:
        extracted_text = " ".join(sentences[i] for i in selected_indices)
        result.update({
            'extracted_text': extracted_text,
            'was_extracted': True,
            'all_sentences': sentences,
            'selected_indices': selected_indices,
            'sentence_scores': sentence_scores,
            'method': 'extraction',
            'sliding_window_text': best_window_text,
            'sliding_window_score': best_window_score,
        })
    else:
        # Not enough relevant sentences found — fall back to full message
        # but still report the analysis
        result.update({
            'all_sentences': sentences,
            'selected_indices': selected_indices,
            'sentence_scores': sentence_scores,
            'sliding_window_text': best_window_text,
            'sliding_window_score': best_window_score,
        })

    return result


def _display_extraction_info(extraction, verbose=False):
    """Print extraction diagnostic info."""
    if not extraction['was_extracted']:
        return

    n_total = len(extraction['all_sentences'])
    n_selected = len(extraction['selected_indices'])
    n_dropped = n_total - n_selected

    print("  {}Extraction:{} {} sentences \u2192 {} relevant, {} noise dropped".format(
        CYAN, RESET, n_total, n_selected, n_dropped))

    if verbose:
        for i, (sent, score) in enumerate(zip(
                extraction['all_sentences'], extraction['sentence_scores'])):
            marker = "\u2714" if i in extraction['selected_indices'] else "\u2718"
            disp = sent if len(sent) <= 60 else sent[:57] + "..."
            score_color = GREEN if score < EXTRACTION_RELEVANCE_TAU else YELLOW
            print("    {} {}{:.3f}{} \"{}\"".format(
                marker, score_color, score, RESET, disp))

    if extraction['sliding_window_score'] > 0:
        ws = extraction['sliding_window_text']
        ws_disp = ws if len(ws) <= 70 else ws[:67] + "..."
        print("  {}Best window:{} score={:.4f} \"{}\"".format(
            DIM, RESET, extraction['sliding_window_score'], ws_disp))


# ---------------------------------------------------------------------------
# Embedding & Scoring
# ---------------------------------------------------------------------------

def load_model():
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        print("\n  ERROR: sentence-transformers not installed.")
        print("  Run: pip install sentence-transformers")
        sys.exit(1)
    print("  Loading embedding model: all-mpnet-base-v2...")
    return SentenceTransformer("all-mpnet-base-v2")


def prepare_anchors(model):
    all_texts = []
    all_categories = []
    for cat, examples in ANCHORS.items():
        for ex in examples:
            all_texts.append(ex)
            all_categories.append(cat)
    print("  Embedding {} anchors...".format(len(all_texts)))
    all_embeddings = model.encode(all_texts, show_progress_bar=False)
    print("  Ready.\n")
    return all_texts, all_categories, all_embeddings


def score_message(model, message, all_embeddings, all_texts, all_categories,
                  use_extraction=True):
    """
    Cosine scoring with proposition-guided extraction for long messages.

    For short messages: embeds the full message, computes cosine to all anchors.
    For long messages (> EXTRACTION_MIN_WORDS): extracts relevant sentences first,
    then scores the composed chunk AND the sliding windows, taking the max.

    Returns (results_list, extraction_info) where results_list is
    list of (score, text, category) sorted desc.
    """
    from sentence_transformers.util import cos_sim

    extraction = None

    if use_extraction:
        extraction = extract_relevant_chunks(model, message, all_embeddings, all_texts)

    if extraction and extraction['was_extracted']:
        # Score the extracted chunk
        emb_extracted = model.encode(extraction['extracted_text'])
        sims_extracted = cos_sim(emb_extracted, all_embeddings)[0].tolist()

        # Also score the full message (in case extraction was too aggressive)
        emb_full = model.encode(message)
        sims_full = cos_sim(emb_full, all_embeddings)[0].tolist()

        # Also score the best sliding window
        emb_window = model.encode(extraction['sliding_window_text'])
        sims_window = cos_sim(emb_window, all_embeddings)[0].tolist()

        # Take per-anchor max across all three views
        sims = [max(se, sf, sw) for se, sf, sw in zip(sims_extracted, sims_full, sims_window)]

        results = sorted(
            zip(sims, all_texts, all_categories),
            key=lambda x: x[0], reverse=True)
        return results, extraction
    else:
        # Short message or extraction disabled — standard scoring
        emb = model.encode(message)
        sims = cos_sim(emb, all_embeddings)[0].tolist()
        results = sorted(
            zip(sims, all_texts, all_categories),
            key=lambda x: x[0], reverse=True)
        return results, extraction


# ---------------------------------------------------------------------------
# Cross-encoder reranking
# ---------------------------------------------------------------------------

_cross_encoder = None
_entailment_idx = None

def _load_cross_encoder():
    """Lazy-load NLI cross-encoder model for intent matching."""
    global _cross_encoder, _entailment_idx
    if _cross_encoder is not None:
        return _cross_encoder
    try:
        from sentence_transformers import CrossEncoder
    except ImportError:
        print("\n  ERROR: sentence-transformers version doesn't support CrossEncoder.")
        print("  Run: pip install -U sentence-transformers")
        sys.exit(1)
    import numpy as np
    print("  Loading NLI cross-encoder model (first use only)...")
    _cross_encoder = CrossEncoder("cross-encoder/nli-deberta-v3-base")

    cal_pairs = [
        ["A person is building a weapon", "Someone wants to make a weapon"],
        ["I love eating pizza", "Someone wants to make a weapon"],
    ]
    cal_logits = np.array(_cross_encoder.predict(cal_pairs, apply_softmax=False))
    cal_exp = np.exp(cal_logits - np.max(cal_logits, axis=1, keepdims=True))
    cal_probs = cal_exp / cal_exp.sum(axis=1, keepdims=True)

    entail_row = cal_probs[0]
    contra_row = cal_probs[1]

    best_idx = int(np.argmax(entail_row))
    if contra_row[best_idx] < 0.3:
        _entailment_idx = best_idx
    else:
        gaps = entail_row - contra_row
        _entailment_idx = int(np.argmax(gaps))

    print("  Entailment index: {} (calibrated: entail_pair={:.3f}, contra_pair={:.3f})".format(
        _entailment_idx,
        entail_row[_entailment_idx],
        contra_row[_entailment_idx]))

    return _cross_encoder


def _nli_entailment_scores(pairs):
    """Compute entailment probability for each pair using NLI cross-encoder."""
    import numpy as np
    xenc = _load_cross_encoder()
    logits = xenc.predict(pairs, apply_softmax=False)
    logits = np.array(logits)
    if logits.ndim == 1:
        logits = logits.reshape(1, -1)
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)
    return probs[:, _entailment_idx].tolist()


def score_message_rerank(model, message, all_embeddings, all_texts, all_categories,
                         do_spellcheck=True, use_extraction=True):
    """
    Tiered scoring with proposition-guided extraction:
      spell-correct -> extract relevant sentences -> cosine pass -> NLI rerank.

    For long messages, extraction removes noise sentences before the cosine pass,
    giving the bi-encoder a focused chunk. The NLI cross-encoder then reranks the
    top-K anchors against the extracted chunk (not the full diluted message).

    Returns list of (combined, cosine, entailment, text, category, corrected_msg,
                     corrections, extraction_info)
    sorted by combined score descending.
    """
    # Step 0: spell correction
    corrected = message
    corrections = []
    if do_spellcheck:
        corrected, corrections = correct_spelling(message)

    # Step 1: proposition-guided extraction
    extraction = None
    effective_message = corrected

    if use_extraction:
        extraction = extract_relevant_chunks(model, corrected, all_embeddings, all_texts)
        if extraction['was_extracted']:
            # Use extracted chunk for scoring, but keep full message as fallback
            effective_message = extraction['extracted_text']

    # Step 2: cosine pass — score extracted chunk AND full message, take max
    cosine_results_extracted, _ = score_message(
        model, effective_message, all_embeddings, all_texts, all_categories,
        use_extraction=False)

    if extraction and extraction['was_extracted']:
        cosine_results_full, _ = score_message(
            model, corrected, all_embeddings, all_texts, all_categories,
            use_extraction=False)

        # Also score sliding window
        cosine_results_window, _ = score_message(
            model, extraction['sliding_window_text'], all_embeddings, all_texts,
            all_categories, use_extraction=False)

        # Merge: per-anchor max across views
        score_map = {}
        for score, text, cat in cosine_results_extracted:
            score_map[text] = (max(score, score_map.get(text, (0,))[0]), text, cat)
        for score, text, cat in cosine_results_full:
            if text in score_map:
                score_map[text] = (max(score, score_map[text][0]), text, cat)
        for score, text, cat in cosine_results_window:
            if text in score_map:
                score_map[text] = (max(score, score_map[text][0]), text, cat)

        cosine_results = sorted(score_map.values(), key=lambda x: x[0], reverse=True)
    else:
        cosine_results = cosine_results_extracted

    top_cosine = cosine_results[0][0]

    # Step 3: check if reranking is needed
    if top_cosine < RERANK_SKIP_BELOW:
        return [(s, s, 0.0, t, c, corrected, corrections, extraction)
                for s, t, c in cosine_results]

    # Step 4: NLI cross-encoder rerank top-K
    # Run NLI against the extracted chunk (not full message) for focused signal
    top_k = cosine_results[:RERANK_TOP_K]

    pairs_fwd = [[effective_message, anchor_text] for _, anchor_text, _ in top_k]
    pairs_bwd = [[anchor_text, effective_message] for _, anchor_text, _ in top_k]

    ent_fwd = _nli_entailment_scores(pairs_fwd)
    ent_bwd = _nli_entailment_scores(pairs_bwd)

    # If extraction was applied, also run NLI on the best sliding window
    if extraction and extraction['was_extracted'] and extraction['sliding_window_score'] > 0:
        win_text = extraction['sliding_window_text']
        pairs_win_fwd = [[win_text, anchor_text] for _, anchor_text, _ in top_k]
        pairs_win_bwd = [[anchor_text, win_text] for _, anchor_text, _ in top_k]
        ent_win_fwd = _nli_entailment_scores(pairs_win_fwd)
        ent_win_bwd = _nli_entailment_scores(pairs_win_bwd)
        # Take max across extraction + window for each anchor
        xenc_scores = [max(f, b, wf, wb) for f, b, wf, wb in
                       zip(ent_fwd, ent_bwd, ent_win_fwd, ent_win_bwd)]
    else:
        xenc_scores = [max(f, b) for f, b in zip(ent_fwd, ent_bwd)]

    # Step 5: combine
    combined_top = []
    for i, (cos_s, text, cat) in enumerate(top_k):
        xs = xenc_scores[i]
        combined = RERANK_ALPHA * cos_s + (1 - RERANK_ALPHA) * xs
        combined_top.append((combined, cos_s, xs, text, cat, corrected, corrections, extraction))

    combined_top.sort(key=lambda x: x[0], reverse=True)

    rest = [(s, s, 0.0, t, c, corrected, corrections, extraction)
            for s, t, c in cosine_results[RERANK_TOP_K:]]

    return combined_top + rest


# ---------------------------------------------------------------------------
# NLI-only scoring (--nli mode) with proposition-guided extraction
# ---------------------------------------------------------------------------

def score_message_nli(message, all_texts, all_categories, model=None,
                      all_embeddings=None, do_spellcheck=True, use_extraction=True):
    """
    NLI scoring on ALL anchors with proposition-guided extraction.

    For long messages, extraction focuses the text into relevant views.
    NLI runs on every anchor against each view, taking the per-anchor max.

    Returns (results, corrected, corrections, extraction_info)
    where results is list of (score, text, category, best_source) sorted desc.
    """
    # Step 0: spell correction
    corrected = message
    corrections = []
    if do_spellcheck:
        corrected, corrections = correct_spelling(message)

    # Step 1: proposition-guided extraction
    extraction = None
    if use_extraction and model is not None and all_embeddings is not None:
        extraction = extract_relevant_chunks(model, corrected, all_embeddings, all_texts)

    _load_cross_encoder()

    best_scores = [0.0] * len(all_texts)
    best_sources = [""] * len(all_texts)

    # Step 2: Build views based on extraction
    views = []  # list of (text, label)

    if extraction and extraction['was_extracted']:
        views.append((extraction['extracted_text'], "[extracted chunk]"))
        if extraction['sliding_window_score'] > 0:
            views.append((extraction['sliding_window_text'], "[sliding window]"))
        # Per-sentence: catches any single sentence that scores high on its own
        for sent in extraction['all_sentences']:
            label = sent if len(sent) <= 40 else sent[:37] + "..."
            views.append((sent, label))
        views.append((corrected, "[full message]"))
    else:
        sentences = _split_into_sentences(corrected)
        for sent in sentences:
            label = sent if len(sent) <= 40 else sent[:37] + "..."
            views.append((sent, label))

    # Step 3: Run NLI on ALL anchors for each view
    for view_text, view_label in views:
        pairs_fwd = [[view_text, anchor] for anchor in all_texts]
        pairs_bwd = [[anchor, view_text] for anchor in all_texts]
        ent_fwd = _nli_entailment_scores(pairs_fwd)
        ent_bwd = _nli_entailment_scores(pairs_bwd)

        for i in range(len(all_texts)):
            score = max(ent_fwd[i], ent_bwd[i])
            if score > best_scores[i]:
                best_scores[i] = score
                best_sources[i] = view_label

    # Build results
    results = list(zip(best_scores, all_texts, all_categories, best_sources))
    results.sort(key=lambda x: x[0], reverse=True)

    return results, corrected, corrections, extraction


# ---------------------------------------------------------------------------
# Display
# ---------------------------------------------------------------------------

def print_banner():
    title = "SEMANTIC ANCHORS: " + SCRIPT_NAME.upper()
    w = 60
    print("")
    print(BOLD + "\u2554" + "\u2550" * w + "\u2557")
    print("\u2551" + title.center(w) + "\u2551")
    print("\u255a" + "\u2550" * w + "\u255d" + RESET)
    print("  Proposition: \"{}\"".format(PROPOSITION))
    print("")


def print_examples():
    total = sum(len(v) for v in ANCHORS.values())
    print("\n{}Generated Anchors ({} total){}".format(BOLD, total, RESET))
    print("=" * 70)
    for cat, examples in ANCHORS.items():
        print("\n{}{}  [{}]{} ({} examples)".format(BOLD, BLUE, cat, RESET, len(examples)))
        print("  " + "-" * 60)
        for i, ex in enumerate(examples, 1):
            print("  {}{:>3}.{} {}".format(DIM, i, RESET, ex))
    print()


def format_verdict(top_score):
    if top_score >= MATCH_THRESHOLD:
        return "{}{}\u25a0 MATCH{} (score {:.4f} \u2265 {})".format(
            RED, BOLD, RESET, top_score, MATCH_THRESHOLD)
    elif top_score >= WARNING_THRESHOLD:
        return "{}{}\u25a0 WARNING{} (score {:.4f} \u2265 {})".format(
            YELLOW, BOLD, RESET, top_score, WARNING_THRESHOLD)
    else:
        return "{}{}\u25a0 NO MATCH{} (score {:.4f} < {})".format(
            GREEN, BOLD, RESET, top_score, WARNING_THRESHOLD)


# --- Cosine display ---

def display_default(results, extraction=None, top_n=3, verbose_extract=False):
    if extraction:
        _display_extraction_info(extraction, verbose=verbose_extract)
    print("\n  {}\n".format(format_verdict(results[0][0])))
    print("  {:<6} {:>8}  {:<35} {}".format("Rank", "Score", "Category", "Nearest Anchor"))
    print("  " + "-" * 95)
    for rank, (score, text, cat) in enumerate(results[:top_n], 1):
        cs = color_score(score)
        dt = text if len(text) <= 55 else text[:52] + "..."
        dc = cat if len(cat) <= 33 else cat[:30] + "..."
        print("  {:<6} {:>17}  {}{:<35}{} \"{}\"".format(rank, cs, DIM, dc, RESET, dt))
    print()


def display_verbose(results, extraction=None):
    if extraction:
        _display_extraction_info(extraction, verbose=True)
    print("\n  {}\n".format(format_verdict(results[0][0])))
    print("  {:<5} {:>8}  {:<35} {}".format("#", "Score", "Category", "Anchor Text"))
    print("  " + "=" * 100)
    for rank, (score, text, cat) in enumerate(results, 1):
        cs = color_score(score)
        dc = cat if len(cat) <= 33 else cat[:30] + "..."
        zone = ""
        if score >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif score >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17}  {}{:<35}{} \"{}\"{}".format(
            rank, cs, DIM, dc, RESET, text, zone))
    above_m = sum(1 for s, _, _ in results if s >= MATCH_THRESHOLD)
    above_w = sum(1 for s, _, _ in results if s >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {}{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w, RESET))
    print()


# --- Reranked display ---

def _show_corrections_rerank(results):
    if not results:
        return
    corrections = results[0][6]
    corrected = results[0][5]
    extraction = results[0][7] if len(results[0]) > 7 else None
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
        print("  {}Evaluating:{} \"{}\"".format(DIM, RESET, corrected))
    if extraction:
        _display_extraction_info(extraction)


def display_default_rerank(results, top_n=3):
    top_combined = results[0][0]
    _show_corrections_rerank(results)
    print("\n  {}\n".format(format_verdict(top_combined)))
    print("  {:<6} {:>8} {:>10} {:>10}  {:<30} {}".format(
        "Rank", "Combined", "Cosine", "CrossEnc", "Category", "Nearest Anchor"))
    print("  " + "-" * 110)
    for rank, item in enumerate(results[:top_n], 1):
        combined, cosine, xenc, text, cat = item[0], item[1], item[2], item[3], item[4]
        cs = color_score(combined)
        dt = text if len(text) <= 45 else text[:42] + "..."
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        xe_s = "{:.4f}".format(xenc) if xenc > 0 else DIM + "  -- " + RESET
        print("  {:<6} {:>17} {:>10.4f} {:>10}  {}{:<30}{} \"{}\"".format(
            rank, cs, cosine, xe_s, DIM, dc, RESET, dt))
    print()


def display_verbose_rerank(results):
    top_combined = results[0][0]
    _show_corrections_rerank(results)
    print("\n  {}\n".format(format_verdict(top_combined)))
    print("  {:<5} {:>8} {:>10} {:>10}  {:<30} {}".format(
        "#", "Combined", "Cosine", "CrossEnc", "Category", "Anchor Text"))
    print("  " + "=" * 115)
    for rank, item in enumerate(results, 1):
        combined, cosine, xenc, text, cat = item[0], item[1], item[2], item[3], item[4]
        cs = color_score(combined)
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        xe_s = "{:.4f}".format(xenc) if xenc > 0 else DIM + "  -- " + RESET
        zone = ""
        if combined >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif combined >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17} {:>10.4f} {:>10}  {}{:<30}{} \"{}\"{}".format(
            rank, cs, cosine, xe_s, DIM, dc, RESET, text, zone))
    above_m = sum(1 for r in results if r[0] >= MATCH_THRESHOLD)
    above_w = sum(1 for r in results if r[0] >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {} | "
          "Cross-encoder on top {}{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w,
        RERANK_TOP_K, RESET))
    print()


# --- NLI display ---

def display_default_nli(results, corrected, corrections, extraction=None, top_n=3):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction)
    print("\n  {}\n".format(format_verdict(top_score)))
    print("  {:<6} {:>8}  {:<30} {}".format(
        "Rank", "NLI", "Category", "Nearest Anchor"))
    print("  " + "-" * 100)
    for rank, (score, text, cat, best_src) in enumerate(results[:top_n], 1):
        cs = color_score(score)
        dt = text if len(text) <= 50 else text[:47] + "..."
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        print("  {:<6} {:>17}  {}{:<30}{} \"{}\"".format(
            rank, cs, DIM, dc, RESET, dt))
        if best_src and best_src not in (corrected, "[full message]"):
            bs = best_src if len(best_src) <= 65 else best_src[:62] + "..."
            print("  {}       matched via: {}{} ".format(DIM, bs, RESET))
    print()


def display_verbose_nli(results, corrected, corrections, extraction=None):
    top_score = results[0][0]
    if corrections:
        print("  {}Spell corrected:{} {}".format(CYAN, RESET,
            ", ".join("{} \u2192 {}".format(o, c) for o, c in corrections)))
    if extraction:
        _display_extraction_info(extraction, verbose=True)
    print("\n  {}\n".format(format_verdict(top_score)))
    print("  {:<5} {:>8}  {:<30} {}".format("#", "NLI", "Category", "Anchor Text"))
    print("  " + "=" * 105)
    for rank, (score, text, cat, best_src) in enumerate(results, 1):
        cs = color_score(score)
        dc = cat if len(cat) <= 28 else cat[:25] + "..."
        zone = ""
        if score >= MATCH_THRESHOLD:
            zone = " " + RED + "\u25c4 MATCH" + RESET
        elif score >= WARNING_THRESHOLD:
            zone = " " + YELLOW + "\u25c4 WARN" + RESET
        print("  {:<5} {:>17}  {}{:<30}{} \"{}\"{}".format(
            rank, cs, DIM, dc, RESET, text, zone))
    above_m = sum(1 for s, _, _, _ in results if s >= MATCH_THRESHOLD)
    above_w = sum(1 for s, _, _, _ in results if s >= WARNING_THRESHOLD)
    print("\n  {}Total: {} | Above match ({}): {} | Above warning ({}): {} | "
          "NLI on ALL anchors{}".format(
        DIM, len(results), MATCH_THRESHOLD, above_m, WARNING_THRESHOLD, above_w,
        RESET))
    print()


# ---------------------------------------------------------------------------
# 3D Visualization
# ---------------------------------------------------------------------------

def show_graph(model):
    """3D interactive scatter plot of anchor embeddings using PCA."""
    try:
        import matplotlib
        from sklearn.decomposition import PCA
        import numpy as np
    except ImportError as e:
        print("\n  ERROR: Missing dependency for --graph: {}".format(e))
        print("  Run: pip install matplotlib scikit-learn")
        sys.exit(1)

    backend_set = False
    for backend in ["macosx", "Qt5Agg", "TkAgg"]:
        try:
            matplotlib.use(backend)
            backend_set = True
            break
        except Exception:
            continue
    if not backend_set:
        matplotlib.use("Agg")

    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401

    using_agg = matplotlib.get_backend().lower() == "agg"

    all_texts = []
    all_cats = []
    for cat, examples in ANCHORS.items():
        for ex in examples:
            all_texts.append(ex)
            all_cats.append(cat)

    print("  Embedding anchors for visualization...")
    all_embs = model.encode(all_texts, show_progress_bar=False)
    prop_emb = model.encode([PROPOSITION])

    combined = np.vstack([all_embs, prop_emb])

    pca = PCA(n_components=3)
    coords_3d = pca.fit_transform(combined)
    anchor_coords = coords_3d[:-1]
    prop_coord = coords_3d[-1]

    explained = pca.explained_variance_ratio_
    print("  PCA variance explained: {:.1f}% + {:.1f}% + {:.1f}% = {:.1f}%".format(
        explained[0]*100, explained[1]*100, explained[2]*100, sum(explained)*100))

    unique_cats = list(dict.fromkeys(all_cats))
    try:
        cmap = matplotlib.colormaps.get_cmap("tab10")
    except AttributeError:
        cmap = plt.cm.get_cmap("tab10")
    cat_colors = {cat: cmap(i % 10) for i, cat in enumerate(unique_cats)}

    fig = plt.figure(figsize=(12, 9))
    ax = fig.add_subplot(111, projection="3d")

    for cat in unique_cats:
        mask = [i for i, c in enumerate(all_cats) if c == cat]
        xs = [anchor_coords[i, 0] for i in mask]
        ys = [anchor_coords[i, 1] for i in mask]
        zs = [anchor_coords[i, 2] for i in mask]
        ax.scatter(xs, ys, zs, c=[cat_colors[cat]], s=50, alpha=0.7,
                   edgecolors="white", linewidths=0.5, label=cat)

    ax.scatter([prop_coord[0]], [prop_coord[1]], [prop_coord[2]],
               c="red", s=300, marker="*", edgecolors="black",
               linewidths=1, label="PROPOSITION", zorder=10)

    for i in range(len(all_texts)):
        ax.plot([prop_coord[0], anchor_coords[i, 0]],
                [prop_coord[1], anchor_coords[i, 1]],
                [prop_coord[2], anchor_coords[i, 2]],
                color="red", alpha=0.08, linewidth=0.5)

    ax.set_xlabel("PC1 ({:.1f}%)".format(explained[0]*100))
    ax.set_ylabel("PC2 ({:.1f}%)".format(explained[1]*100))
    ax.set_zlabel("PC3 ({:.1f}%)".format(explained[2]*100))
    ax.set_title("Semantic Anchors: {} \u2014 {} anchors in 3D embedding space".format(
        SCRIPT_NAME, len(all_texts)), fontsize=13, fontweight="bold")
    ax.legend(loc="upper left", fontsize=8, framealpha=0.9)

    norms = np.linalg.norm(all_embs, axis=1, keepdims=True)
    normed = all_embs / (norms + 1e-10)
    sim_m = normed @ normed.T
    np.fill_diagonal(sim_m, 0)
    stats = "Avg pairwise sim: {:.4f} | Min: {:.4f} | Max: {:.4f}".format(
        sim_m.mean(), sim_m[sim_m > 0].min(), sim_m.max())
    ax.text2D(0.02, 0.96, stats, transform=ax.transAxes, fontsize=8,
              bbox=dict(boxstyle="round", facecolor="white", alpha=0.8))

    plt.tight_layout()

    if using_agg:
        out_file = "semantic_anchor_{}_3d.png".format(SCRIPT_NAME)
        plt.savefig(out_file, dpi=150)
        print("  Backend is non-interactive. Saved plot to: {}".format(out_file))
        import subprocess, platform
        try:
            if platform.system() == "Darwin":
                subprocess.Popen(["open", out_file])
            elif platform.system() == "Linux":
                subprocess.Popen(["xdg-open", out_file])
            elif platform.system() == "Windows":
                os.startfile(out_file)
        except Exception:
            pass
    else:
        print("  Showing 3D plot (drag to rotate)...")
        plt.show()


# ---------------------------------------------------------------------------
# Multi-line input handling
# ---------------------------------------------------------------------------

def _read_message(prompt):
    """
    Read a complete message from the user. Handles both single-line and
    multi-line input reliably without timing-based paste detection.

    Rules:
      - If the line ends with sentence-ending punctuation (. ? ! " ' ) ]),
        it's treated as complete and evaluated immediately.
      - If it doesn't (indicating a line break mid-sentence from a paste),
        a continuation prompt is shown. The user keeps pasting/typing until
        they enter a blank line.
      - Commands (starting with /) are always single-line.
    """
    first_line = input(prompt).strip()
    if not first_line:
        return first_line

    # Commands are always single-line
    if first_line.startswith("/"):
        return first_line

    # Check if the line looks complete (ends with sentence punctuation)
    if first_line and first_line[-1] in '.?!"\')]':
        # Try to drain any paste buffer quickly (best-effort)
        try:
            import select, sys
            lines = [first_line]
            while True:
                ready, _, _ = select.select([sys.stdin], [], [], 0.08)
                if ready:
                    line = sys.stdin.readline()
                    if not line:
                        break
                    line = line.strip()
                    if line:
                        lines.append(line)
                else:
                    break
            return " ".join(lines)
        except (ImportError, OSError):
            return first_line

    # Line doesn't end with punctuation — it's a multi-line paste
    # Show continuation prompt and read until blank line
    lines = [first_line]
    while True:
        try:
            line = input("  {}...>{} ".format(BOLD, RESET))
        except (EOFError, KeyboardInterrupt):
            break
        if not line.strip():
            break
        lines.append(line.strip())

    return " ".join(lines)


# ---------------------------------------------------------------------------
# Interactive mode
# ---------------------------------------------------------------------------

def run_interactive(model, embs, texts, cats, verbose=False, use_rerank=False,
                    use_nli=False, use_extraction=True):
    if use_nli:
        mode_label = " + NLI-only (all anchors)"
    elif use_rerank:
        mode_label = " + CrossEncoder reranking"
    else:
        mode_label = ""
    extract_label = " + extraction" if use_extraction else ""
    print("{}Interactive Mode{}{}{}".format(BOLD, mode_label, extract_label, RESET))
    print("  Type a message to evaluate. Commands: /verbose  /top N  /rerank  /nli  /extract  /quit\n")
    print("  Thresholds: match={}{}{}  warning={}{}{}".format(
        RED, MATCH_THRESHOLD, RESET, YELLOW, WARNING_THRESHOLD, RESET))
    if use_nli:
        print("  NLI mode: {}ON{} (entailment on ALL anchors)".format(
            GREEN, RESET))
    elif use_rerank:
        print("  Reranking: {}ON{} (alpha={}, top-k={}, spell-correct=ON)".format(
            GREEN, RESET, RERANK_ALPHA, RERANK_TOP_K))
    print("  Extraction: {}{}{}  (long-message defense, tau={})".format(
        GREEN if use_extraction else YELLOW,
        "ON" if use_extraction else "OFF",
        RESET, EXTRACTION_RELEVANCE_TAU))
    print("  " + "-" * 70)
    top_n = 3
    verbose_extract = False

    while True:
        try:
            msg = _read_message("\n  {}Message>{} ".format(BOLD, RESET))
        except (EOFError, KeyboardInterrupt):
            print("\n\n  Goodbye!")
            break
        if not msg:
            continue

        if msg.startswith("/"):
            c = msg.lower().split()
            if c[0] in ("/quit", "/exit", "/q"):
                print("  Goodbye!"); break
            elif c[0] == "/verbose":
                verbose = not verbose
                print("  Verbose: {}".format("ON" if verbose else "OFF")); continue
            elif c[0] == "/top" and len(c) > 1:
                try: top_n = int(c[1]); print("  Showing top {}".format(top_n))
                except ValueError: print("  Usage: /top N")
                continue
            elif c[0] == "/rerank":
                use_rerank = not use_rerank
                use_nli = False
                if use_rerank:
                    _load_cross_encoder()
                print("  Reranking: {}  NLI: OFF".format("ON" if use_rerank else "OFF")); continue
            elif c[0] == "/nli":
                use_nli = not use_nli
                use_rerank = False
                if use_nli:
                    _load_cross_encoder()
                print("  NLI: {}  Reranking: OFF".format("ON" if use_nli else "OFF")); continue
            elif c[0] == "/extract":
                use_extraction = not use_extraction
                print("  Extraction: {} (tau={})".format(
                    "ON" if use_extraction else "OFF", EXTRACTION_RELEVANCE_TAU)); continue
            elif c[0] == "/extractv":
                verbose_extract = not verbose_extract
                print("  Verbose extraction: {}".format(
                    "ON" if verbose_extract else "OFF")); continue
            elif c[0] == "/help":
                print("  /verbose   \u2014 toggle full table")
                print("  /top N     \u2014 show top N results")
                print("  /rerank    \u2014 toggle cross-encoder reranking + spell correction")
                print("  /nli       \u2014 toggle NLI scoring (cross-encoder on all anchors)")
                print("  /extract   \u2014 toggle proposition-guided extraction (long msgs)")
                print("  /extractv  \u2014 toggle verbose extraction diagnostics")
                print("  /quit      \u2014 exit"); continue
            else:
                print("  Unknown command. /help for options."); continue

        if use_nli:
            results, corrected, corrections, extraction = score_message_nli(
                msg, texts, cats, model=model, all_embeddings=embs,
                use_extraction=use_extraction)
            if verbose:
                display_verbose_nli(results, corrected, corrections, extraction)
            else:
                display_default_nli(results, corrected, corrections, extraction, top_n)
        elif use_rerank:
            results = score_message_rerank(model, msg, embs, texts, cats,
                                          use_extraction=use_extraction)
            if verbose:
                display_verbose_rerank(results)
            else:
                display_default_rerank(results, top_n)
        else:
            results, extraction = score_message(model, msg, embs, texts, cats,
                                               use_extraction=use_extraction)
            if verbose:
                display_verbose(results, extraction)
            else:
                display_default(results, extraction, top_n, verbose_extract)


# ---------------------------------------------------------------------------
# File mode
# ---------------------------------------------------------------------------

def run_file(filepath, model, embs, texts, cats, verbose=False, use_rerank=False,
             use_nli=False, use_extraction=True):
    if not os.path.exists(filepath):
        print("  ERROR: File not found: {}".format(filepath)); sys.exit(1)
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    sentences = [s.strip() for s in content.split("###") if s.strip()]
    if use_nli:
        mode_label = " (NLI-only)"
    elif use_rerank:
        mode_label = " (reranking enabled)"
    else:
        mode_label = ""
    extract_label = " + extraction" if use_extraction else ""
    print("  Processing {} sentences from: {}{}{}\n".format(
        len(sentences), filepath, mode_label, extract_label))
    print("  " + "=" * 100)
    all_top_scores = []
    for idx, sent in enumerate(sentences, 1):
        disp = sent[:80] + ("..." if len(sent) > 80 else "")
        print("\n  {}[{}/{}] Message:{} \"{}\"".format(BOLD, idx, len(sentences), RESET, disp))

        if use_nli:
            results, corrected, corrections, extraction = score_message_nli(
                sent, texts, cats, model=model, all_embeddings=embs,
                use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose_nli(results, corrected, corrections, extraction)
            else:
                display_default_nli(results, corrected, corrections, extraction)
        elif use_rerank:
            results = score_message_rerank(model, sent, embs, texts, cats,
                                          use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose_rerank(results)
            else:
                display_default_rerank(results)
        else:
            results, extraction = score_message(model, sent, embs, texts, cats,
                                               use_extraction=use_extraction)
            top_score = results[0][0]
            if verbose:
                display_verbose(results, extraction)
            else:
                display_default(results, extraction)
        all_top_scores.append(top_score)

    matches = sum(1 for s in all_top_scores if s >= MATCH_THRESHOLD)
    warns = sum(1 for s in all_top_scores if WARNING_THRESHOLD <= s < MATCH_THRESHOLD)
    clean = sum(1 for s in all_top_scores if s < WARNING_THRESHOLD)
    print("\n  {}Summary{}".format(BOLD, RESET))
    print("  " + "=" * 60)
    print("  Total sentences:  {}".format(len(sentences)))
    print("  {}\u25a0 Matches:         {}{}".format(RED, matches, RESET))
    print("  {}\u25a0 Warnings:        {}{}".format(YELLOW, warns, RESET))
    print("  {}\u25a0 No match:        {}{}".format(GREEN, clean, RESET))
    print()


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Semantic Anchor Evaluator: " + SCRIPT_NAME)
    parser.add_argument("--file", "-f", type=str, default=None,
        help="Input file with sentences separated by ###")
    parser.add_argument("--verbose", "-v", action="store_true",
        help="Show full scored table (all anchors sorted)")
    parser.add_argument("--show-examples", action="store_true",
        help="Print all positive examples by category")
    parser.add_argument("--graph", "-g", action="store_true",
        help="Show 3D interactive visualization of anchor spread")
    parser.add_argument("--rerank", "-r", action="store_true",
        help="Enable cross-encoder reranking + spell correction")
    parser.add_argument("--nli", action="store_true",
        help="NLI scoring on all anchors (most accurate, slower)")
    parser.add_argument("--no-extract", action="store_true",
        help="Disable proposition-guided extraction for long messages")
    args = parser.parse_args()

    if args.nli and args.rerank:
        print("  NOTE: --nli and --rerank are mutually exclusive. Using --nli.")
        args.rerank = False

    use_extraction = not args.no_extract

    print_banner()

    if args.show_examples:
        print_examples()
        if not args.file and not args.graph:
            try:
                r = input("  Enter interactive mode? [Y/n] ").strip().lower()
                if r == "n": return
            except (EOFError, KeyboardInterrupt):
                print(); return

    model = load_model()

    if args.graph:
        show_graph(model)
        return

    all_texts, all_cats, all_embs = prepare_anchors(model)

    if use_extraction:
        print("  Extraction: {}ON{} (tau={}, min_words={})".format(
            GREEN, RESET, EXTRACTION_RELEVANCE_TAU, EXTRACTION_MIN_WORDS))

    if args.nli:
        _load_cross_encoder()
        checker = _load_spellchecker()
        spell_status = "{}ON{}".format(GREEN, RESET) if checker else "{}OFF (pip install autocorrect){}".format(YELLOW, RESET)
        print("  NLI-only mode: {}ON{}".format(GREEN, RESET))
        print("  Spell correction: {}".format(spell_status))
        total_anchors = sum(len(v) for v in ANCHORS.values())
        print("  Scoring: NLI entailment on ALL {} anchors".format(total_anchors))
        print("  Long messages: proposition-guided extraction + multi-view NLI\n")
    elif args.rerank:
        _load_cross_encoder()
        checker = _load_spellchecker()
        spell_status = "{}ON{}".format(GREEN, RESET) if checker else "{}OFF (pip install autocorrect){}".format(YELLOW, RESET)
        print("  Reranking mode: {}ON{}".format(GREEN, RESET))
        print("  Spell correction: {}".format(spell_status))
        print("  Scoring = {:.0f}% cosine + {:.0f}% cross-encoder (on top {} candidates)\n".format(
            RERANK_ALPHA * 100, (1 - RERANK_ALPHA) * 100, RERANK_TOP_K))

    if args.file:
        run_file(args.file, model, all_embs, all_texts, all_cats,
                 args.verbose, args.rerank, args.nli, use_extraction)
    else:
        run_interactive(model, all_embs, all_texts, all_cats,
                        args.verbose, args.rerank, args.nli, use_extraction)


if __name__ == "__main__":
    main()
